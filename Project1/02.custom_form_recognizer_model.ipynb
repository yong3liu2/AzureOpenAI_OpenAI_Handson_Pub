{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Custom Form Recognizer model\n",
    "\n",
    "* [x] Provide exported project file \n",
    "\n",
    "* [x] Provide code to call your model with sample data  \n",
    "\n",
    "* [?] Optional: Provide code to create custom model \n",
    "\n",
    "* [x] Build and train a custom classifier - Document Intelligence (formerly Form Recognizer) - Azure AI services | Microsoft Learn: https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/build-a-custom-classifier?view=doc-intel-3.1.0\n",
    "    * [x] Data Source: 5 arxiv paper  \n",
    "\n",
    "    * [x] Extract: Name of the paper, author list, abstract, number of pages \n",
    "\n",
    "    * [x] Optional: classify content page vs reference page "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1: Custom Extraction Model - trainings\n",
    "\n",
    "* Training #1:\n",
    "\n",
    "    project name: CustomPaperExtraction\n",
    "    \n",
    "    Form Recognizer at tier(F0) which causes only the first two pages will be processed for training. Has no capability to train total page which would be in the last page.   \n",
    "    This is no go.\n",
    "\n",
    "* Training #2:\n",
    "\n",
    "    project name: CustomPaperExtraction_S0\n",
    "    \n",
    "    Form Recognizer at tier(S0) which entire document will be processed for training. Therefore, it is capable to train total page number.\n",
    "    \n",
    "    Projec token:\n",
    "\n",
    "    \"eyJpZCI6Ii9zdWJzY3JpcHRpb25zLzdjYjA5ODk0LWNjNTgtNDczNS1iNjQ0LTVkY2FlYWVjNTI4OC9yZXNvdXJjZUdyb3Vwcy9BenVyZVN0b3JhZ2VBY2NvdW50L3Byb3ZpZGVycy9NaWNyb3NvZnQuU3RvcmFnZS9zdG9yYWdlQWNjb3VudHMveW9uZ2Fpc3RvcmFnZWFjY291bnQiLCJjb250YWluZXIiOiJwYXBlcmV4dHJhY3Rpb24iLCJwYXRoIjoiY29uZmlnLWY3NzczNTYwLWYxZGUtNGNlZS04N2RlLTE1MTRhYjlmNjhiNi05NzY0NzgxMy0zNjYzLTQyNjctYTg1OC01OGFlOWM4ZWNhZjMuanNvbiIsInR5cGUiOjB9\"\n",
    "    \n",
    "* Azure AI | Document Intelligence Studio: https://formrecognizer.appliedai.azure.com/studio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2: Code to call trained Custom Extraction model\n",
    "\n",
    "* Custome model extraction project name: CustomPaperExtraction_S0\n",
    "    \n",
    "    Form Recognizer at tier(S0) which entire document will be processed for training. Therefore, it is capable to train total page number.\n",
    "    \n",
    "    pdf_file_path = './data_source/testing_data/arxiv.org/ai_paper/...'\n",
    "\n",
    "\n",
    "#### Testing and evaluation (Training #2 - retrained with more data)\n",
    "* Model ID: 11092023\n",
    "* Data Source: 5 arxiv paper in AI area\n",
    "* Extract Tile, Authors, Abstract and Total pages\n",
    "<br>\n",
    "\n",
    "| Documents | Title | Authors | Abstract | Total page extracted | Total page calculated | Accuracy | Extraction<br>note |\n",
    "| --------- | ----- | ------- | -------- | ------------------ | --------------------- | -------- | -------------- |\n",
    "| 2311.01043.pdf | Yes and 100% accurate | Yes and 100% accurate | Yes and 100% accurate | None | Yes and 100% accurate. 13 Pages. | 100% | This doc does not have page number. |\n",
    "| 2311.01193.pdf | Yes and 100% accurate | Yes and 100% accurate | Yes and 100% accurate | Yes and 100% accurate. 36 Pages| If calculated, then the total pages would be 38! | 100%| Since this doc has page number and the last page's page number is 36, hence it is 36. The first two pages (abstract page and toc page) of the original doc does not have page numbers. |\n",
    "| 2311.01258.pdf | Yes and 100% accurate | 8 out of 9 authors were extracted | Yes and 100% accurate | Yes and 100% accurate | N/A |  97.22% | Authors in this doc are mixed with Universities. This is a tough case. Might need some special treatment. |\n",
    "| 2311.01460.pdf | Yes and 100% accurate | Yes and 100% accurate | Yes and 100% accurate | Yes and 100% accurate | N/A | 100%| This doc has page number. |\n",
    "| 2310.18168.pdf | Yes and 100% accurate | Yes and 100% accurate | Yes and 100% accurate | Yes and 100% accurate | N/A | 100%| This doc has page number. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access different trained models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier level: S0\n",
      "~~~~~~Total number of pages in the PDF: 15\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code sample shows Custom Extraction Model operations with the Azure Form Recognizer client library. \n",
    "The async versions of the samples require Python 3.6 or later.\n",
    "\n",
    "To learn more, please visit the documentation - Quickstart: Form Recognizer Python client library SDKs\n",
    "https://docs.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/quickstarts/try-v3-python-sdk\n",
    "\"\"\"\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "\"\"\"\n",
    "Remember to remove the key from your code when you're done, and never post it publicly. For production, use\n",
    "secure methods to store and access your credentials. For more information, see \n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-security?tabs=command-line%2Ccsharp#environment-variables-and-application-configuration\n",
    "\"\"\"\n",
    "# endpoint = \"YOUR_FORM_RECOGNIZER_ENDPOINT\"\n",
    "# key = \"YOUR_FORM_RECOGNIZER_KEY\"\n",
    "# Get key and region from system env\n",
    "# model_id = \"YOUR_CUSTOM_BUILT_MODEL_ID\"\n",
    "\n",
    "# tier = 'F0'\n",
    "tier_f0 = False\n",
    "if (tier_f0):\n",
    "    # Trained using Form Recognizer at Tier(F0) - only read first two pages of each uploaded doc\n",
    "    # Pros: free\n",
    "    # Cons: cannot extract total page\n",
    "    endpoint=os.getenv(\"FORM_RECOGNIZER_ENDPOINT\")\n",
    "    key=os.getenv('FORM_RECOGNIZER_KEY')\n",
    "    model_id=os.getenv('CUSTOM_BUILT_MODEL_ID_F0')\n",
    "    print(\"Tier level: F0\")\n",
    "else: \n",
    "    # Trained using Document Intelligence (form recognizer) at Tier(SO) - read entire doc\n",
    "    # Pros: can extract total page\n",
    "    # Cons: S0\n",
    "    endpoint=os.getenv(\"DI_FORM_RECOGNIZER_ENDPOINT\")\n",
    "    key=os.getenv('DI_FORM_RECOGNIZER_KEY')\n",
    "    # model_id=os.getenv('CUSTOM_BUILT_MODEL_ID_S0')\n",
    "    model_id=os.getenv('CUSTOM_BUILT_MODEL_ID_CEM')    \n",
    "    print(\"Tier level: S0\")\n",
    "\n",
    "# Access testing data\n",
    "# formUrl = \"YOUR_DOCUMENT\"\n",
    "# Path to testing data - PDF files\n",
    "\n",
    "# Test case 1: 13 pages\n",
    "# pdf_file_path = './data_source/testing_data/arxiv.org/ai_paper/2311.01043.pdf'\n",
    "\n",
    "# Test case 2: 39 pages\n",
    "# pdf_file_path = './data_source/testing_data/arxiv.org/ai_paper/2311.01193.pdf'\n",
    "\n",
    "# Test case 3: 175 pages\n",
    "# pdf_file_path = './data_source/testing_data/arxiv.org/ai_paper/2311.01258.pdf'\n",
    "\n",
    "# Test case 4: 18 pages\n",
    "# pdf_file_path = './data_source/testing_data/arxiv.org/ai_paper/2311.01460.pdf'\n",
    "\n",
    "# Test case 5: 15 pages\n",
    "pdf_file_path = './data_source/testing_data/arxiv.org/ai_paper/2310.18168.pdf'\n",
    "\n",
    "def get_pdf_total_pages():\n",
    "    # Open the PDF file in binary read mode\n",
    "    with open(pdf_file_path, 'rb') as pdf_file:\n",
    "        # Create a PDF object to read the file\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "        # Check the total number of pages in the PDF\n",
    "        total_pages = len(pdf_reader.pages)\n",
    "\n",
    "    # Print the total number of pages\n",
    "    print(f'~~~~~~Total number of pages in the PDF: {total_pages}')\n",
    "\n",
    "get_pdf_total_pages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract paper title, authors, abstract, and number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Analyzing document #1--------\n",
      "Document has type 11092023\n",
      "Document has confidence 0.987\n",
      "Document was analyzed by model with ID 11092023\n",
      "......found field name:  paper_title\n",
      "......found field of type 'string' with value 'PERSONAS AS A WAY TO MODEL TRUTHFULNESS IN LANGUAGE MODELS' and with confidence 0.601\n",
      "......found field name:  authors\n",
      "......found field of type 'string' with value 'Nitish Joshi1* Javier Rando2* Abulhair Saparov1 Najoung Kim3 He He1' and with confidence 0.624\n",
      "......found field name:  abstract\n",
      "......found field of type 'string' with value 'ABSTRACT Large Language Models (LLMs) are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent \"Wikipedia\" will behave truthfully on topics that were only generated by \"Science\" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a model's answer will be truthful before it is generated; (2) finetuning a model on a set of facts improves its truthfulness on unseen topics. Next, using arithmetics as a synthetic environment, we show that language models can separate true and false statements, and generalize truthfulness across agents; but only if agents in the training data share a truthful generative process that enables the creation of a truthful persona. Overall, our findings suggest that models can exploit hierarchical structures in the data to learn abstract concepts like truthfulness.' and with confidence 0.07\n",
      "......found field name:  total_page\n",
      "......found field of type 'string' with value '15' and with confidence 0.562\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    ")\n",
    "\n",
    "# Make sure your document's type is included in the list of document types the custom model can analyze\n",
    "# poller = document_analysis_client.begin_analyze_document_from_url(model_id, formUrl)\n",
    "with open(pdf_file_path, 'rb') as pdf_file:\n",
    "    poller = document_analysis_client.begin_analyze_document(model_id, pdf_file)\n",
    "result = poller.result()\n",
    "\n",
    "for idx, document in enumerate(result.documents):\n",
    "    print(\"--------Analyzing document #{}--------\".format(idx + 1))\n",
    "    print(\"Document has type {}\".format(document.doc_type))\n",
    "    print(\"Document has confidence {}\".format(document.confidence))\n",
    "    print(\"Document was analyzed by model with ID {}\".format(result.model_id))\n",
    "    for name, field in document.fields.items():\n",
    "        print(\"......found field name: \", name)\n",
    "        field_value = field.value if field.value else field.content\n",
    "        print(\"......found field of type '{}' with value '{}' and with confidence {}\".format(field.value_type, field_value, field.confidence))\n",
    "        # We only fetch total page number using PDF lib if the total page number can not be found by the model.\n",
    "        if (name == 'total_page' and field_value == None ):\n",
    "            print(\"~~~~~~total_page's value is None returned from Custom Extraction Model! Let's use PDF lib to find out document's total page.\")\n",
    "            get_pdf_total_pages()\n",
    "\n",
    "# # iterate over tables, lines, and selection marks on each page\n",
    "# for page in result.pages:\n",
    "#     print(\"\\nLines found on page {}\".format(page.page_number))\n",
    "#     for line in page.lines:\n",
    "#         print(\"...Line '{}'\".format(line.content.encode('utf-8')))\n",
    "#     for word in page.words:\n",
    "#         print(\n",
    "#             \"...Word '{}' has a confidence of {}\".format(\n",
    "#                 word.content.encode('utf-8'), word.confidence\n",
    "#             )\n",
    "#         )\n",
    "#     for selection_mark in page.selection_marks:\n",
    "#         print(\n",
    "#             \"...Selection mark is '{}' and has a confidence of {}\".format(\n",
    "#                 selection_mark.state, selection_mark.confidence\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "# for i, table in enumerate(result.tables):\n",
    "#     print(\"\\nTable {} can be found on page:\".format(i + 1))\n",
    "#     for region in table.bounding_regions:\n",
    "#         print(\"...{}\".format(i + 1, region.page_number))\n",
    "#     for cell in table.cells:\n",
    "#         print(\n",
    "#             \"...Cell[{}][{}] has content '{}'\".format(\n",
    "#                 cell.row_index, cell.column_index, cell.content.encode('utf-8')\n",
    "#             )\n",
    "#         )\n",
    "print(\"-----------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3: Build and train a Custom Classification model\n",
    "\n",
    "* [Optional]: classify content page vs reference page \n",
    "\n",
    "* Training #1:*This training has some issues if the reference pages are not at the end. Hence, the training data set would need to break into more granular data units.*\n",
    "\n",
    "    project name: CustomPaperClassification_S0\n",
    "    \n",
    "    Training data: pdf files need to be preprocessed at page level. Documents were splitted into documents containing content pages or reference pages.\n",
    "    \n",
    "    Projec token:\n",
    "\n",
    "    \"eyJpZCI6Ii9zdWJzY3JpcHRpb25zLzdjYjA5ODk0LWNjNTgtNDczNS1iNjQ0LTVkY2FlYWVjNTI4OC9yZXNvdXJjZUdyb3Vwcy9BenVyZVN0b3JhZ2VBY2NvdW50L3Byb3ZpZGVycy9NaWNyb3NvZnQuU3RvcmFnZS9zdG9yYWdlQWNjb3VudHMveW9uZ2Fpc3RvcmFnZWFjY291bnQiLCJjb250YWluZXIiOiJwYXBlcmNsYXNzaWZpY2F0aW9uIiwicGF0aCI6ImNvbmZpZy1mNzc3MzU2MC1mMWRlLTRjZWUtODdkZS0xNTE0YWI5ZjY4YjYtN2FjZTRmM2UtYzY5ZS00YWFmLWEzYTktZDIwMmRlYmUyMjg2Lmpzb24iLCJ0eXBlIjoxfQ==\"\n",
    "    \n",
    "*  Training #2: This training provides very good classification result!\n",
    "\n",
    "    project name: CustomDocClassification\n",
    "    \n",
    "    Training data: pdf files need to be preprocessed at page level. Documents were splitted into documents containing single content page or single reference page.\n",
    "    \n",
    "    Projec token:\n",
    "\n",
    "    \"eyJpZCI6Ii9zdWJzY3JpcHRpb25zLzdjYjA5ODk0LWNjNTgtNDczNS1iNjQ0LTVkY2FlYWVjNTI4OC9yZXNvdXJjZUdyb3Vwcy9BenVyZVN0b3JhZ2VBY2NvdW50L3Byb3ZpZGVycy9NaWNyb3NvZnQuU3RvcmFnZS9zdG9yYWdlQWNjb3VudHMveW9uZ2Fpc3RvcmFnZWFjY291bnQiLCJjb250YWluZXIiOiJkb2NjbGFzc2lmaWNhdGlvbiIsInBhdGgiOiJjb25maWctZjc3NzM1NjAtZjFkZS00Y2VlLTg3ZGUtMTUxNGFiOWY2OGI2LWVjNWM5ZTYyLWFiYjUtNGYyOC04M2YwLTRiM2U1MzljYzBmOS5qc29uIiwidHlwZSI6MX0=\"\n",
    "    \n",
    "* Azure AI | Document Intelligence Studio: https://formrecognizer.appliedai.azure.com/studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task4: Code to call trained Custom Classification model\n",
    "\n",
    "* project name: CustomDocClassification\n",
    "\n",
    "* file path : './data_source/testing_data/arxiv.org/ai_paper/...'\n",
    "\n",
    "#### Testing and evaluation (training #2)\n",
    "* Data Source: 5 arxiv paper in AI area\n",
    "\n",
    "* Classify content page vs reference page\n",
    "\n",
    "| Documents | Total pages | Content pages | Reference pages | Classified contend pages | Classified reference pages | Accuracy | Misclassified<br>note |\n",
    "| --------- | ----------- | ------------- | --------------- | ------------------------ | -------------------------- | -------- | ----------------- |\n",
    "| 2311.01043.pdf | 13 | 1 to 7 | 8, 9, 10, 11, 12, 13, | 1 to 7 | 8, 9, 10, 11, 12, 13, | 100% | 0 |\n",
    "| 2311.01193.pdf | 38 | 1 to 29 | 30, 31,<br>32, 33, 34, 35, 36, 37, 38, | 1 to 31 | 32, 33, 34, 35, 36, 37, 38, | (38 -2)/38 = 94.74% | 2 pages are supposed to be reference page, but classified as content page. Model can be retrained with even more data, which might help to resolve this issue. Also, training data ratio might be a factor: current ratio content page vs reference page are 79 : 16. Might need to balance them out. |\n",
    "| 2311.01258.pdf | 175 | 1 to 154 | 155 to 175 | 1 to 154 | 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, | 100% | 0 |\n",
    "| 2311.01455.pdf | 39 | 1 to 10, 17 to 39 | 11, 12, 13, 14, 15, 16, | 1 to 10, 17 to 39 | 11, 12, 13, 14, 15, 16, | 100% | 0 |\n",
    "| 2311.01460.pdf | 18 | 1 to 9, 14 to 18 | 10, 11, 12, 13, | 1 to 9, 14 to 18 | 10, 11, 12, 13, | 100% | 0 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Analyzing document #1--------\n",
      "Document has type content_page\n",
      "content_page pages: 1, \n",
      "Document has confidence 0.964\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #2--------\n",
      "Document has type content_page\n",
      "content_page pages: 2, \n",
      "Document has confidence 0.982\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #3--------\n",
      "Document has type content_page\n",
      "content_page pages: 3, \n",
      "Document has confidence 0.974\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #4--------\n",
      "Document has type content_page\n",
      "content_page pages: 4, \n",
      "Document has confidence 0.978\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #5--------\n",
      "Document has type content_page\n",
      "content_page pages: 5, \n",
      "Document has confidence 0.996\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #6--------\n",
      "Document has type content_page\n",
      "content_page pages: 6, \n",
      "Document has confidence 0.994\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #7--------\n",
      "Document has type content_page\n",
      "content_page pages: 7, \n",
      "Document has confidence 0.996\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #8--------\n",
      "Document has type content_page\n",
      "content_page pages: 8, \n",
      "Document has confidence 0.986\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #9--------\n",
      "Document has type content_page\n",
      "content_page pages: 9, \n",
      "Document has confidence 0.978\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #10--------\n",
      "Document has type content_page\n",
      "content_page pages: 10, \n",
      "Document has confidence 0.965\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #11--------\n",
      "Document has type reference_page\n",
      "reference_page page: 11,\n",
      "reference_page page: 12,\n",
      "reference_page page: 13,\n",
      "reference_page page: 14,\n",
      "reference_page page: 15,\n",
      "reference_page page: 16,\n",
      "Please note: above 'reference_page' pages were encapsulated in a sub JSON structure within a single document returned by Custom Classification model,\n",
      "the document was given the confidence value as following:\n",
      "Document has confidence 0.001\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #12--------\n",
      "Document has type content_page\n",
      "content_page pages: 17, \n",
      "Document has confidence 0.993\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #13--------\n",
      "Document has type content_page\n",
      "content_page pages: 18, \n",
      "Document has confidence 0.967\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #14--------\n",
      "Document has type content_page\n",
      "content_page pages: 19, \n",
      "Document has confidence 0.988\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #15--------\n",
      "Document has type content_page\n",
      "content_page pages: 20, \n",
      "Document has confidence 0.99\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #16--------\n",
      "Document has type content_page\n",
      "content_page pages: 21, \n",
      "Document has confidence 0.958\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #17--------\n",
      "Document has type content_page\n",
      "content_page pages: 22, \n",
      "Document has confidence 0.987\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #18--------\n",
      "Document has type content_page\n",
      "content_page pages: 23, \n",
      "Document has confidence 0.988\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #19--------\n",
      "Document has type content_page\n",
      "content_page pages: 24, \n",
      "Document has confidence 0.987\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #20--------\n",
      "Document has type content_page\n",
      "content_page pages: 25, \n",
      "Document has confidence 0.993\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #21--------\n",
      "Document has type content_page\n",
      "content_page pages: 26, \n",
      "Document has confidence 0.968\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #22--------\n",
      "Document has type content_page\n",
      "content_page pages: 27, \n",
      "Document has confidence 0.989\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #23--------\n",
      "Document has type content_page\n",
      "content_page pages: 28, \n",
      "Document has confidence 0.974\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #24--------\n",
      "Document has type content_page\n",
      "content_page pages: 29, \n",
      "Document has confidence 0.983\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #25--------\n",
      "Document has type content_page\n",
      "content_page pages: 30, \n",
      "Document has confidence 0.973\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #26--------\n",
      "Document has type content_page\n",
      "content_page pages: 31, \n",
      "Document has confidence 0.972\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #27--------\n",
      "Document has type content_page\n",
      "content_page pages: 32, \n",
      "Document has confidence 0.972\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #28--------\n",
      "Document has type content_page\n",
      "content_page pages: 33, \n",
      "Document has confidence 0.866\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #29--------\n",
      "Document has type content_page\n",
      "content_page pages: 34, \n",
      "Document has confidence 0.848\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #30--------\n",
      "Document has type content_page\n",
      "content_page pages: 35, \n",
      "Document has confidence 0.889\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #31--------\n",
      "Document has type content_page\n",
      "content_page pages: 36, \n",
      "Document has confidence 0.95\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #32--------\n",
      "Document has type content_page\n",
      "content_page pages: 37, \n",
      "Document has confidence 0.964\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #33--------\n",
      "Document has type content_page\n",
      "content_page pages: 38, \n",
      "Document has confidence 0.949\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "--------Analyzing document #34--------\n",
      "Document has type content_page\n",
      "content_page pages: 39, \n",
      "Document has confidence 0.973\n",
      "Document was analyzed by model with ID CCM110623_1\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Custom Classification Model ID\n",
    "# ccm_model_id=os.getenv('CUSTOM_BUILT_MODEL_ID_CCM')\n",
    "ccm_model_id=os.getenv('CUSTOM_BUILT_MODEL_ID_CCM_PAGE')\n",
    "\n",
    "# Access testing data\n",
    "# formUrl = \"YOUR_DOCUMENT\"\n",
    "# Path to testing data - PDF files\n",
    "\n",
    "# Test case 1: 13 pages\n",
    "# pdf_file_path = './data_source/testing_data/arxiv.org/ai_paper/2311.01043.pdf'\n",
    "\n",
    "# Test case 2: 39 pages\n",
    "# pdf_file_path = './data_source/testing_data/arxiv.org/ai_paper/2311.01193.pdf'\n",
    "\n",
    "# Test case 3: 175 pages\n",
    "# pdf_file_path = './data_source/testing_data/arxiv.org/ai_paper/2311.01258.pdf'\n",
    "\n",
    "# Test case 4: 39 pages\n",
    "pdf_file_path = './data_source/testing_data/arxiv.org/ai_paper/2311.01455.pdf'\n",
    "\n",
    "# Test case 5:\n",
    "# pdf_file_path = './data_source/testing_data/arxiv.org/ai_paper/2311.01460.pdf'\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    ")\n",
    "\n",
    "# Make sure your document's type is included in the list of document types the custom model can analyze\n",
    "# poller = document_analysis_client.begin_analyze_document_from_url(model_id, formUrl)\n",
    "with open(pdf_file_path, 'rb') as pdf_file:\n",
    "    poller = document_analysis_client.begin_classify_document(ccm_model_id, pdf_file)\n",
    "result = poller.result()\n",
    "\n",
    "for idx, document in enumerate(result.documents):\n",
    "    pages = \"\"\n",
    "    print(\"--------Analyzing document #{}--------\".format(idx + 1))\n",
    "    print(\"Document has type {}\".format(document.doc_type))\n",
    "    pages += document.doc_type + ' pages: '\n",
    "    if (document.doc_type == 'reference_page'):\n",
    "        pages = ''\n",
    "        for page in document.bounding_regions:\n",
    "            # print(\"......found field name: \", page.page_number)\n",
    "            pages += document.doc_type + ' page: ' + str(page.page_number) + ',\\n'\n",
    "            # pages += str(page.page_number) + ', '        \n",
    "        pages += \"Please note: above 'reference_page' pages were encapsulated in a sub JSON structure within a single document returned by Custom Classification model,\\n\"\n",
    "        pages += \"the document was given the confidence value as following:\"\n",
    "    else:\n",
    "        for page in document.bounding_regions:\n",
    "            pages += str(page.page_number) + ', '\n",
    "    print(pages)\n",
    "    # if (document.doc_type == 'reference_page'):\n",
    "    #     print(\"Custom Classiication Mode returns a JSON structure which encapsolates 'reference_page' in a single document and the document was given the confidence value as following:\")\n",
    "    print(\"Document has confidence {}\".format(document.confidence))\n",
    "    print(\"Document was analyzed by model with ID {}\".format(result.model_id))\n",
    "   \n",
    "print(\"-----------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task5: [Optional]: Provide code to create custom model \n",
    "\n",
    "* TODO: issue to access Azure contrainer, need to figure out if there are additional setups needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.formrecognizer import FormTrainingClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Replace with your Form Recognizer service endpoint and API key\n",
    "# endpoint = \"YOUR_FORM_RECOGNIZER_ENDPOINT\"\n",
    "# credential = AzureKeyCredential(\"YOUR_API_KEY\")\n",
    "credential=AzureKeyCredential(key)\n",
    "\n",
    "# Create a FormTrainingClient\n",
    "form_training_client = FormTrainingClient(endpoint, credential)\n",
    "\n",
    "# Define the blob container where your training documents are stored\n",
    "# training_container = \"your-training-container\"\n",
    "# TODO: need to figure out access container ...\n",
    "training_container =\"https://yongaitrainingdata.blob.core.windows.net/customextractionmodel/2310.17811.pdf?sp=r&st=2023-11-08T01:57:45Z&se=2023-11-08T09:57:45Z&sv=2022-11-02&sr=b&sig=v6HKu0mb9cBQnyjAODh6lPOZVKoZ6S6%2B6BlMujlpByU%3D\"\n",
    "\n",
    "# Define a name for your custom model\n",
    "model_name = \"custom-model-created-from-code_1\"\n",
    "\n",
    "# # Train the custom model\n",
    "# poller = form_training_client.begin_training(training_container, model_name)\n",
    "\n",
    "# # # Wait for training to complete\n",
    "# model = poller.result()\n",
    "\n",
    "# # # Get the model ID\n",
    "# model_id = model.model_id\n",
    "\n",
    "# print(f\"Custom model ID: {model_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
