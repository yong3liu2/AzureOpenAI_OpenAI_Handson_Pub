{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data and save them in vector store\n",
    "\n",
    "# Embedding and vector store\n",
    "\n",
    "* Data source: SEC filing reports\n",
    "\n",
    "* Azure OpenAI - embedding\n",
    "\n",
    "* FAISS\n",
    "\n",
    "* Azure AI Search (Azure Cognitive Searc) - vector store and vector search, semantic search, or both\n",
    "\n",
    "* LangChain framework - Azure OpenAI, Azure AI Search\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Langchain libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries  \n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SemanticSettings,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure OpenAI Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "# Set up Azure OpenAI\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AAG_AZURE_OPENAI_API_VERSION\")\n",
    "openai.api_version = AZURE_OPENAI_API_VERSION\n",
    "\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AAG_AZURE_OPENAI_API_KEY\").strip()\n",
    "assert AZURE_OPENAI_API_KEY, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = AZURE_OPENAI_API_KEY\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AAG_AZURE_OPENAI_ENDPOINT\",\"\").strip()\n",
    "assert AZURE_OPENAI_ENDPOINT, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "openai.api_base = AZURE_OPENAI_ENDPOINT\n",
    "\n",
    "# Deployment for Chat\n",
    "# DEPLOYMENT_NAME_CHAT = os.getenv('DEPLOYMENT_NAME_CHAT')\n",
    "DEPLOYMENT_NAME_CHAT = os.getenv('AAG_DEPLOYMENT_NAME_CHAT_16K')\n",
    "\n",
    "# Deployment for embedding\n",
    "DEPLOYMENT_NAME_EMBEDDING = os.getenv(\"AAG_DEPLOYMENT_NAME_EMBEDDING\")\n",
    "model: str = DEPLOYMENT_NAME_EMBEDDING\n",
    "\n",
    "# Azure AI Search (Cognitive vector store)\n",
    "vector_store_address: str = os.getenv(\"AAG_AZURE_SEARCH_SERVICE_ENDPOINT\")  \n",
    "vector_store_password: str = os.getenv(\"AAG_AZURE_SEARCH_ADMIN_KEY\")\n",
    "# index_name: str = \"langchain-vector-arxiv-physics\"\n",
    "\n",
    "# Deployment for embedding\n",
    "BING_SUBSCRIPTION_KEY = os.getenv(\"BING_SUBSCRIPTION_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SEC data\n",
    "\n",
    "* 10-K, 10-Q, 8-K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loas single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load pdf files\n",
    "loader = PyPDFLoader(\"./data_source/zbra-20221231_10-K.pdf\")\n",
    "loaded_documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch process: load all pdf files in a folder\n",
    "\n",
    "* RateLimitError - need to do in small number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation >>>\n",
      "Found PDF file: data_source/10-K\\zbra-20221231_10-K.pdf\n",
      "Data preparation is done! <<<\n"
     ]
    }
   ],
   "source": [
    "loaded_documents=[]\n",
    "print(\"Data preparation >>>\")\n",
    "# Ask the user to provide the folder path\n",
    "folder_path = input('Enter the path to the folder: ')\n",
    "\n",
    "# Check if the provided path exists\n",
    "if not os.path.exists(folder_path):\n",
    "    print(f'The folder path \"{folder_path}\" does not exist.')\n",
    "else:\n",
    "    # Loop through the files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is a PDF\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            # If it's a PDF, print the file name or perform any other desired action\n",
    "            # print(f'Found PDF file: {filename}')\n",
    "            orig_file_with_full_path = os.path.join(folder_path, filename)\n",
    "            print(f'Found PDF file: {orig_file_with_full_path}')\n",
    "\n",
    "            loader = PyPDFLoader(orig_file_with_full_path)\n",
    "            loaded_documents += loader.load()\n",
    "            \n",
    "            # Break out of the loop after processing the first PDF file - testing only\n",
    "            # break\n",
    "print(\"Data preparation is done! <<<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Split documents to chucks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "splitted_docs = text_splitter.split_documents(loaded_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings and vector store instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: FAISS vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Get Azure OpenAI embedding\n",
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(\n",
    "    deployment=model,\n",
    "    model=model,\n",
    "    chunk_size=1,   # this 'chunk_size' is misleading, it is really about 'input' text string, not the number of words or characters in the text.\n",
    "    openai_api_base=AZURE_OPENAI_ENDPOINT,\n",
    "    openai_api_type=\"azure\",\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "# Create the vector index\n",
    "db = FAISS.from_documents(splitted_docs, embeddings)\n",
    "# Query the index\n",
    "# query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "# docs = db.similarity_search(query)\n",
    "# # Print the results\n",
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on different data source, save them into different FAISS db. The thought behind this approach is credibility (audited) vs most recent data. There could be different weight to it while giving rating later on. So, prepare the data this way could provide flexibility for future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-K\n",
    "db.save_local(\"faiss_index_8-K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Q\n",
    "db.save_local(\"faiss_index_10-Q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-K\n",
    "db.save_local(\"faiss_index_10-K\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Azure AI Search (Cognitive search)\n",
    "\n",
    "* TODO: will do indexing later.  Need to watch for the cost, hold on for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Azure OpenAI embedding\n",
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(deployment=model, model=model, \n",
    "                                                chunk_size=1, \n",
    "                                                openai_api_base = AZURE_OPENAI_ENDPOINT, \n",
    "                                                openai_api_type = \"azure\", \n",
    "                                                api_key = AZURE_OPENAI_API_KEY)\n",
    "# Define index (aka embedding) name\n",
    "index_name: str = \"langchain-vector-zebra-10k-10q-8k\"\n",
    "\n",
    "# Create index in the vector store\n",
    "azure_ai_search_vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    semantic_configuration_name='config',\n",
    "        semantic_settings=SemanticSettings(\n",
    "            default_configuration='config',\n",
    "            configurations=[\n",
    "                SemanticConfiguration(\n",
    "                    name='config',\n",
    "                    prioritized_fields=PrioritizedFields(\n",
    "                        title_field=SemanticField(field_name='content'),\n",
    "                        prioritized_content_fields=[SemanticField(field_name='content')],\n",
    "                        prioritized_keywords_fields=[SemanticField(field_name='metadata')]\n",
    "                    ))\n",
    "            ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert text and embeddings into vector store - need to warch the code, so hold for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute following will start embedding ...\n",
    "# azure_ai_search_vector_store.add_documents(documents=splitted_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
