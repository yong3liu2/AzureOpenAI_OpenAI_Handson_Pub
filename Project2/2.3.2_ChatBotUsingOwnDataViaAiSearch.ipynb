{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 3\n",
    "\n",
    "# Build Question and Answer ChatBot from PDF document (Another approach: leverage Azure AI Search)\n",
    "\n",
    "* Arxiv AI papers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Use Azure OpenAI Studio load Arxiv AI papers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Set up Azure OpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: The openai-python library support for Azure OpenAI is in preview.\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "# Set up Azure OpenAI\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "\n",
    "CHAT_API_KEY = os.getenv(\"OPENAI_API_CHAT_KEY\",\"\").strip()\n",
    "assert CHAT_API_KEY, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = CHAT_API_KEY\n",
    "\n",
    "CHAT_RESOURCE_ENDPOINT = os.getenv(\"OPENAI_API_CHAT_ENDPOINT\",\"\").strip()\n",
    "assert CHAT_RESOURCE_ENDPOINT, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in CHAT_RESOURCE_ENDPOINT.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = CHAT_RESOURCE_ENDPOINT\n",
    "\n",
    "# Deployment for Chat\n",
    "# DEPLOYMENT_NAME_CHAT = os.getenv('DEPLOYMENT_NAME_CHAT')\n",
    "DEPLOYMENT_NAME_CHAT = os.getenv('DEPLOYMENT_NAME_CHAT_16K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Set up Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "deployment_id = DEPLOYMENT_NAME_CHAT\n",
    "\n",
    "# Azure AI Search setup\n",
    "search_endpoint = \"https://yongaisearchservice.search.windows.net\"; # Add your Azure AI Search endpoint here\n",
    "# search_key = os.getenv(\"SEARCH_KEY\"); # Add your Azure AI Search admin key here\n",
    "search_key = \"14jbI4GODPsnWwml22frAAV1tQyM0b1gWb0P9B0jahAzSeBXq7OQ\"\n",
    "\n",
    "search_index_name = \"arxivpapers\"; # Add your Azure AI Search index name here\n",
    "\n",
    "def setup_byod(deployment_id: str) -> None:\n",
    "    \"\"\"Sets up the OpenAI Python SDK to use your own data for the chat endpoint.\n",
    "\n",
    "    :param deployment_id: The deployment ID for the model to use with your own data.\n",
    "\n",
    "    To remove this configuration, simply set openai.requestssession to None.\n",
    "    \"\"\"\n",
    "\n",
    "    class BringYourOwnDataAdapter(requests.adapters.HTTPAdapter):\n",
    "\n",
    "        def send(self, request, **kwargs):\n",
    "            request.url = f\"{openai.api_base}/openai/deployments/{deployment_id}/extensions/chat/completions?api-version={openai.api_version}\"\n",
    "            return super().send(request, **kwargs)\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    # Mount a custom adapter which will use the extensions endpoint for any call using the given `deployment_id`\n",
    "    session.mount(\n",
    "        prefix=f\"{openai.api_base}/openai/deployments/{deployment_id}\",\n",
    "        adapter=BringYourOwnDataAdapter()\n",
    "    )\n",
    "\n",
    "    openai.requestssession = session\n",
    "\n",
    "setup_byod(deployment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Build a Q&A ChatBot using own data\n",
    "\n",
    "* Arxiv papers stored in Azure storage container\n",
    "\n",
    "* Azure AI Search - vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This the main loop for ChatBot conversation till user say 'quit'\n",
    "\n",
    "* TODO: need to test more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot ('quit' to exit):\n",
      "Me: \"Model-Based Minimum Bayes Risk Decoding\"\n",
      "Bot: {\n",
      "  \"id\": \"0485851c-12e8-4214-bddb-d48020ce5a48\",\n",
      "  \"model\": \"gpt-35-turbo-16k\",\n",
      "  \"created\": 1700154268,\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"messages\": [\n",
      "        {\n",
      "          \"index\": 0,\n",
      "          \"role\": \"tool\",\n",
      "          \"content\": \"{\\\"citations\\\": [{\\\"content\\\": \\\"Model-Based Minimum Bayes Risk Decoding\\\\nYuu Jinnai, Tetsuro Morimura, Ukyo Honda, Kaito Ariu, Kenshi Abe\\\\nCyberAgent\\\\n{jinnai_yu,morimura_tetsuro,honda_ukyo,kaito_ariu,abe_kenshi}@cyberagent.co.jp\\\\nAbstract\\\\nMinimum Bayes Risk (MBR) decoding has\\\\nbeen shown to be a powerful alternative to\\\\nbeam search decoding in a variety of text gener-\\\\nation tasks. MBR decoding selects a hypothesis\\\\nfrom a pool of hypotheses that has the least ex-\\\\npected risk under a probability model according\\\\nto a given utility function. Since it is impracti-\\\\ncal to compute the expected risk exactly over\\\\nall possible hypotheses, two approximations\\\\nare commonly used in MBR. First, it integrates\\\\nover a sampled set of hypotheses rather than\\\\nover all possible hypotheses. Second, it esti-\\\\nmates the probability of each hypothesis using\\\\na Monte Carlo estimator. While the first approx-\\\\nimation is necessary to make it computationally\\\\nfeasible, the second is not essential since we\\\\ntypically have access to the model probability\\\\nat inference time. We propose Model-Based\\\\nMBR (MBMBR), a variant of MBR that uses\\\\nthe model probability itself as the estimate of\\\\nthe probability distribution instead of the Monte\\\\nCarlo estimate. We show analytically and em-\\\\npirically that the model-based estimate is more\\\\npromising than the Monte Carlo estimate in\\\\ntext generation tasks. Our experiments show\\\\nthat MBMBR outperforms MBR in several text\\\\ngeneration tasks, both with encoder-decoder\\\\nmodels and with large language models.\\\\n1 Introduction\\\\nOne of the key components of text generation is the\\\\ndecoding strategy, which is the decision rule used to\\\\ngenerate sentences from the model. Beam search is\\\\nthe popular decoding strategy that has been widely\\\\nused in many directed text generation tasks, includ-\\\\ning machine translation (Wu et al., 2016; Ott et al.,\\\\n2019; Wolf et al., 2020), text summarization (Rush\\\\net al., 2015; Narayan et al., 2018), and image cap-\\\\ntioning (Anderson et al., 2017). However, beam\\\\nsearch is known to have several degeneration prob-\\\\nlems. For example, Welleck et al. (2020) report\\\\nthat beam search can yield infinite-length outputs\\\\nthat the model assigns zero probability to.\\\\nMinimum Bayes Risk (MBR) decoding has re-\\\\ncently gained attention as a decoding strategy with\\\\nthe potential to overcome the problems of beam\\\\nsearch (Goodman, 1996; Kumar and Byrne, 2004;\\\\nEikema and Aziz, 2020, 2022; Freitag et al., 2022;\\\\nBertsch et al., 2023). MBR decoding consists of\\\\nthe following steps. First, it samples multiple se-\\\\nquences from the model. Then, it compares each\\\\nsequence to the others according to a utility func-\\\\ntion. Finally, it selects the sequence that maximizes\\\\nthe expected utility over the estimated probability\\\\ndistribution over the sequences.\\\\nPrevious work on MBR decoding uses a Monte\\\\nCarlo estimate to approximate the probability dis-\\\\ntribution since it is an unbiased estimate of the true\\\\nmodel distribution. However, because the number\\\\nof possible hypotheses is enormous compared to\\\\nthe number of samples, the estimation error of the\\\\nMonte Carlo estimate is huge. This can lead to a\\\\nhuge error in the expected utility estimate.\\\\nWe propose Model-Based MBR (MBMBR), a\\\\nvariant of MBR that uses a model-based estimate\\\\nof the MBR objective instead of a Monte Carlo\\\\nestimate. The model-based estimate uses the prob-\\\\nability model itself, but with its domain limited to\\\\nthe set of observed hypotheses. As such, the esti-\\\\nmate is computationally feasible and as accurate as\\\\nthe model for the observed sequences. MBMBR\\\\nis easy to implement and requires only the model\\\\nprobability of the sampled sequences, which can be\\\\nobtained concurrently with the sampling procedure.\\\\nWe first evaluate the model-based estimate and\\\\nshow analytically that the Kullback-Leiber (KL)\\\\ndivergence from the true model probability is guar-\\\\nanteed to be lower than that of the Monte Carlo\\\\nestimate. We also evaluate it empirically on var-\\\\nious text generation tasks and show that the KL\\\\ndivergence of the model-based estimate is signifi-\\\\ncantly lower than that of the Monte Carlo estimate,\\\\nindicating that it is a better estimator for the use of\\\\nMBR\\\", \\\"id\\\": null, \\\"title\\\": \\\"Model-Based Minimum Bayes Risk Decoding\\\", \\\"filepath\\\": \\\"2311.05263.pdf\\\", \\\"url\\\": \\\"https://yongaistorageaccount.blob.core.windows.net/arxivpapers/2311.05263.pdf\\\", \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1004. Scores=11.020248Org Highlight count=48.\\\"}, \\\"chunk_id\\\": \\\"1\\\"}, {\\\"content\\\": \\\"co/mistralai/Mistral-7B-Instruct-v0.1\\\\nhttps://huggingface.co/Salesforce/blip2-flan-t5-xl-coco\\\\nhttps://huggingface.co/Salesforce/blip2-flan-t5-xl-coco\\\\nhttps://huggingface.co/Salesforce/blip2-flan-t5-xl\\\\nhttps://huggingface.co/Salesforce/blip2-flan-t5-xl\\\\nhttps://github.com/suzgunmirac/crowd-sampling/blob/main/prompts/e2e_nlg_clean_fs.txt\\\\nhttps://github.com/suzgunmirac/crowd-sampling/blob/main/prompts/e2e_nlg_clean_fs.txt\\\\nhttps://github.com/suzgunmirac/crowd-sampling/blob/main/prompts/e2e_nlg_clean_fs.txt\\\\n7 Conclusions\\\\nWe propose Model-Based Minimum Bayes Risk\\\\n(MBMBR) decoding, a variant of MBR that uses\\\\na model-based distribution as an estimator of the\\\\nmodel probability. We evaluate the model-based\\\\ndistribution analytically and empirically to show\\\\nthat it is closer to the true model probability than\\\\nthe Monte Carlo estimate with respect to KL diver-\\\\ngence. The result suggests that the model-based dis-\\\\ntribution is likely to be a better estimator for the pur-\\\\npose of MBR decoding. We perform MBMBR de-\\\\ncoding on a variety of text generation tasks, includ-\\\\ning machine translation, text summarization, image\\\\ncaptioning, and data-to-text, using both domain-\\\\nspecific sequence-to-sequence models and domain-\\\\nindependent large language models. The empirical\\\\nresults show that MBMBR outperforms MBR in\\\\nmost cases.\\\\n8 Limitations\\\\nThe weakness of the proposed method is that it\\\\nrequires access to the model probability. Therefore,\\\\nit is not applicable to a situation where the sam-\\\\nples are generated by an external model where the\\\\nmodel probability is not available.\\\\nOne of the critical limitations of MBR decod-\\\\ning is its decoding speed. It requires computing a\\\\nutility function that is quadratic in the number of\\\\nsamples. This makes MBR decoding very expen-\\\\nsive and impractical in many situations. MBMBR\\\\ninherits the same limitation because it is derived\\\\nfrom MBR.\\\\nDue to limited computational resources, some of\\\\nour experiments use only part of the dataset instead\\\\nof the whole dataset. As a result, the scores are not\\\\ndirectly comparable with the existing literature.\\\\nReferences\\\\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\\\\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi\\\\nParikh, Stefan Lee, and Peter Anderson. 2019. no-\\\\ncaps: novel object captioning at scale. In Proceed-\\\\nings of the IEEE International Conference on Com-\\\\nputer Vision, pages 8948\\u20138957.\\\\nPeter Anderson, Basura Fernando, Mark Johnson, and\\\\nStephen Gould. 2017. Guided open vocabulary im-\\\\nage captioning with constrained beam search. In\\\\nProceedings of the 2017 Conference on Empirical\\\\nMethods in Natural Language Processing, pages 936\\u2013\\\\n945, Copenhagen, Denmark. Association for Compu-\\\\ntational Linguistics.\\\\nLo\\u00efc Barrault, Ondr\\u030cej Bojar, Marta R. Costa-juss\\u00e0,\\\\nChristian Federmann, Mark Fishel, Yvette Gra-\\\\nham, Barry Haddow, Matthias Huck, Philipp Koehn,\\\\nShervin Malmasi, Christof Monz, Mathias M\\u00fcller,\\\\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019.\\\\nFindings of the 2019 conference on machine trans-\\\\nlation (WMT19). In Proceedings of the Fourth Con-\\\\nference on Machine Translation (Volume 2: Shared\\\\nTask Papers, Day 1), pages 1\\u201361, Florence, Italy. As-\\\\nsociation for Computational Linguistics.\\\\nAmanda Bertsch, Alex Xie, Graham Neubig, and\\\\nMatthew R Gormley. 2023. It\\u2019s mbr all the\\\\nway down: Modern generation techniques through\\\\nthe lens of minimum bayes risk. arXiv preprint\\\\narXiv:2310.01387\\\", \\\"id\\\": null, \\\"title\\\": \\\"Model-Based Minimum Bayes Risk Decoding\\\", \\\"filepath\\\": \\\"2311.05263.pdf\\\", \\\"url\\\": \\\"https://yongaistorageaccount.blob.core.windows.net/arxivpapers/2311.05263.pdf\\\", \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1006. Scores=10.786992Org Highlight count=23.\\\"}, \\\"chunk_id\\\": \\\"2\\\"}, {\\\"content\\\": \\\",\\\\n2023) using a set of reference hypotheses Href \\u2286 Y\\\\nsampled from the model P :\\\\nhMC = argmax\\\\nh\\u2208Hcand\\\\n1\\\\n|Href |\\\\n\\u2211\\\\ny\\u2208Href\\\\nu(h,y). (5)\\\\nEq. (5) is derived by replacing the true model prob-\\\\nability P in Eq. (4) with the empirical distribution\\\\nP\\u0302 , which is the number of occurrences of y in Href\\\\ndivided by the sample size |Href |:\\\\nP\\u0302 (y) =\\\\n\\u2211\\\\ny\\u2032\\u2208Href\\\\nI(y = y\\u2032)\\\\n|Href |\\\\n. (6)\\\\nMethod Estimator Objective\\\\nTarget Objective hmodel (Eq. 4) P\\\\n\\u2211\\\\ny\\u2208Y u(h,y)P (y)\\\\nMonte Carlo (MBR) hMC (Eq. 7) P\\u0302\\\\n\\u2211\\\\ny\\u2208Href\\\\nu(h,y)P\\u0302 (y)\\\\nModel-Based (MBMBR) hMB (Eq. 8) P\\u0302MB\\\\n\\u2211\\\\ny\\u2208Href\\\\nu(h,y)P (y)\\\\nTable 1: Comparison of the surrogate objective functions of MBMBR and MBR. MBR uses Monte Carlo estimate\\\\nto approximate the target MBR objective whereas MBMBR uses the model P as is.\\\\nUsing P\\u0302 , we can rewrite Eq. (5) as follows:\\\\nhMC = argmax\\\\nh\\u2208Hcand\\\\nE\\\\ny\\u223cP\\u0302\\\\n[u(h,y)]\\\\n= argmax\\\\nh\\u2208Hcand\\\\n\\u2211\\\\ny\\u2208Y\\\\nu(h,y) \\u00b7 P\\u0302 (y)\\\\n= argmax\\\\nh\\u2208Hcand\\\\n\\u2211\\\\ny\\u2208Href\\\\nu(h,y) \\u00b7 P\\u0302 (y). (7)\\\\nNote that P\\u0302 (y) is zero for y \\u2208 Y \\\\\\\\ Href . Standard\\\\npractice is to use the same set of hypotheses for the\\\\ncandidate set Hcand and the reference pool Href\\\\n(H := Hcand = Href ).\\\\n2.2 Sampling Algorithms for MBR\\\\nThe choice of sampling algorithm to collect H has\\\\nbeen studied extensively, as it has been shown to be\\\\ncritical to the performance of MBR. While most of\\\\nthe classical work on MBR relies on beam search to\\\\ngenerate samples, Eikema and Aziz (2020) propose\\\\nto use unbiased sampling through ancestral sam-\\\\npling (Robert and Casella, 1999). Fernandes et al.\\\\n(2022) have found that MBR with top-k sampling\\\\n(Fan et al., 2018) and nucleus sampling (Holtz-\\\\nman et al., 2020) generates much higher quality\\\\nsequences compared to ancestral sampling. Top-\\\\nk sampling is a simple modification of ancestral\\\\nsampling that truncates all tokens except the M\\\\nmost probable tokens. Similar to top-k sampling,\\\\nnucleus sampling also truncates the lower tail of\\\\nthe probability distribution. Nucleus sampling trun-\\\\ncates all tokens except those in the nucleus, the\\\\nsmallest possible set of tokens that covers a frac-\\\\ntion p of the model probability. Freitag et al. (2023)\\\\nshow that epsilon sampling (Hewitt et al., 2022)\\\\nfurther improves these sampling algorithms on ma-\\\\nchine translation tasks. Epsilon sampling is also a\\\\nvariant of ancestral sampling that truncates tokens\\\\nwhose probability is less than a fixed threshold \\u03f5.\\\\nWhile the empirical distributions of top-k sampling,\\\\nnucleus sampling, and epsilon sampling are biased\\\\nand inconsistent estimators of the model probability\\\\nP , MBR decoding using these algorithms has been\\\\nempirically shown to produce higher quality text\\\\nthan using ancestral sampling, which is unbiased\\\\nand consistent.\\\\n3 Model-Based Minimum Bayes Risk\\\\n(MBMBR) Decoding\\\\nThe estimation error of the Monte Carlo sum comes\\\\nfrom two approximations. First, the domain of\\\\nthe reference hypotheses is restricted to the set\\\\nof sampled sentences Href instead of Y . This ap-\\\\nproximation is necessary because enumerating all\\\\nhypotheses in Y is infeasible. Second, the proba-\\\\nbility of each sentence in Href is estimated by the\\\\nMonte Carlo estimate instead of the true model\\\\nprobability P\\\", \\\"id\\\": null, \\\"title\\\": \\\"Model-Based Minimum Bayes Risk Decoding\\\", \\\"filepath\\\": \\\"2311.05263.pdf\\\", \\\"url\\\": \\\"https://yongaistorageaccount.blob.core.windows.net/arxivpapers/2311.05263.pdf\\\", \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=979. Scores=9.775235Org Highlight count=15.\\\"}, \\\"chunk_id\\\": \\\"3\\\"}, {\\\"content\\\": \\\"76 30.85\\\\nNucleus Sampling (p = 0.9)\\\\n|H| 4 8 16 32 64\\\\nMBR 26.40 29.12 31.12 31.88 32.98\\\\nMBMBR 21.68 24.36 26.84 29.08 31.00\\\\nMBMBRL 21.75 24.31 27.07 29.08 30.95\\\\nAncestral Sampling\\\\n|H| 4 8 16 32 64\\\\nMBR 22.70 25.87 28.08 29.84 31.55\\\\nMBMBR 18.63 20.88 22.93 25.26 27.59\\\\nMBMBRL 18.59 20.48 22.65 25.90 29.52\\\\nTable 8: BLEU scores on MS COCO dataset.\\\\nBERTScore is used as the utility function.\\\\nNoCaps\\\\nEpsilon Sampling (\\u03f5 = 0.02)\\\\n|H| 4 8 16 32 64\\\\nMBR 25.43 27.67 28.73 30.93 31.42\\\\nMBMBR 23.31 24.73 25.87 26.91 27.27\\\\nMBMBRL 28.86 29.78 30.47 32.10 32.68\\\\nTop-k Sampling (k = 10)\\\\n|H| 4 8 16 32 64\\\\nMBR 23.69 25.91 27.41 29.33 30.67\\\\nMBMBR 21.39 23.01 24.05 25.18 25.66\\\\nMBMBRL 26.64 28.63 29.15 31.53 31.58\\\\nNucleus Sampling (p = 0.9)\\\\n|H| 4 8 16 32 64\\\\nMBR 21.58 25.76 27.20 28.55 29.24\\\\nMBMBR 20.89 22.91 25.29 26.70 27.14\\\\nMBMBRL 25.48 28.72 29.72 30.56 31.43\\\\nAncestral Sampling\\\\n|H| 4 8 16 32 64\\\\nMBR 18.48 21.54 23.93 24.89 25.61\\\\nMBMBR 17.42 19.80 22.35 23.40 23.66\\\\nMBMBRL 21.40 25.10 27.57 28.47 29.36\\\\nTable 9: BLEU scores on NoCaps dataset. BERTScore\\\\nis used as the utility function.\\\\n\\\\tIntroduction\\\\n\\\\tBackground\\\\n\\\\tMinimum Bayes Risk Decoding\\\\n\\\\tSampling Algorithms for MBR\\\\n\\\\tModel-Based Minimum Bayes Risk (MBMBR) Decoding\\\\n\\\\tProperties of the Model-Based Estimate\\\\n\\\\tExperiments\\\\n\\\\tMachine Translation\\\\n\\\\tMachine Translation Model\\\\n\\\\tLarge Language Model with Prompting\\\\n\\\\tText Summarization\\\\n\\\\tText Summarization Model\\\\n\\\\tLarge Language Model with Prompting\\\\n\\\\tImage Captioning with BLIP-2\\\\n\\\\tData-to-Text with Few-Shot Learning\\\\n\\\\tRelated Work\\\\n\\\\tConclusions\\\\n\\\\tLimitations\\\\n\\\\tProof of Theorem 4.1\\\\n\\\\tEmpirical Evaluation of the Divergence\\\\n\\\\tEvaluation of Sampling Algorithms\\\", \\\"id\\\": null, \\\"title\\\": \\\"Model-Based Minimum Bayes Risk Decoding\\\", \\\"filepath\\\": \\\"2311.05263.pdf\\\", \\\"url\\\": \\\"https://yongaistorageaccount.blob.core.windows.net/arxivpapers/2311.05263.pdf\\\", \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=670. Scores=9.583577Org Highlight count=16.\\\"}, \\\"chunk_id\\\": \\\"4\\\"}, {\\\"content\\\": \\\"Mauro Cettolo, Marcello Federico, Luisa Bentivogli,\\\\nJan Niehues, Sebastian St\\u00fcker, Katsuhito Sudoh,\\\\nKoichiro Yoshino, and Christian Federmann. 2017.\\\\nOverview of the IWSLT 2017 evaluation campaign.\\\\nIn Proceedings of the 14th International Conference\\\\non Spoken Language Translation, pages 2\\u201314, Tokyo,\\\\nJapan. International Workshop on Spoken Language\\\\nTranslation.\\\\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\\\\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\\\\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\\\\n2022. Scaling instruction-finetuned language models.\\\\narXiv preprint arXiv:2210.11416.\\\\nEldan Cohen and Christopher Beck. 2019. Empirical\\\\nanalysis of beam search performance degradation\\\\nin neural sequence models. In Proceedings of the\\\\n36th International Conference on Machine Learn-\\\\ning, volume 97 of Proceedings of Machine Learning\\\\nResearch, pages 1290\\u20131299. PMLR.\\\\nThomas M. Cover and Joy A. Thomas. 2006. Elements\\\\nof Information Theory. Wiley-Interscience, USA.\\\\nBryan Eikema and Wilker Aziz. 2020. Is MAP decoding\\\\nall you need? the inadequacy of the mode in neural\\\\nmachine translation. In Proceedings of the 28th Inter-\\\\nnational Conference on Computational Linguistics,\\\\npages 4506\\u20134520, Barcelona, Spain (Online). Inter-\\\\nnational Committee on Computational Linguistics.\\\\nBryan Eikema and Wilker Aziz. 2022. Sampling-based\\\\napproximations to minimum Bayes risk decoding\\\\nfor neural machine translation. In Proceedings of\\\\nthe 2022 Conference on Empirical Methods in Natu-\\\\nral Language Processing, pages 10978\\u201310993, Abu\\\\nDhabi, United Arab Emirates. Association for Com-\\\\nputational Linguistics.\\\\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\\\\nHierarchical neural story generation. In Proceedings\\\\nof the 56th Annual Meeting of the Association for\\\\nComputational Linguistics (Volume 1: Long Papers),\\\\npages 889\\u2013898, Melbourne, Australia. Association\\\\nfor Computational Linguistics.\\\\nAnt\\u00f3nio Farinhas, Jos\\u00e9 G. C. de Souza, and Andr\\u00e9 F. T.\\\\nMartins. 2023. An empirical study of translation\\\\nhttps://doi.org/10.18653/v1/D17-1098\\\\nhttps://doi.org/10.18653/v1/D17-1098\\\\nhttps://doi.org/10.18653/v1/W19-5301\\\\nhttps://doi.org/10.18653/v1/W19-5301\\\\nhttps://aclanthology.org/2017.iwslt-1.1\\\\nhttps://proceedings.mlr.press/v97/cohen19a.html\\\\nhttps://proceedings.mlr.press/v97/cohen19a.html\\\\nhttps://proceedings.mlr.press/v97/cohen19a.html\\\\nhttps://doi.org/10.18653/v1/2020.coling-main.398\\\\nhttps://doi.org/10.18653/v1/2020.coling-main.398\\\\nhttps://doi.org/10.18653/v1/2020.coling-main.398\\\\nhttps://doi.org/10.18653/v1/2022.emnlp-main.754\\\\nhttps://doi.org/10.18653/v1/2022.emnlp-main.754\\\\nhttps://doi.org/10.18653/v1/2022.emnlp-main.754\\\\nhttps://doi.org/10.18653/v1/P18-1082\\\\nhttp://arxiv.org/abs/2310.11430\\\\nhypothesis ensembling with large language models.\\\\narXiv preprint arXiv:2310.11430.\\\\nPatrick Fernandes, Ant\\u00f3nio Farinhas, Ricardo Rei,\\\\nJos\\u00e9 G. C. de Souza, Perez Ogayo, Graham Neubig,\\\\nand Andre Martins. 2022. Quality-aware decoding\\\\nfor neural machine translation. In Proceedings of\\\\nthe 2022 Conference of the North American Chap-\\\\nter of the Association for Computational Linguistics:\\\\nHuman Language Technologies, pages 1396\\u20131412,\\\\nSeattle, United States. Association for Computational\\\\nLinguistics\\\", \\\"id\\\": null, \\\"title\\\": \\\"Model-Based Minimum Bayes Risk Decoding\\\", \\\"filepath\\\": \\\"2311.05263.pdf\\\", \\\"url\\\": \\\"https://yongaistorageaccount.blob.core.windows.net/arxivpapers/2311.05263.pdf\\\", \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1022. Scores=7.7302294Org Highlight count=7.\\\"}, \\\"chunk_id\\\": \\\"5\\\"}], \\\"intent\\\": \\\"[\\\\\\\"Model-Based Minimum Bayes Risk Decoding\\\\\\\"]\\\"}\",\n",
      "          \"end_turn\": false\n",
      "        },\n",
      "        {\n",
      "          \"index\": 1,\n",
      "          \"role\": \"assistant\",\n",
      "          \"content\": \"The document \\\"Model-Based Minimum Bayes Risk Decoding\\\" discusses a decoding strategy called Minimum Bayes Risk (MBR) decoding, which is an alternative to beam search decoding in text generation tasks[doc1]. MBR decoding selects a hypothesis from a pool of hypotheses based on the least expected risk under a probability model and a given utility function[doc1]. The document proposes a variant of MBR called Model-Based MBR (MBMBR), which uses the model probability itself as the estimate of the probability distribution instead of a Monte Carlo estimate[doc1]. The document shows analytically and empirically that the model-based estimate is more promising than the Monte Carlo estimate in text generation tasks[doc1]. MBMBR outperforms MBR in several text generation tasks, including machine translation, text summarization, image captioning, and data-to-text generation[doc1]. The document also discusses the limitations of MBMBR, such as the requirement of access to the model probability and the computational cost of MBR decoding[doc1].\",\n",
      "          \"end_turn\": true\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Me: \"Model-Based Minimum Bayes Risk Decoding\" authors\n",
      "Bot: {\n",
      "  \"id\": \"022f2c7d-efab-431f-8ad1-7c1835a53316\",\n",
      "  \"model\": \"gpt-35-turbo-16k\",\n",
      "  \"created\": 1700154283,\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"messages\": [\n",
      "        {\n",
      "          \"index\": 0,\n",
      "          \"role\": \"tool\",\n",
      "          \"content\": \"{\\\"citations\\\": [{\\\"content\\\": \\\"Model-Based Minimum Bayes Risk Decoding\\\\nYuu Jinnai, Tetsuro Morimura, Ukyo Honda, Kaito Ariu, Kenshi Abe\\\\nCyberAgent\\\\n{jinnai_yu,morimura_tetsuro,honda_ukyo,kaito_ariu,abe_kenshi}@cyberagent.co.jp\\\\nAbstract\\\\nMinimum Bayes Risk (MBR) decoding has\\\\nbeen shown to be a powerful alternative to\\\\nbeam search decoding in a variety of text gener-\\\\nation tasks. MBR decoding selects a hypothesis\\\\nfrom a pool of hypotheses that has the least ex-\\\\npected risk under a probability model according\\\\nto a given utility function. Since it is impracti-\\\\ncal to compute the expected risk exactly over\\\\nall possible hypotheses, two approximations\\\\nare commonly used in MBR. First, it integrates\\\\nover a sampled set of hypotheses rather than\\\\nover all possible hypotheses. Second, it esti-\\\\nmates the probability of each hypothesis using\\\\na Monte Carlo estimator. While the first approx-\\\\nimation is necessary to make it computationally\\\\nfeasible, the second is not essential since we\\\\ntypically have access to the model probability\\\\nat inference time. We propose Model-Based\\\\nMBR (MBMBR), a variant of MBR that uses\\\\nthe model probability itself as the estimate of\\\\nthe probability distribution instead of the Monte\\\\nCarlo estimate. We show analytically and em-\\\\npirically that the model-based estimate is more\\\\npromising than the Monte Carlo estimate in\\\\ntext generation tasks. Our experiments show\\\\nthat MBMBR outperforms MBR in several text\\\\ngeneration tasks, both with encoder-decoder\\\\nmodels and with large language models.\\\\n1 Introduction\\\\nOne of the key components of text generation is the\\\\ndecoding strategy, which is the decision rule used to\\\\ngenerate sentences from the model. Beam search is\\\\nthe popular decoding strategy that has been widely\\\\nused in many directed text generation tasks, includ-\\\\ning machine translation (Wu et al., 2016; Ott et al.,\\\\n2019; Wolf et al., 2020), text summarization (Rush\\\\net al., 2015; Narayan et al., 2018), and image cap-\\\\ntioning (Anderson et al., 2017). However, beam\\\\nsearch is known to have several degeneration prob-\\\\nlems. For example, Welleck et al. (2020) report\\\\nthat beam search can yield infinite-length outputs\\\\nthat the model assigns zero probability to.\\\\nMinimum Bayes Risk (MBR) decoding has re-\\\\ncently gained attention as a decoding strategy with\\\\nthe potential to overcome the problems of beam\\\\nsearch (Goodman, 1996; Kumar and Byrne, 2004;\\\\nEikema and Aziz, 2020, 2022; Freitag et al., 2022;\\\\nBertsch et al., 2023). MBR decoding consists of\\\\nthe following steps. First, it samples multiple se-\\\\nquences from the model. Then, it compares each\\\\nsequence to the others according to a utility func-\\\\ntion. Finally, it selects the sequence that maximizes\\\\nthe expected utility over the estimated probability\\\\ndistribution over the sequences.\\\\nPrevious work on MBR decoding uses a Monte\\\\nCarlo estimate to approximate the probability dis-\\\\ntribution since it is an unbiased estimate of the true\\\\nmodel distribution. However, because the number\\\\nof possible hypotheses is enormous compared to\\\\nthe number of samples, the estimation error of the\\\\nMonte Carlo estimate is huge. This can lead to a\\\\nhuge error in the expected utility estimate.\\\\nWe propose Model-Based MBR (MBMBR), a\\\\nvariant of MBR that uses a model-based estimate\\\\nof the MBR objective instead of a Monte Carlo\\\\nestimate. The model-based estimate uses the prob-\\\\nability model itself, but with its domain limited to\\\\nthe set of observed hypotheses. As such, the esti-\\\\nmate is computationally feasible and as accurate as\\\\nthe model for the observed sequences. MBMBR\\\\nis easy to implement and requires only the model\\\\nprobability of the sampled sequences, which can be\\\\nobtained concurrently with the sampling procedure.\\\\nWe first evaluate the model-based estimate and\\\\nshow analytically that the Kullback-Leiber (KL)\\\\ndivergence from the true model probability is guar-\\\\nanteed to be lower than that of the Monte Carlo\\\\nestimate. We also evaluate it empirically on var-\\\\nious text generation tasks and show that the KL\\\\ndivergence of the model-based estimate is signifi-\\\\ncantly lower than that of the Monte Carlo estimate,\\\\nindicating that it is a better estimator for the use of\\\\nMBR\\\", \\\"id\\\": null, \\\"title\\\": \\\"Model-Based Minimum Bayes Risk Decoding\\\", \\\"filepath\\\": \\\"2311.05263.pdf\\\", \\\"url\\\": \\\"https://yongaistorageaccount.blob.core.windows.net/arxivpapers/2311.05263.pdf\\\", \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1004. Scores=5.430578Org Highlight count=6.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}, {\\\"content\\\": \\\"Conversational AI Threads for Visualizing Multidimensional Datasets\\\\nMATT-HEUN HONG, University of North Carolina, USA\\\\nANAMARIA CRISAN, Tableau Research, USA\\\\nE03 E07E04\\\\nP32\\\\nP40\\\\nFig. 1. Excerpts of participants\\u2019 analytic conversations with AI Threads. We show participant conversations in threads\\\\nmodifying the visual encoding (E03, E04, P40), as well as for sequential (E07) and individual (P32) utterances. Excerpts were derived\\\\nfrom participants in crowd-sourced (P#) and interview (E#) studies that we conducted. AI Threads is capable of editing operations\\\\nthat add or remove attributes to encodings and change encoding types, among others. Overall, the chatbot proved itself capable, but\\\\nwe still encountered instances of incorrect responses, including within a continuous thread (P40).\\\\nGenerative Large Language Models (LLMs) show potential in data analysis, yet their full capabilities remain uncharted. Our work\\\\nexplores the capabilities of LLMs for creating and refining visualizations via conversational interfaces. We used an LLM to conduct a\\\\nre-analysis of a prior Wizard-of-Oz study examining the use of chatbots for conducting visual analysis. We surfaced the strengths and\\\\nweaknesses of LLM-driven analytic chatbots, finding that they fell short in supporting progressive visualization refinements. From\\\\nthese findings, we developed AI Threads, a multi-threaded analytic chatbot that enables analysts to proactively manage conversational\\\\ncontext and improve the efficacy of its outputs. We evaluate its usability through a crowdsourced study (\\ud835\\udc5b = 40) and in-depth interviews\\\\nwith expert analysts (\\ud835\\udc5b = 10). We further demonstrate the capabilities of AI Threads on a dataset outside the LLM\\u2019s training corpus.\\\\nOur findings show the potential of LLMs while also surfacing challenges and fruitful avenues for future research.\\\\nCode & Supplemental Materials: https://osf.io/6wxpa/\\\\nCCS Concepts: \\u2022 Human-centered computing \\u2192 Empirical studies in visualization; Natural language interfaces.\\\\nAuthors\\u2019 addresses: Matt-Heun Hong, University of North Carolina, USA, mhh@cs.unc.edu; Anamaria Crisan, Tableau Research, USA, acrisan@tableau.\\\\ncom.\\\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\\\\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\\\\nof this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to\\\\nredistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\\\\n\\u00a9 2023 Association for Computing Machinery.\\\\nManuscript submitted to ACM\\\\nManuscript submitted to ACM 1\\\\nar\\\\nX\\\\niv\\\\n:2\\\\n31\\\\n1.\\\\n05\\\\n59\\\\n0v\\\\n1 \\\\n [\\\\ncs\\\\n.H\\\\nC\\\\n] \\\\n 9\\\\n N\\\\nov\\\\n 2\\\\n02\\\\n3\\\\nhttps://osf.io/6wxpa/\\\\n2 Hong et al.\\\\nAdditional Key Words and Phrases: Visual Analysis, Conversational Interfaces, LLM, GPT, User Study\\\\nACM Reference Format:\\\\nMatt-Heun Hong and Anamaria Crisan. 2023. Conversational AI Threads for Visualizing Multidimensional Datasets. 1, 1 (Novem-\\\\nber 2023), 37 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\\\\n1 INTRODUCTION\\\\nGenerative Pre-trained Transformer (GPT) large language models (LLMs) [48] can assist in data-driven knowledge\\\\nwork by generating analytic code from natural language utterances [2, 14, 47]. These capabilities can support novice\\\\nand expert analysts alike by reducing barriers to conducting preliminary investigations of data [8, 10, 40]. However,\\\\nLLMs do not reflect the first attempts to facilitate analyses through natural language. Prior research on Visual Natural\\\\nLanguage Interfaces (V-NLIs) and conversational agents have explored varying techniques for translating plain language\\\\nutterances into corresponding data transformations and visualization specifications [22, 41, 57, 62, 66]. A recent head-\\\\nto-head comparison of LLMs and several V-NLIs suggests that LLM performance is as capable, and potentially superior,\\\\nto prior systems for resolving ambiguities around user intent behind an utterance [40]\\\", \\\"id\\\": null, \\\"title\\\": \\\"Conversational AI Threads for Visualizing Multidimensional Datasets\\\", \\\"filepath\\\": \\\"2311.05590.pdf\\\", \\\"url\\\": \\\"https://yongaistorageaccount.blob.core.windows.net/arxivpapers/2311.05590.pdf\\\", \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1013. Scores=1.5278064Org Highlight count=1.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}], \\\"intent\\\": \\\"[\\\\\\\"Model-Based Minimum Bayes Risk Decoding\\\\\\\" authors]\\\"}\",\n",
      "          \"end_turn\": false\n",
      "        },\n",
      "        {\n",
      "          \"index\": 1,\n",
      "          \"role\": \"assistant\",\n",
      "          \"content\": \"The authors of the paper titled \\\"Model-Based Minimum Bayes Risk Decoding\\\" are Yuu Jinnai, Tetsuro Morimura, Ukyo Honda, Kaito Ariu, and Kenshi Abe[doc1].\",\n",
      "          \"end_turn\": true\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Me: quit\n",
      "Exiting conversation.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Chat history\n",
    "message_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI assistant that helps people find information.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\\\"Use below context:...[your provided message context here]\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to handle bot responses\n",
    "def chat_with_bot():\n",
    "    while True:\n",
    "        # 0. Get user input\n",
    "        user_input = input(\"You: \")\n",
    "        print(\"Me:\", user_input)\n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Exiting conversation.\")\n",
    "            break  # Exit the loop if user inputs 'quit'\n",
    "\n",
    "        # message_text = [{\"role\": \"user\", \"content\": \"What are the differences between Azure Machine Learning and Azure AI services?\"}]\n",
    "\n",
    "        # Append user input to message history\n",
    "        # message_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        message_text = [{\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            messages=message_text,\n",
    "            deployment_id=deployment_id,\n",
    "            dataSources=[  # camelCase is intentional, as this is the format the API expects\n",
    "                {\n",
    "                    \"type\": \"AzureCognitiveSearch\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": search_endpoint,\n",
    "                        \"key\": search_key,\n",
    "                        \"indexName\": search_index_name,\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=400,\n",
    "        )\n",
    "        # print(completion)\n",
    "        \n",
    "        # 5. Get bot response\n",
    "        # bot_response = completion['choices'][0]['message']['content']\n",
    "        bot_response = completion\n",
    "        print(\"Bot:\", bot_response)\n",
    "\n",
    "        # 6. Append bot response to message history\n",
    "        # message_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "        # print(\"message_history:\", message_history)\n",
    "\n",
    "# Start the conversation\n",
    "print(\"Start chatting with the bot ('quit' to exit):\")\n",
    "chat_with_bot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
