{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 3\n",
    "\n",
    "# Build Question and Answer ChatBot from PDF document \n",
    "\n",
    "* Arxiv AI papers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Embedding on Arxiv AI papers\n",
    "\n",
    "* Pre-processing Embedding was done in '2.3.1_DataPreparationEmbedding.ipynb'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Setup Azure OpenAI - \"2023-09-15-preview\", this version does not support ChatCompletion, but for Embedding it suports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up Azure OpenAI\n",
    "load_dotenv()\n",
    "\n",
    "# openai.api_type = \"azure\"\n",
    "# openai.api_base = \"\" # Api base is the 'Endpoint' which can be found in Azure Portal where Azure OpenAI is created. It looks like https://xxxxxx.openai.azure.com/\n",
    "# openai.api_version = \"2023-07-01-preview\"\n",
    "# openai.api_key = \"\" # Or os.getenv(\"OPENAI_API_KEY\") using local .env file. For more details, please see https://github.com/theskumar/python-dotenv\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "# openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_version = \"2023-09-15-preview\"\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\",\"\").strip()\n",
    "assert API_KEY, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = API_KEY\n",
    "RESOURCE_ENDPOINT = os.getenv(\"OPENAI_API_ENDPOINT\",\"\").strip()\n",
    "assert RESOURCE_ENDPOINT, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in RESOURCE_ENDPOINT.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "# Deployment for embedding\n",
    "DEPLOYMENT_NAME_EMBEDDING = os.getenv('DEPLOYMENT_NAME_EMBEDDING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Load all embeddings and get ready for cosine_similarity compare\n",
    "\n",
    "#### Load all embeddings - we have already process all the embeddings and saved into .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename        True\n",
      "page_number     True\n",
      "page_content    True\n",
      "embedding       True\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Load small embeddings dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your folder containing .csv files\n",
    "folder_path = './data_source/arxiv.org/AI/embedding_output'\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file has a .csv extension\n",
    "    if filename.endswith('.csv'):\n",
    "        # Create the full file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df_per_file = pd.read_csv(file_path, delimiter='\\t')  # Adjust the delimiter if necessary\n",
    "       \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df_per_file)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print the concatenated DataFrame or perform further analysis\n",
    "print(df.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert string representation of an array to NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# convert string to array\n",
    "df[\"embedding\"] = df['embedding'].apply(eval).apply(np.array)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0121987  -0.0319206   0.01821196 ...  0.00610266  0.00498677\n",
      " -0.02164243]\n",
      "[-0.00444533 -0.02816371  0.0076111  ...  0.00585418 -0.0036398\n",
      " -0.02948968]\n",
      "[ 2.48839566e-03 -3.82125872e-05  1.97755042e-02 ... -9.17019881e-03\n",
      " -1.04341458e-03 -2.44363081e-02]\n",
      "[ 0.00171012 -0.01008363  0.02309742 ...  0.00328956  0.00269286\n",
      " -0.03097999]\n",
      "[-0.01112995 -0.03496641  0.01722827 ...  0.01701359 -0.00748705\n",
      " -0.04138005]\n",
      "[-0.00427608 -0.01750331  0.02493823 ...  0.00921052 -0.00648062\n",
      " -0.03149532]\n",
      "[-0.0059232  -0.01444069  0.0191263  ...  0.01421568  0.00407675\n",
      " -0.03285224]\n",
      "[-0.01414606 -0.02144012  0.02106713 ... -0.0195199   0.00549472\n",
      " -0.03594536]\n",
      "[ 0.00452031 -0.03532026  0.03015807 ... -0.00830705  0.01104437\n",
      " -0.03556479]\n",
      "[-0.02255906 -0.02277108 -0.01040318 ... -0.02039645 -0.03918153\n",
      " -0.0073218 ]\n",
      "[-0.02307236 -0.01539069 -0.01627913 ... -0.01960057 -0.02663983\n",
      " -0.00425431]\n",
      "[-0.01230936 -0.03176699 -0.01748448 ... -0.0054244  -0.01869225\n",
      " -0.02078477]\n",
      "[-0.00501806 -0.01657354 -0.00816829 ... -0.00556517 -0.02184251\n",
      " -0.02302733]\n",
      "[-0.00873956 -0.02994043  0.00597076 ... -0.01013955 -0.02569887\n",
      " -0.02899786]\n",
      "[-0.01812947 -0.00723221 -0.00371402 ... -0.00189723 -0.00641036\n",
      " -0.03259388]\n",
      "[-0.00475781 -0.01140052 -0.00453008 ... -0.00500656 -0.01119731\n",
      " -0.02379604]\n",
      "[-0.00686152 -0.02306488  0.00496418 ... -0.00710178 -0.00980823\n",
      " -0.03134676]\n",
      "[-0.02997245 -0.00520335  0.0048489  ...  0.00024922 -0.01056975\n",
      " -0.02393259]\n",
      "[-0.02127064 -0.01920718 -0.00579466 ... -0.01847224 -0.03790555\n",
      " -0.01064945]\n",
      "[-0.0159358   0.0005339   0.00308575 ... -0.02589049 -0.02240151\n",
      " -0.02349528]\n",
      "[-0.02462699  0.00030673  0.02354554 ... -0.04117731 -0.01134156\n",
      " -0.02535252]\n",
      "[-0.02688645 -0.00635149 -0.00282657 ... -0.03793434 -0.00719998\n",
      " -0.02405115]\n",
      "[ 0.00934095  0.00575606 -0.01216837 ... -0.0022183   0.00932\n",
      " -0.02439258]\n",
      "[-0.00115164 -0.00262386 -0.01076724 ... -0.00265522 -0.00664851\n",
      " -0.0316954 ]\n",
      "[ 0.00121891 -0.01774319  0.00718729 ... -0.00684633 -0.00852382\n",
      " -0.02715348]\n",
      "[ 0.00220342 -0.00601592  0.0051752  ... -0.02213416 -0.00715984\n",
      " -0.01804086]\n",
      "[-0.01547396 -0.00292814  0.00801139 ... -0.02366606 -0.01097634\n",
      " -0.04387859]\n",
      "[-0.02485705 -0.01278207  0.02677436 ... -0.01509372 -0.01812607\n",
      " -0.03655129]\n",
      "[-0.03661504 -0.00937187  0.00982853 ... -0.02093846 -0.00866302\n",
      " -0.02257428]\n",
      "[-0.01969589 -0.01412604  0.00299848 ... -0.02254333 -0.02332116\n",
      " -0.03864173]\n",
      "[-0.00387039 -0.01257185 -0.01056754 ... -0.01080944 -0.01879903\n",
      " -0.03994794]\n",
      "[-0.02892934 -0.00669597 -0.00508686 ... -0.01427088 -0.02917849\n",
      " -0.03358018]\n",
      "[-0.01344561 -0.00498355  0.01687589 ... -0.0177438  -0.01972757\n",
      " -0.0350192 ]\n",
      "[ 0.0044791   0.00672405  0.02016136 ...  0.00268386 -0.02078016\n",
      " -0.04495652]\n",
      "[ 0.00120267 -0.00376685  0.02297112 ... -0.00876952 -0.02284545\n",
      " -0.04208812]\n",
      "[ 0.00241047  0.02134485  0.02816334 ... -0.00189697 -0.02532583\n",
      " -0.03907576]\n",
      "[-0.02880276 -0.00569132  0.02392845 ... -0.01089104 -0.00497817\n",
      " -0.03791441]\n",
      "[-0.04325412 -0.0118513   0.02352271 ... -0.02307993 -0.00244048\n",
      " -0.03287645]\n",
      "[ 0.00250802 -0.00384918  0.03119191 ...  0.00501248  0.0038314\n",
      " -0.03711154]\n",
      "[ 0.00096508 -0.00589998  0.02247954 ...  0.00927909 -0.00080662\n",
      " -0.03809312]\n",
      "[ 0.00656112 -0.01240101  0.01159575 ...  0.01379446 -0.00197639\n",
      " -0.04083719]\n",
      "[ 0.01076506 -0.00723559  0.0143486  ...  0.01129862  0.00100404\n",
      " -0.04741528]\n",
      "[ 0.0047783  -0.0094289   0.0054594  ...  0.01281309  0.00259667\n",
      " -0.03263579]\n",
      "[ 0.00951853 -0.01362304  0.0156577  ...  0.00246588  0.00460438\n",
      " -0.04466387]\n",
      "[ 0.01101456  0.01182202  0.01497594 ... -0.00302797 -0.0027985\n",
      " -0.03671521]\n",
      "[ 0.0115909  -0.01649739  0.01520185 ...  0.00914454  0.00024873\n",
      " -0.03349094]\n",
      "[ 0.01908759 -0.00215186  0.01689585 ...  0.01129164  0.00132129\n",
      " -0.02835395]\n",
      "[ 0.0004353   0.00031682  0.01580451 ... -0.00059284 -0.01104788\n",
      " -0.02755374]\n",
      "[ 0.00554419  0.00575892  0.02128971 ... -0.01175721 -0.00933537\n",
      " -0.03100525]\n",
      "[-0.00454411 -0.0115108  -0.00295562 ... -0.00246154 -0.01596637\n",
      " -0.03111104]\n",
      "[ 0.0078322  -0.02177767  0.01814575 ...  0.00854611 -0.00027768\n",
      " -0.04200276]\n",
      "[ 0.00532544 -0.03054431  0.01137273 ...  0.0093204   0.00992902\n",
      " -0.03631913]\n",
      "[ 0.01189894 -0.02095614  0.01246589 ...  0.01674251  0.00235529\n",
      " -0.04292019]\n",
      "[-0.00790395 -0.02306908  0.0042557  ...  0.01524283 -0.00098799\n",
      " -0.04944383]\n",
      "[ 0.01158365 -0.00595241  0.02319584 ...  0.00576327 -0.00158267\n",
      " -0.03717045]\n",
      "[ 1.30372541e-02 -7.39742396e-03  1.43653760e-05 ... -5.57983201e-03\n",
      " -1.21549275e-02 -3.29213589e-02]\n",
      "[ 0.00931543  0.00378861  0.00764107 ... -0.00444487  0.01030161\n",
      " -0.03564413]\n",
      "[ 0.02277616 -0.01209939 -0.01032112 ...  0.0032418   0.01243371\n",
      " -0.0405447 ]\n",
      "[ 0.00646363  0.00332243  0.01937314 ... -0.00419657 -0.00386965\n",
      " -0.04940648]\n",
      "[-0.00143618 -0.01102235  0.02884206 ...  0.00667161 -0.01572158\n",
      " -0.04058295]\n",
      "[ 0.01574405 -0.0087397   0.03006064 ...  0.02534443 -0.00056897\n",
      " -0.04089255]\n",
      "[ 0.02008804  0.00421281  0.01279104 ...  0.01058348  0.00176569\n",
      " -0.04011929]\n",
      "[ 0.00182496 -0.00886535  0.02322251 ...  0.00131857 -0.00516373\n",
      " -0.03956956]\n",
      "[-0.00805057 -0.02196636  0.02226244 ...  0.03107436 -0.00178618\n",
      " -0.03744714]\n",
      "[ 0.01209322  0.00246883  0.00571673 ...  0.01233185 -0.00088216\n",
      " -0.04003466]\n",
      "[ 0.01237195 -0.00107141  0.00706609 ...  0.00489274 -0.00629015\n",
      " -0.03391149]\n",
      "[ 1.01750605e-02 -8.61128420e-03  9.45277791e-03 ... -1.33674766e-03\n",
      "  4.00202516e-05 -4.35893685e-02]\n",
      "[-0.01977837  0.00195351  0.00249019 ... -0.00183097 -0.00418609\n",
      " -0.0312275 ]\n",
      "[-0.00109907 -0.00416037 -0.00157622 ... -0.00907131  0.01795677\n",
      " -0.0231465 ]\n",
      "[ 0.00890701 -0.00493633  0.01067976 ...  0.00951234 -0.00552004\n",
      " -0.04920474]\n",
      "[ 0.01010683 -0.01286324  0.01773996 ...  0.01896975  0.0013199\n",
      " -0.03398157]\n",
      "[ 0.00402328 -0.0051738   0.02018069 ...  0.01487826  0.00296207\n",
      " -0.05428206]\n",
      "[-0.00936656  0.00855934  0.02182283 ... -0.01197611  0.01160034\n",
      " -0.02276923]\n",
      "[-0.01164089 -0.00123226  0.03118221 ...  0.00201181  0.00269349\n",
      " -0.03761441]\n",
      "[-0.0076892  -0.00438832  0.0265053  ... -0.0118004   0.01175831\n",
      " -0.02101902]\n",
      "[-0.02281595 -0.01028109  0.0104689  ... -0.00781168 -0.01298701\n",
      " -0.02587663]\n",
      "[-0.0089263  -0.00143034  0.03123156 ... -0.01382633 -0.00010654\n",
      " -0.026003  ]\n",
      "[-0.01711493 -0.00132026  0.02730371 ... -0.01374863  0.00227934\n",
      " -0.01935453]\n",
      "[-0.00427006 -0.01118399  0.0232073  ... -0.0073161  -0.0450437\n",
      " -0.03242587]\n",
      "[-0.02782938 -0.00286856  0.02606838 ...  0.00208512 -0.02014753\n",
      " -0.03153165]\n",
      "[-0.03943584 -0.00736319  0.03400307 ... -0.01141019 -0.03061103\n",
      " -0.03063861]\n",
      "[-0.01902011  0.00562615  0.01915784 ... -0.00265813 -0.0174638\n",
      " -0.03602941]\n",
      "[-0.01320995  0.00343616  0.01471416 ...  0.00489403 -0.02928574\n",
      " -0.04938939]\n",
      "[ 0.00139656 -0.01530859  0.02241514 ... -0.01198705 -0.03011157\n",
      " -0.0486785 ]\n",
      "[-0.02711126  0.00783983  0.01823412 ...  0.00193526 -0.02342775\n",
      " -0.04044814]\n",
      "[-0.02619531 -0.00146405  0.01732963 ...  0.00908732 -0.01193403\n",
      " -0.0523075 ]\n",
      "[-0.01548366 -0.01324514  0.0196163  ... -0.00750076 -0.04981219\n",
      " -0.02396936]\n",
      "[-0.0306505   0.00745664  0.00013239 ...  0.00260227 -0.03342442\n",
      " -0.04235042]\n",
      "[-0.03836987 -0.00484461  0.01856294 ... -0.00063106 -0.0271464\n",
      " -0.03013196]\n",
      "[-0.01117518 -0.01263399  0.0006764  ... -0.00081846 -0.01422173\n",
      " -0.02178041]\n",
      "[-0.03925081 -0.00546018  0.01907158 ... -0.00232892 -0.05160545\n",
      " -0.04396546]\n",
      "[-0.01476202  0.00730606  0.01443365 ...  0.00195946 -0.02422741\n",
      " -0.0526807 ]\n",
      "[-0.03382529 -0.00793658  0.02635226 ... -0.00117644 -0.02986403\n",
      " -0.03621329]\n",
      "[-0.03090438 -0.0201209   0.03065648 ... -0.01237415 -0.02094722\n",
      " -0.02797094]\n",
      "[-0.03893031 -0.00696164  0.02540484 ... -0.0023577  -0.04433502\n",
      " -0.03983567]\n",
      "[-0.01413138 -0.01672442  0.01067399 ... -0.00849255 -0.04072032\n",
      " -0.03161038]\n",
      "[-0.01020829 -0.00577067  0.01645955 ... -0.01382855 -0.03210524\n",
      " -0.02423329]\n",
      "[-0.0309797  -0.01773554  0.01784377 ...  0.01370412 -0.02997861\n",
      " -0.02846345]\n",
      "[-0.03241034 -0.01118074  0.01617756 ...  0.01214698 -0.03456367\n",
      " -0.03103   ]\n",
      "[-0.02543871 -0.02192507  0.00373851 ... -0.00586425 -0.02295105\n",
      " -0.03032968]\n",
      "[-0.02126476 -0.01589941  0.02623684 ...  0.00743705 -0.02286594\n",
      " -0.02858243]\n",
      "[-0.02703675 -0.01300458  0.01739961 ...  0.00532542 -0.02030186\n",
      " -0.03613232]\n",
      "[-0.01718194 -0.01202736  0.00910993 ... -0.00117731 -0.02347964\n",
      " -0.04098418]\n",
      "[-0.02736063 -0.00168538  0.0068789  ... -0.00886896 -0.02386659\n",
      " -0.04285842]\n",
      "[-0.02965793 -0.01509452  0.02160752 ... -0.0199024  -0.0462619\n",
      " -0.02921068]\n",
      "[-0.02085689 -0.01367922  0.02414864 ... -0.01357678 -0.02518671\n",
      " -0.02671648]\n",
      "[-0.01879874 -0.01404369  0.0262601  ... -0.00729524 -0.03280782\n",
      " -0.03164501]\n",
      "[-0.01193265  0.00299342  0.02018424 ... -0.00962001 -0.02920214\n",
      " -0.04918111]\n",
      "[-0.00868819 -0.0047605   0.0216393  ...  0.00278784 -0.04974357\n",
      " -0.04214935]\n",
      "[-0.02526436  0.0090998   0.01749176 ... -0.01185628 -0.03446624\n",
      " -0.05112765]\n",
      "[-0.01721004 -0.00662523  0.024408   ...  0.00307751 -0.04409277\n",
      " -0.04963619]\n",
      "[-0.00499159 -0.01543041  0.02828451 ...  0.01021613 -0.03938454\n",
      " -0.03072377]\n",
      "[-0.02291236  0.00995522  0.02602598 ...  0.00176625 -0.04138466\n",
      " -0.03948577]\n",
      "[-0.02028269 -0.00360704  0.01247205 ...  0.01019684 -0.03743003\n",
      " -0.04375623]\n",
      "[ 0.00977969 -0.00096969  0.0232863  ... -0.01289926 -0.04925045\n",
      " -0.02931838]\n",
      "[ 0.00712328  0.02519274  0.00664197 ... -0.01460409 -0.02000843\n",
      " -0.00420108]\n",
      "[-0.01511878  0.00542908  0.01299863 ... -0.00943569 -0.01240255\n",
      " -0.01746246]\n",
      "[-0.00133981  0.01170258  0.01127039 ... -0.01586498 -0.00895647\n",
      " -0.02498768]\n",
      "[-0.00730097  0.01960596  0.02354356 ... -0.01696722 -0.01890868\n",
      " -0.02035793]\n",
      "[-0.01015894  0.02349085  0.01276677 ... -0.01322978 -0.01519756\n",
      " -0.04423089]\n",
      "[-0.02567464  0.01006262  0.02118228 ... -0.01691244 -0.01919339\n",
      " -0.02575809]\n",
      "[-0.00286255  0.01212746  0.02862554 ... -0.01488528 -0.01974464\n",
      " -0.023836  ]\n",
      "[-0.00074237  0.00918112  0.00468542 ... -0.01910758 -0.00701289\n",
      " -0.02386415]\n",
      "[-0.01943893  0.01853734  0.02495779 ... -0.0190701  -0.01968482\n",
      " -0.0190701 ]\n",
      "[ 0.00455237  0.01867394  0.02223933 ... -0.02331851 -0.0037703\n",
      " -0.02386494]\n",
      "[-0.02245762  0.02366046  0.02955166 ... -0.00797567 -0.015924\n",
      " -0.02197921]\n",
      "[-0.00953053  0.02029434  0.02015731 ... -0.01200394 -0.00972237\n",
      " -0.01782778]\n",
      "[-0.01204527  0.01013441  0.01418134 ... -0.01441338 -0.00945878\n",
      " -0.02852648]\n",
      "[ 0.00248041  0.00948127  0.01018853 ... -0.00534447 -0.00534447\n",
      " -0.01527946]\n",
      "[ 0.00307535  0.01632461  0.01157179 ... -0.02948521 -0.01390387\n",
      " -0.02967614]\n",
      "[-0.00190912  0.00839666  0.03220446 ... -0.00549756 -0.00362127\n",
      " -0.03673796]\n",
      "[ 0.00636719  0.01207943  0.01592135 ... -0.02135001 -0.02094488\n",
      " -0.01983755]\n",
      "[ 0.02111754  0.00155301  0.01602537 ... -0.01960623 -0.01906161\n",
      " -0.01633852]\n",
      "[-0.01248578  0.00862629  0.01482856 ... -0.0137655  -0.01322382\n",
      " -0.02816748]\n",
      "[-3.65368178e-05  1.62638426e-02  1.70736033e-02 ... -2.05185190e-02\n",
      " -1.54815316e-02 -2.63240933e-02]\n",
      "[ 0.00903911  0.01495537  0.03117569 ... -0.01486314 -0.00349508\n",
      " -0.02438977]\n",
      "[ 0.01075409  0.01147277  0.02146897 ... -0.01117223 -0.0031181\n",
      " -0.01991401]\n",
      "[-0.00996367  0.01141269  0.0116121  ...  0.02081802 -0.0236363\n",
      " -0.02610894]\n",
      "[ 0.01361339  0.02563151  0.01926385 ... -0.01845952 -0.02445182\n",
      " -0.03383574]\n",
      "[-0.012957    0.01908438  0.03814121 ... -0.01905684 -0.01872637\n",
      " -0.03230299]\n"
     ]
    }
   ],
   "source": [
    "df[\"embedding\"]\n",
    "# Printing all members in df[\"embedding\"]\n",
    "for embed in df[\"embedding\"]:\n",
    "    print(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Create functions\n",
    "\n",
    "* Embedding for user question, \n",
    "\n",
    "* Similarity comparision between user input embedding and previous processed embeddings (Arxiv papers in AI) stored in DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding user Question input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding user input text so that we will use it later for similiarity search\n",
    "def user_input_text_embedding(user_input):\n",
    "    user_input_embedding = openai.Embedding().create(input=[user_input], deployment_id=DEPLOYMENT_NAME_EMBEDDING)\n",
    "    return user_input_embedding['data'][0]['embedding']\n",
    "\n",
    "# text = 'the quick brown fox jumped over the lazy dog'\n",
    "# user_input_text_embedding(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity search and find the DataFrame record with the highest similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import cosine_similarity\n",
    "debug = False\n",
    "def cosine_similarity_search(question_embedding, df):\n",
    "    highest_score = 0\n",
    "    embedding_record_index = 0\n",
    "    # question_embedding = df['embedding'][0]\n",
    "    for i in range(len(df)):\n",
    "        # df['embedding'][i]\n",
    "        if debug: print(i)\n",
    "        score = cosine_similarity(question_embedding, df['embedding'][i])\n",
    "        if (score > highest_score):\n",
    "            if debug: print(f\"highest_score: {highest_score}, score: {score}\")\n",
    "            highest_score = score\n",
    "            embedding_record_index = i\n",
    "    \n",
    "    # Return page content at with the highest similarity\n",
    "    # return df['page_content'][embedding_record_index]\n",
    "    print(\">>> internal msg: cosine_similarity_search(question_embedding, df): highest_score: \", highest_score)\n",
    "    return embedding_record_index\n",
    "\n",
    "# text = 'the quick brown fox jumped over the lazy dog'\n",
    "# user_question_embedding = user_input_text_embedding(text)\n",
    "# relevant_content = cosine_similarity_search(user_question_embedding, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Biuld a Q&A ChatBot based on the content stored in DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Azure OpenAI\n",
    "\n",
    "* Note: the reason to set this again is that the 'gpt-35-turbo-instruct-0914' does not support ChatCompletion, but Completion. So, had to use 'gpt-35-turbo' to support ChatCompletion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: The openai-python library support for Azure OpenAI is in preview.\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "# Set up Azure OpenAI\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "\n",
    "CHAT_API_KEY = os.getenv(\"OPENAI_API_CHAT_KEY\",\"\").strip()\n",
    "assert CHAT_API_KEY, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = CHAT_API_KEY\n",
    "\n",
    "CHAT_RESOURCE_ENDPOINT = os.getenv(\"OPENAI_API_CHAT_ENDPOINT\",\"\").strip()\n",
    "assert CHAT_RESOURCE_ENDPOINT, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in CHAT_RESOURCE_ENDPOINT.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = CHAT_RESOURCE_ENDPOINT\n",
    "\n",
    "# Deployment for Chat\n",
    "# DEPLOYMENT_NAME_CHAT = os.getenv('DEPLOYMENT_NAME_CHAT')\n",
    "DEPLOYMENT_NAME_CHAT = os.getenv('DEPLOYMENT_NAME_CHAT_16K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt options ...\n",
    "\n",
    "* Build different prompts based on strategies, their tactics, and user own data (here use the result from cosine_similarity comparesion)\n",
    "\n",
    "* There are plenty of prompt examples: https://platform.openai.com/docs/guides/prompt-engineering\n",
    "\n",
    "For instance:\n",
    "\n",
    "Tactic: Instruct the model to answer with citations from a reference text\n",
    "If the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.\n",
    "\n",
    "SYSTEM\n",
    "You will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \"Insufficient information.\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\"citation\": …}).\n",
    "USER\n",
    "\"\"\"<insert document here>\"\"\"\n",
    "\n",
    "Question: <insert question here>\n",
    "\n",
    "\n",
    "\"What significant paradigm shifts have occurred in the history of artificial intelligence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build different prompts based on strategies, their tactics, and user own data (here use the result from cosine_similarity comparesion\n",
    "# TODO: more experiments are needed to 'fine-tune' the prompt to get better results ...\n",
    "\n",
    "def prompt_engineering(tactic, relevant_content_index, user_question):\n",
    "    system_content = ''\n",
    "    user_content = ''\n",
    "    relevant_content = df['page_content'][relevant_content_index]\n",
    "\n",
    "    if (tactic == 1):\n",
    "        system_content = \\\n",
    "        \"\"\"You will be provided with a document delimited by triple quotes and a question. \n",
    "            Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. \n",
    "            If the document does not contain the information needed to answer this question then simply write: 'Insufficient information.' \n",
    "            If an answer to the question is provided, it must be annotated with a citation. Use the following format for citing relevant passages ({'citation': …}).\"\"\"        \n",
    "        user_content = f'\"\"\"{relevant_content}\"\"\"\\n\\nQuestion: {user_question}'\n",
    "    elif (tactic == 2):\n",
    "        system_content = \"You are an AI assistant that helps people find information. \"\n",
    "        user_content = f\"Using below context: \\n{relevant_content}\\nAnswer the following question: {user_question}\"\n",
    "    elif (tactic == 3):\n",
    "        system_content = \"You are an AI assistant that helps people find information. If the document does not contain the information needed to answer this question then simply write: 'Insufficient information.' \"\n",
    "        user_content = f\"Using below context: \\n{relevant_content}\\nAnswer the following question: {user_question}\"       \n",
    "        \n",
    "    message_text = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_content        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_content\n",
    "        }\n",
    "    ]\n",
    "    print(\">>> internal msg: prompt_engineering(tactic, relevant_content_index, user_question): message_text: \", message_text)\n",
    "    return message_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This the main loop for ChatBot conversation till user say 'quit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot ('quit' to exit):\n",
      "Me:  \"Model-Based Minimum Bayes Risk Decoding\"\n",
      ">>> internal msg: cosine_similarity_search(question_embedding, df): highest_score:  0.9016991761938595\n",
      ">>> internal msg: prompt_engineering(tactic, relevant_content_index, user_question): message_text:  [{'role': 'system', 'content': \"You are an AI assistant that helps people find information. If the document does not contain the information needed to answer this question then simply write: 'Insufficient information.' \"}, {'role': 'user', 'content': 'Using below context: \\nModel-Based Minimum Bayes Risk Decoding Yuu Jinnai, Tetsuro Morimura, Ukyo Honda, Kaito Ariu, Kenshi Abe CyberAgent {jinnai_yu, morimura_tetsuro, honda_ukyo,kaito_ariu, abe_kenshi}@cyberagent.co.jp Abstract Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative to beam search decoding in a variety of text gener- ation tasks. MBR decoding selects a hypothesis from a pool of hypotheses that has the least ex- pected risk under a probability model according to a given utility function. Since it is impracti- cal to compute the expected risk exactly over all possible hypotheses, two approximations are commonly used in MBR. First, it integrates over a sampled set of hypotheses rather than over all possible hypotheses. Second, it esti- mates the probability of each hypothesis using a Monte Carlo estimator. While the first approx- imation is necessary to make it computationally feasible, the second is not essential since we typically have access to the model probability at inference time. We propose Model-Based MBR (MBMBR), a variant of MBR that uses the model probability itself as the estimate of the probability distribution instead of the Monte Carlo estimate. We show analytically and em- pirically that the model-based estimate is more promising than the Monte Carlo estimate in text generation tasks. Our experiments show that MBMBR outperforms MBR in several text generation tasks, both with encoder-decoder models and with large language models. arXiv:2311.05263v1 [cs.AI] 9 Nov 2023 1 Introduction One of the key components of text generation is the decoding strategy, which is the decision rule used to generate sentences from the model. Beam search is the popular decoding strategy that has been widely used in many directed text generation tasks, includ- ing machine translation (Wu et al., 2016; Ott et al., 2019; Wolf et al., 2020), text summarization (Rush et al., 2015; Narayan et al., 2018), and image cap- tioning (Anderson et al., 2017). However, beam search is known to have several degeneration prob- lems. For example, Welleck et al. (2020) report that beam search can yield infinite-length outputs that the model assigns zero probability to. Minimum Bayes Risk (MBR) decoding has re- cently gained attention as a decoding strategy with the potential to overcome the problems of beam search (Goodman, 1996; Kumar and Byrne, 2004; Eikema and Aziz, 2020, 2022; Freitag et al., 2022; Bertsch et al., 2023). MBR decoding consists of the following steps. First, it samples multiple se- quences from the model. Then, it compares each sequence to the others according to a utility func- tion. Finally, it selects the sequence that maximizes the expected utility over the estimated probability distribution over the sequences. Previous work on MBR decoding uses a Monte Carlo estimate to approximate the probability dis- tribution since it is an unbiased estimate of the true model distribution. However, because the number of possible hypotheses is enormous compared to the number of samples, the estimation error of the Monte Carlo estimate is huge. This can lead to a huge error in the expected utility estimate. We propose Model-Based MBR (MBMBR), a variant of MBR that uses a model-based estimate of the MBR objective instead of a Monte Carlo estimate. The model-based estimate uses the prob- ability model itself, but with its domain limited to the set of observed hypotheses. As such, the esti- mate is computationally feasible and as accurate as the model for the observed sequences. MBMBR is easy to implement and requires only the model probability of the sampled sequences, which can be obtained concurrently with the sampling procedure. We first evaluate the model-based estimate and show analytically that the Kullback-Leiber (KL) divergence from the true model probability is guar- anteed to be lower than that of the Monte Carlo estimate. We also evaluate it empirically on var- ious text generation tasks and show that the KL divergence of the model-based estimate is signifi- cantly lower than that of the Monte Carlo estimate, indicating that it is a better estimator for the use of MBR.\\nAnswer the following question: \"Model-Based Minimum Bayes Risk Decoding\"'}]\n",
      "Bot:  The document discusses a decoding strategy called \"Model-Based Minimum Bayes Risk Decoding\" (MBMBR). MBMBR is a variant of Minimum Bayes Risk (MBR) decoding that uses the model probability itself as the estimate of the probability distribution, instead of using a Monte Carlo estimate. The document explains that MBMBR has shown promising results in text generation tasks and outperforms MBR in several scenarios.\n",
      "citation: file name: 2311.05263.pdf, page number: 1\n",
      "Me:  \"Model-Based Minimum Bayes Risk Decoding\" authors\n",
      ">>> internal msg: cosine_similarity_search(question_embedding, df): highest_score:  0.8778186003150849\n",
      ">>> internal msg: prompt_engineering(tactic, relevant_content_index, user_question): message_text:  [{'role': 'system', 'content': \"You are an AI assistant that helps people find information. If the document does not contain the information needed to answer this question then simply write: 'Insufficient information.' \"}, {'role': 'user', 'content': 'Using below context: \\nModel-Based Minimum Bayes Risk Decoding Yuu Jinnai, Tetsuro Morimura, Ukyo Honda, Kaito Ariu, Kenshi Abe CyberAgent {jinnai_yu, morimura_tetsuro, honda_ukyo,kaito_ariu, abe_kenshi}@cyberagent.co.jp Abstract Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative to beam search decoding in a variety of text gener- ation tasks. MBR decoding selects a hypothesis from a pool of hypotheses that has the least ex- pected risk under a probability model according to a given utility function. Since it is impracti- cal to compute the expected risk exactly over all possible hypotheses, two approximations are commonly used in MBR. First, it integrates over a sampled set of hypotheses rather than over all possible hypotheses. Second, it esti- mates the probability of each hypothesis using a Monte Carlo estimator. While the first approx- imation is necessary to make it computationally feasible, the second is not essential since we typically have access to the model probability at inference time. We propose Model-Based MBR (MBMBR), a variant of MBR that uses the model probability itself as the estimate of the probability distribution instead of the Monte Carlo estimate. We show analytically and em- pirically that the model-based estimate is more promising than the Monte Carlo estimate in text generation tasks. Our experiments show that MBMBR outperforms MBR in several text generation tasks, both with encoder-decoder models and with large language models. arXiv:2311.05263v1 [cs.AI] 9 Nov 2023 1 Introduction One of the key components of text generation is the decoding strategy, which is the decision rule used to generate sentences from the model. Beam search is the popular decoding strategy that has been widely used in many directed text generation tasks, includ- ing machine translation (Wu et al., 2016; Ott et al., 2019; Wolf et al., 2020), text summarization (Rush et al., 2015; Narayan et al., 2018), and image cap- tioning (Anderson et al., 2017). However, beam search is known to have several degeneration prob- lems. For example, Welleck et al. (2020) report that beam search can yield infinite-length outputs that the model assigns zero probability to. Minimum Bayes Risk (MBR) decoding has re- cently gained attention as a decoding strategy with the potential to overcome the problems of beam search (Goodman, 1996; Kumar and Byrne, 2004; Eikema and Aziz, 2020, 2022; Freitag et al., 2022; Bertsch et al., 2023). MBR decoding consists of the following steps. First, it samples multiple se- quences from the model. Then, it compares each sequence to the others according to a utility func- tion. Finally, it selects the sequence that maximizes the expected utility over the estimated probability distribution over the sequences. Previous work on MBR decoding uses a Monte Carlo estimate to approximate the probability dis- tribution since it is an unbiased estimate of the true model distribution. However, because the number of possible hypotheses is enormous compared to the number of samples, the estimation error of the Monte Carlo estimate is huge. This can lead to a huge error in the expected utility estimate. We propose Model-Based MBR (MBMBR), a variant of MBR that uses a model-based estimate of the MBR objective instead of a Monte Carlo estimate. The model-based estimate uses the prob- ability model itself, but with its domain limited to the set of observed hypotheses. As such, the esti- mate is computationally feasible and as accurate as the model for the observed sequences. MBMBR is easy to implement and requires only the model probability of the sampled sequences, which can be obtained concurrently with the sampling procedure. We first evaluate the model-based estimate and show analytically that the Kullback-Leiber (KL) divergence from the true model probability is guar- anteed to be lower than that of the Monte Carlo estimate. We also evaluate it empirically on var- ious text generation tasks and show that the KL divergence of the model-based estimate is signifi- cantly lower than that of the Monte Carlo estimate, indicating that it is a better estimator for the use of MBR.\\nAnswer the following question: \"Model-Based Minimum Bayes Risk Decoding\" authors'}]\n",
      "Bot:  The authors of \"Model-Based Minimum Bayes Risk Decoding\" are Yuu Jinnai, Tetsuro Morimura, Ukyo Honda, Kaito Ariu, and Kenshi Abe.\n",
      "citation: file name: 2311.05263.pdf, page number: 1\n",
      "Me:  Please summarize this paper \"Accuracy of a Vision-Language Model on Challenging Medical Cases\"\n",
      ">>> internal msg: cosine_similarity_search(question_embedding, df): highest_score:  0.9210763380371784\n",
      ">>> internal msg: prompt_engineering(tactic, relevant_content_index, user_question): message_text:  [{'role': 'system', 'content': \"You are an AI assistant that helps people find information. If the document does not contain the information needed to answer this question then simply write: 'Insufficient information.' \"}, {'role': 'user', 'content': 'Using below context: \\nAccuracy of a Vision-Language Model on Challenging Medical Cases Thomas Buckley1, James A. Diao, B.S.1, Adam Rodman, M.D.2, and Arjun K. Manrai, Ph.D.1* 1 Department of Biomedical Informatics, Harvard Medical School, Boston, MA 2 Department of Medicine, Beth Israel Deaconess Medical Center, Boston, MA *Correspondence: Arjun K. Manrai, Ph.D. Department of Biomedical Informatics, Harvard Medical School 10 Shattuck St., Boston, MA, 02115 Arjun Manrai@hms.harvard.edu 1\\nAnswer the following question: Please summarize this paper \"Accuracy of a Vision-Language Model on Challenging Medical Cases\"'}]\n",
      "Bot:  Insufficient information.\n",
      "citation: file name: 2311.05591.pdf, page number: 1\n",
      "Me:  \"Accuracy of a Vision-Language Model on Challenging Medical Cases\" authors\n",
      ">>> internal msg: cosine_similarity_search(question_embedding, df): highest_score:  0.9304476223552508\n",
      ">>> internal msg: prompt_engineering(tactic, relevant_content_index, user_question): message_text:  [{'role': 'system', 'content': \"You are an AI assistant that helps people find information. If the document does not contain the information needed to answer this question then simply write: 'Insufficient information.' \"}, {'role': 'user', 'content': 'Using below context: \\nAccuracy of a Vision-Language Model on Challenging Medical Cases Thomas Buckley1, James A. Diao, B.S.1, Adam Rodman, M.D.2, and Arjun K. Manrai, Ph.D.1* 1 Department of Biomedical Informatics, Harvard Medical School, Boston, MA 2 Department of Medicine, Beth Israel Deaconess Medical Center, Boston, MA *Correspondence: Arjun K. Manrai, Ph.D. Department of Biomedical Informatics, Harvard Medical School 10 Shattuck St., Boston, MA, 02115 Arjun Manrai@hms.harvard.edu 1\\nAnswer the following question: \"Accuracy of a Vision-Language Model on Challenging Medical Cases\" authors'}]\n",
      "Bot:  The authors of the document \"Accuracy of a Vision-Language Model on Challenging Medical Cases\" are Thomas Buckley, James A. Diao, Adam Rodman, and Arjun K. Manrai.\n",
      "citation: file name: 2311.05591.pdf, page number: 1\n",
      "Me:  what is \"META4: Metaphoric Gesture Generation\" about?\n",
      ">>> internal msg: cosine_similarity_search(question_embedding, df): highest_score:  0.8928677355518925\n",
      ">>> internal msg: prompt_engineering(tactic, relevant_content_index, user_question): message_text:  [{'role': 'system', 'content': \"You are an AI assistant that helps people find information. If the document does not contain the information needed to answer this question then simply write: 'Insufficient information.' \"}, {'role': 'user', 'content': 'Using below context: \\nMETA4: SEMANTICALLY-ALIGNED GENERATION OF METAPHORIC GESTURES USING SELF-SUPERVISED TEXT AND SPEECH REPRESENTATIONS Mireille Fares ISIR, STMS Sorbonne University Paris, France Catherine Pelachaud ISIR, CNRS Sorbonne University Paris, France Nicolas Obin STMS Sorbonne University Paris, France ABSTRACT Image Schemas are repetitive cognitive patterns that influence the way we conceptualize and reason about various concepts present in speech. These patterns are deeply embedded within our cognitive processes and are reflected in our bodily expressions including gestures. Particularly, metaphoric gestures possess essential characteristics and semantic meanings that align with Image Schemas, to visually represent abstract concepts. The shape and form of gestures can convey abstract concepts, such as extending one\\'s forearm and hand or tracing a line with hand movements to visually represent the image schema of \"PATH\". Previous behavior generation models have primarily focused on uti- lizing speech (acoustic features and text) to drive the generation model of virtual agents. They have not considered key semantic information as those carried by Image Schemas to effectively generate metaphoric gestures. To address this limitation, we introduce META4, a deep-learning approach that generates metaphoric gestures from both speech and Image Schemas. Our approach has two pri- mary goals: (1) computing Image Schemas from input text to capture the underlying semantic and metaphorical meaning, and (2) generating metaphoric gestures driven by speech and the computed image schemas. We make two key contributions: (1) BERTIS, a model that computes Image Schema tags from text input, and (2) META4, which makes use of BERTIS to model and synthesize the cor- responding metaphoric gestures from speech and the generated Image Schemas. To the best of our knowledge, our approach is the first method for generating speech-driven metaphoric gestures while leveraging the potential of Image Schemas. We present evaluation studies to demonstrate the effec- tiveness of our approach and highlight the importance of both speech and image schemas in model- ing metaphoric gestures. Link to code, data and videos: https://github.com/mireillefares/META4/. arXiv:2311.05481v1 [cs.AI] 9 Nov 2023 1 Introduction In the realm of human communication, our cognitive processes play a vital role in shaping the way we conceptualize and reason about various ideas and concepts. One significant aspect of this cognitive influence is manifested through repetitive patterns known as \"Image Schemas\". These patterns, as described by Johnson et al. [Johnson(2005)], exert a profound impact on our understanding and expression of concepts within speech. They are deeply in- grained in our cognitive processes and find expression in our bodily movements and gestures, as observed by Cienki [Cienki and Müller(2008)]. Of particular interest are metaphoric gestures, which are gestures that symbolically repre- sent a concept, object, or event [McNeill and Levy(1982)]. These gestures possess inherent characteristics and seman- tic meanings that align with Image Schemas. They serve as visual representations of abstract concepts [Kendon(2004)], employing specific shapes and forms to convey complex ideas. The conveyed image is a visual representation that is associated with something concrete and actionable in the world. For example, one can sweep his/her flat hand through space, or trace a surface, to visually represent the image schema \"SURFACE\". When a speaker discusses the promotion of an individual in an organization, he/she may use a metaphoric gesture such as raising his/her hand upward to symbolize the promotion. Despite the significant role that Image Schemas and metaphoric gestures play in conveying abstract concepts, previous gesture generation models [Ravenet et al.(2018), Kucherenko et al.(2020),\\nAnswer the following question: what is \"META4: Metaphoric Gesture Generation\" about?'}]\n",
      "Bot:  \"META4: Metaphoric Gesture Generation\" is about a deep-learning approach that generates metaphoric gestures from both speech and Image Schemas. The approach aims to compute Image Schemas from input text to capture the underlying semantic and metaphorical meaning, and then generate metaphoric gestures driven by speech and the computed Image Schemas. The paper introduces two key contributions: BERTIS, a model that computes Image Schema tags from text input, and META4, which utilizes BERTIS to model and synthesize the corresponding metaphoric gestures from speech and the generated Image Schemas. The effectiveness of the approach is evaluated and the importance of both speech and Image Schemas in modeling metaphoric gestures is highlighted.\n",
      "citation: file name: 2311.05481.pdf, page number: 1\n",
      "Me:  Could explain \"Width-based search methods\"\n",
      ">>> internal msg: cosine_similarity_search(question_embedding, df): highest_score:  0.8146783006147736\n",
      ">>> internal msg: prompt_engineering(tactic, relevant_content_index, user_question): message_text:  [{'role': 'system', 'content': \"You are an AI assistant that helps people find information. If the document does not contain the information needed to answer this question then simply write: 'Insufficient information.' \"}, {'role': 'user', 'content': 'Using below context: \\nGeneral Policies, Subgoal Structure, and Planning Width Blai Bonet Universitat Pompeu Fabra, Spain Hector Geffner RWTH Aachen University, Germany Linköping University, Sweden BONETBLAI@GMAIL.COM HECTOR.GEFFNER@ML.RWTH-AACHEN.DE Abstract It has been observed that many classical planning domains with atomic goals can be solved by means of a simple polynomial exploration procedure, called IW, that runs in time exponential in the problem width, which in these cases is bounded and small. Yet, while the notion of width has become part of state-of-the-art planning algorithms such as BFWS, there is no good explanation for why so many benchmark domains have bounded width when atomic goals are considered. In this work, we address this question by relating bounded width with the existence of general optimal policies that in each planning instance are represented by tuples of atoms of bounded size. We also define the notions of (explicit) serializations and serialized width that have a broader scope as many domains have a bounded serialized width but no bounded width. Such problems are solved non-optimally in polynomial time by a suitable variant of the Serialized IW algorithm. Finally, the language of general policies and the semantics of serializations are combined to yield a simple, meaningful, and expressive language for specifying serializations in compact form in the form of sketches, which can be used for encoding domain control knowledge by hand or for learning it from small examples. Sketches express general problem decompositions in terms of subgoals, and sketches of bounded width express problem decompositions that can be solved in polynomial time.1 arXiv:2311.05490v1 [cs.AI] 9 Nov 2023 1. Introduction Width-based search methods exploit the structure of states to enumerate the state space in ways that are different than \"blind\" search methods such as breadth-first and depth-first (Lipovetzky & Geffner, 2012). This is achieved by associating a non-negative integer to each state generated in the search, a so-called novelty measure, which is defined by the size of the smallest factor in the state that has not been seen in previously generated states. States are deemed more novel and hence preferred in the exploration search when this novelty measure is smaller. Other types of novelty measures have been used in reinforcement learning for dealing with sparse rewards and in genetic algorithms for dealing with local minima (Tang, Houthooft, Foote, Stooke, Chen, Duan, Schulman, DeTurck, & Abbeel, 2017; Pathak, Agrawal, Efros, & Darrell, 2017; Ostrovski, Bellemare, Oord, & Munos, 2017), but the results are mostly empirical. In classical planning, where novelty measures are part of state-of-the-art search algorithms (Lipovetzky & Geffner, 2017b, 2017a), there is a solid 1. This paper is a heavily revised version of Bonet and Geffner (2021). While the key notions of problem width, general policies, and sketches are from that paper, the relations among these notions have been reworked to make them more meaningful and transparent. The paper also contains many new results. A summary of them and a discussion of their meaning can be found in Sect. 7.\\nAnswer the following question: Could explain \"Width-based search methods\"'}]\n",
      "Bot:  Width-based search methods are a type of search algorithm used in classical planning. These methods exploit the structure of states to explore the state space in a different way than \"blind\" search methods like breadth-first and depth-first search. In width-based search, a non-negative integer called a novelty measure is associated with each generated state. This novelty measure is determined by the size of the smallest factor in the state that has not been seen before in previously generated states. States with smaller novelty measures are considered more novel and are preferred in the exploration search. Width-based search methods aim to efficiently explore the state space by prioritizing states with smaller novelty measures.\n",
      "citation: file name: 2311.05490.pdf, page number: 1\n",
      "Me:  \n",
      "Exiting conversation.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Chat history\n",
    "message_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI assistant that helps people find information.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\\\"Use below context:...[your provided message context here]\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to handle bot responses\n",
    "def chat_with_bot():\n",
    "    while True:\n",
    "        # 0. Get user input\n",
    "        user_input = input(\"You: \")\n",
    "        print(\"Me: \", user_input)\n",
    "        if user_input == '' or user_input.lower() == 'quit':\n",
    "            print(\"Exiting conversation.\")\n",
    "            break  # Exit the loop if user inputs 'quit'\n",
    "\n",
    "        # Swtich to the right Azure OpenAI resource: gpt-35-turbo-instruct-0914, this version support Completion, Embedding, but not ChatCompletion\n",
    "        openai.api_type = \"azure\"\n",
    "        openai.api_version = \"2023-09-15-preview\"\n",
    "        openai.api_key = API_KEY\n",
    "        openai.api_base = RESOURCE_ENDPOINT\n",
    "        # Deployment for embedding\n",
    "        # DEPLOYMENT_NAME_EMBEDDING = os.getenv('DEPLOYMENT_NAME_EMBEDDING')\n",
    "        \n",
    "        # 1. Now we have user input, let's do embedding on it first\n",
    "        user_question_embedding = user_input_text_embedding(user_input)\n",
    "        \n",
    "        # 2. Let's do similarity search first using our own data (Arxiv papers in AI) stored in DataFrame\n",
    "        relevant_content_index = cosine_similarity_search(user_question_embedding, df)\n",
    "        # print(\"relevant_content_index: \", relevant_content_index)\n",
    "\n",
    "        # 3. Build up prompt ....\n",
    "\n",
    "        # Build the prompt using f-string\n",
    "        # prompt = f\"Using below context: \\n{df['page_content'][relevant_content_index]}\\nAnswer the following question: {user_input}\"\n",
    "        # print(\"Prompt(internal):\", prompt)\n",
    "        # # Append user input to message history\n",
    "        # message_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        message_text = prompt_engineering(3, relevant_content_index, user_input)\n",
    "        # Append user input to message history\n",
    "        message_history.append(message_text[1])\n",
    "\n",
    "        # Use chat history may help on the context of a conversation\n",
    "        prompt_message =''\n",
    "        use_chat_history = False\n",
    "        if use_chat_history:\n",
    "            prompt_message = message_history\n",
    "        else:\n",
    "            prompt_message = message_text\n",
    "\n",
    "        # Swtich to the right Azure OpenAI resource: gpt-35-turbo, this version support ChatCompletion\n",
    "        openai.api_type = \"azure\"\n",
    "        openai.api_version = \"2023-07-01-preview\"\n",
    "        openai.api_key = CHAT_API_KEY\n",
    "        openai.api_base = CHAT_RESOURCE_ENDPOINT\n",
    "        # Deployment for Chat\n",
    "        # DEPLOYMENT_NAME_CHAT = os.getenv('DEPLOYMENT_NAME_CHAT')\n",
    "\n",
    "        # 4. Create chat completion using OpenAI API\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            engine=DEPLOYMENT_NAME_CHAT,\n",
    "            # messages=message_history,\n",
    "            messages=prompt_message,\n",
    "            temperature=0.1,\n",
    "            max_tokens=400,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            stop=None\n",
    "        )\n",
    "\n",
    "        # 5. Get bot response\n",
    "        bot_response = completion['choices'][0]['message']['content']\n",
    "        # print(\"Bot:\", bot_response)\n",
    "        # TODO: add where info coming from which doc and page number ...\n",
    "        citation = f\"citation: file name: {df['filename'][relevant_content_index]}, page number: {df['page_number'][relevant_content_index]}\"\n",
    "        bot_response_with_citation = f\"{bot_response}\\n{citation}\"        \n",
    "        print(\"Bot: \", bot_response_with_citation)\n",
    "\n",
    "        # 6. Append bot response to message history\n",
    "        message_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "        # print(\"message_history:\", message_history)\n",
    "\n",
    "# Start the conversation\n",
    "print(\"Start chatting with the bot ('quit' to exit):\")\n",
    "chat_with_bot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
