[
    {
        "filename": "2311.05263.pdf",
        "page_number": 1,
        "page_content": "Model-Based Minimum Bayes Risk Decoding Yuu Jinnai, Tetsuro Morimura, Ukyo Honda, Kaito Ariu, Kenshi Abe CyberAgent {jinnai_yu, morimura_tetsuro, honda_ukyo,kaito_ariu, abe_kenshi}@cyberagent.co.jp Abstract Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative to beam search decoding in a variety of text gener- ation tasks. MBR decoding selects a hypothesis from a pool of hypotheses that has the least ex- pected risk under a probability model according to a given utility function. Since it is impracti- cal to compute the expected risk exactly over all possible hypotheses, two approximations are commonly used in MBR. First, it integrates over a sampled set of hypotheses rather than over all possible hypotheses. Second, it esti- mates the probability of each hypothesis using a Monte Carlo estimator. While the first approx- imation is necessary to make it computationally feasible, the second is not essential since we typically have access to the model probability at inference time. We propose Model-Based MBR (MBMBR), a variant of MBR that uses the model probability itself as the estimate of the probability distribution instead of the Monte Carlo estimate. We show analytically and em- pirically that the model-based estimate is more promising than the Monte Carlo estimate in text generation tasks. Our experiments show that MBMBR outperforms MBR in several text generation tasks, both with encoder-decoder models and with large language models. arXiv:2311.05263v1 [cs.AI] 9 Nov 2023 1 Introduction One of the key components of text generation is the decoding strategy, which is the decision rule used to generate sentences from the model. Beam search is the popular decoding strategy that has been widely used in many directed text generation tasks, includ- ing machine translation (Wu et al., 2016; Ott et al., 2019; Wolf et al., 2020), text summarization (Rush et al., 2015; Narayan et al., 2018), and image cap- tioning (Anderson et al., 2017). However, beam search is known to have several degeneration prob- lems. For example, Welleck et al. (2020) report that beam search can yield infinite-length outputs that the model assigns zero probability to. Minimum Bayes Risk (MBR) decoding has re- cently gained attention as a decoding strategy with the potential to overcome the problems of beam search (Goodman, 1996; Kumar and Byrne, 2004; Eikema and Aziz, 2020, 2022; Freitag et al., 2022; Bertsch et al., 2023). MBR decoding consists of the following steps. First, it samples multiple se- quences from the model. Then, it compares each sequence to the others according to a utility func- tion. Finally, it selects the sequence that maximizes the expected utility over the estimated probability distribution over the sequences. Previous work on MBR decoding uses a Monte Carlo estimate to approximate the probability dis- tribution since it is an unbiased estimate of the true model distribution. However, because the number of possible hypotheses is enormous compared to the number of samples, the estimation error of the Monte Carlo estimate is huge. This can lead to a huge error in the expected utility estimate. We propose Model-Based MBR (MBMBR), a variant of MBR that uses a model-based estimate of the MBR objective instead of a Monte Carlo estimate. The model-based estimate uses the prob- ability model itself, but with its domain limited to the set of observed hypotheses. As such, the esti- mate is computationally feasible and as accurate as the model for the observed sequences. MBMBR is easy to implement and requires only the model probability of the sampled sequences, which can be obtained concurrently with the sampling procedure. We first evaluate the model-based estimate and show analytically that the Kullback-Leiber (KL) divergence from the true model probability is guar- anteed to be lower than that of the Monte Carlo estimate. We also evaluate it empirically on var- ious text generation tasks and show that the KL divergence of the model-based estimate is signifi- cantly lower than that of the Monte Carlo estimate, indicating that it is a better estimator for the use of MBR."
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 2,
        "page_content": "We apply MBMBR to several text generation tasks: machine translation, text summarization, image captioning, and data-to-text. We evaluate it in two settings: using a domain-specific con- ditional generation model, and using a domain- independent large language model (LLM) with prompting. MBMBR outperforms MBR in all set- tings except when the model generates low-quality sequences. The experimental results show that MBMBR is an effective decoding strategy that can replace MBR to improve text generation in a wide range of settings. 2 Background Text generation is the task of generating an output sequence y given an input sequence x. Probabilis- tic text generators define a probability distribution Pmodel(y|x) over an output space of hypotheses y. The set of complete hypotheses J is: y = {BOS \u00b7 v \u00b7 EOS|V E V* }, (1) where o is a string concatenation and V* is the Kleene closure of a set of vocabulary V. In practice, we set the maximum sequence length to nmax to limit the hypothesis space to V\"max. The goal of decoding is to find the highest-scoring hypothesis for a given input. One of the most common decision rules is maximum-a-posteriori (MAP) decoding. MAP de- coding finds the most probable translation under the model: hMAP = arg max Pmodel (h|x). (2) hey Although it seems intuitive to solve this MAP ob- jective, previous work has pointed out two critical problems with this strategy. First, because the size of the hypothesis set |J| is extremely large, it is intractable to solve optimally. Second, the MAP ob- jective often leads to low quality output (Stahlberg and Byrne, 2019; Holtzman et al., 2020; Meister et al., 2020). Indeed, Stahlberg and Byrne (2019) show that hMAP is often found to be the empty sequence in their experimental setting. As such, beam search is commonly used as a heuristic algorithm to solve a decoding problem (Graves, 2012; Sutskever et al., 2014). Beam search is an extension of greedy search that considers a fixed number k of options at each step. It is known to produce higher quality sequences than greedy decoding in a wide range of tasks. However, beam search is known to have degeneration problems such as repetitions and infinite length outputs (Co- hen and Beck, 2019; Holtzman et al., 2020). 2.1 Minimum Bayes Risk Decoding Unlike MAP decoding, which searches for the most probable output, MBR decoding searches for the output that maximizes expected utility, which is equivalent to minimizing risk (Goel and Byrne, 2000; Kumar and Byrne, 2002, 2004). The proce- dure consists of two components: a text generation model and a utility metric. The model Pmodel(y) estimates the probability of an output y given an input sentence x. The utility metric u(h, y) esti- mates the quality of a candidate output h given a reference output y. Given a set of candidate hy- potheses Hcand C J, MBR decoding selects the best hypothesis according to its expected utility over the distribution of human references: hhuman = arg max hEHcand y~Phuman E [u(h, y)]. (3) Since Phuman is unknown, MBR instead uses the model probability Pmodel to approximate Phuman. hmodel = arg max hEHcand y~Pmodel E [u(h, y)] = arg max hE Hcand yEy > u(h, y) . Pmodel(y). (4) For the rest of the paper, we will denote Pmodel as P for simplicity, unless otherwise noted. Since integration over J is computationally intractable, Eq. (4) is approximated by a Monte Carlo es- timate (Eikema and Aziz, 2022; Farinhas et al., 2023) using a set of reference hypotheses Href S y sampled from the model P: hMC = arg max 1H ref 1 yEHref u(h,y). (5) hEHcand Eq. (5) is derived by replacing the true model prob- ability P in Eq. (4) with the empirical distribution \u20b1, which is the number of occurrences of y in Href divided by the sample size | Href |: \u20b1(y) | Href| Ey'EHref I(y = y') (6)"
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 3,
        "page_content": "Objective Method Target Objective Monte Carlo (MBR) Model-Based (MBMBR) hMB (Eq. 8) Estimator P Eyey u(h, y)P(y) EyeHref u(h, y)?(y) hmodel (Eq. 4) hMC (Eq. 7) \u00ceMB LyEHref u(h, y) P(y) Table 1: Comparison of the surrogate objective functions of MBMBR and MBR. MBR uses Monte Carlo estimate to approximate the target MBR objective whereas MBMBR uses the model P as is. Using \u20b1, we can rewrite Eq. (5) as follows: hMC = arg max E [u(h, y)] hEHcand y~\u20b1 = arg max >u(h, y) . \u20b1(y) hEHcand yEy u(h,y) . \u20b1(y). (7) heHcand yEHref = arg max Note that P(y) is zero for y E ) \\ Href. Standard practice is to use the same set of hypotheses for the candidate set Hcand and the reference pool Href (H := Hcand = Href). 2.2 Sampling Algorithms for MBR The choice of sampling algorithm to collect H has been studied extensively, as it has been shown to be critical to the performance of MBR. While most of the classical work on MBR relies on beam search to generate samples, Eikema and Aziz (2020) propose to use unbiased sampling through ancestral sam- pling (Robert and Casella, 1999). Fernandes et al. (2022) have found that MBR with top-k sampling (Fan et al., 2018) and nucleus sampling (Holtz- man et al., 2020) generates much higher quality sequences compared to ancestral sampling. Top- k sampling is a simple modification of ancestral sampling that truncates all tokens except the M most probable tokens. Similar to top-k sampling, nucleus sampling also truncates the lower tail of the probability distribution. Nucleus sampling trun- cates all tokens except those in the nucleus, the smallest possible set of tokens that covers a frac- tion p of the model probability. Freitag et al. (2023) show that epsilon sampling (Hewitt et al., 2022) further improves these sampling algorithms on ma- chine translation tasks. Epsilon sampling is also a variant of ancestral sampling that truncates tokens whose probability is less than a fixed threshold E. While the empirical distributions of top-k sampling, nucleus sampling, and epsilon sampling are biased and inconsistent estimators of the model probability P, MBR decoding using these algorithms has been empirically shown to produce higher quality text than using ancestral sampling, which is unbiased and consistent. 3 Model-Based Minimum Bayes Risk (MBMBR) Decoding The estimation error of the Monte Carlo sum comes from two approximations. First, the domain of the reference hypotheses is restricted to the set of sampled sentences Href instead of y. This ap- proximation is necessary because enumerating all hypotheses in J is infeasible. Second, the proba- bility of each sentence in Href is estimated by the Monte Carlo estimate instead of the true model probability P. While the first approximation is necessary, the second approximation is unneces- sary if the model probability is accessible, which is the case for most decoding scenarios. To this end, we propose a model-based Minimum Bayes Risk (MBMBR) decoding, a variant of MBR that uses a model-based estimate instead of a Monte Carlo estimate .: hMB = arg max u(h,y) . P(y) (8) hEHcand yEHref The model-based estimate simply replaces \u20b1 with P. While vanilla MBR estimates the probability distribution using only the samples, MBMBR uses both the samples and their model probability to estimate. Intuitively, MBMBR can fully exploit the inherent properties of the given model. This allows a more accurate probability density to be computed, resulting in more accurate estimates. The idea of using model probability for MBR is not new per se. The model probability P is used when Href is collected by a beam search instead of probabilistic sampling algorithms (Goel and Byrne, 2000; Kumar and Byrne, 2002, 2004). The novelty of the idea is to show that we can make use of the model probability with a sampling algorithm to fur- ther improve the MBR decoding. In the following sections, we show that (1) the divergence of the"
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 4,
        "page_content": "estimated distribution is guaranteed to be smaller (or equal in extremely unlikely cases) than using the Monte Carlo estimate (Section 4), (2) the di- vergence of the estimated distribution is reduced so that about half as many samples are needed to achieve the same divergence as with Monte Carlo estimate (Figure 1 and 2), and (3) using the model probability, MBMBR outperforms MBR in a wide range of text generation tasks (Section 5). Mitigating the Length Bias: One of the problems with using P directly for MBMBR is that it im- poses a bias on the MBR if the model is biased. To evaluate the length bias, we sample 64 sequences with epsilon sampling for each source sentence of XSum dataset. We observe that there is a negative correlation between the sentence length and the model probability. The correlation of the sequence length relative to the target sequence length and the probability of the model. The average correla- tion is -0.264 with a standard deviation of 0.114. This is problematic since Phuman is unlikely to have such a bias as strong as Pmodel. Although previous work shows that vanilla MBR decoding generates outputs relatively close in length to the references (Bertsch et al., 2023), MBMBR is di- rectly subject to the length bias since it uses the probability itself. To remedy this problem, we use length normalization with a Pmodel normalized by the length of the sequence (Murray and Chiang, 2018; Bertsch et al., 2023). Suppose the model probability is biased to generate shorter sequences: Pmodel(y) ~ e-e(y) Phuman (y), where ((y) is the length of y, we compensate for this bias as follows: hMBL '= arg max yEHref u(h, y). ee(y) P(y). (9) hEHcand We denote a variant of MBMBR with hMBL as the objective function as length-normalized MBMBR (MBMBRL). 4 Properties of the Model-Based Estimate In this section, we first introduce the model-based estimator and give an example to illustrate its differ- ence from the Monte Carlo estimator (Table 2). We then evaluate its divergence from the true model probability analytically and empirically. MBMBR can be understood as a standard MBR, but using the following probability distribution in- stead of the empirical distribution: JaP(y) if y E Href (10) PMB(y) = 10 otherwise, where a = Ey'EHref P(y) 1 is a normalization factor so that it sums to 1: Eyey \u20b1MB(y) = 1. PMB is a probability distribution with support restricted to Href and probability proportional to P. We denote PMB (y) as a model-based distribution. Using PMB, we can rewrite the form of the MBMBR as follows: hMB = arg max L u(h,y) . P(y) hEHcand yEHref = arg max Du(h, y) . PMB(y). (11) hEHcand yEy Compared to hmodel (Eq. 4), the difference is that the probability distribution P is replaced by the model-based distribution PMB. Therefore, as PMB gets closer to P, we expect hMB to get closer to hmodel, which is the gold standard for the MBR objective. Example: Table 2 describes an example to il- lustrate the property of model-based distribu- tion. Suppose we sample three sentences Href = (yo, y1, y2) and their probabilities according to the model are Pmodel = (0.3, 0.1, 0.1). The em- pirical distribution uses the Monte Carlo sum as an estimate to get \u20b1 = (0.33, 0.33, 0.33). The model-based distribution uses Pmodel to weight the sample, thus PMB = (0.6, 0.2, 0.2). The Kullback-Leibler (KL) divergence of the model- based distribution from the model probability is DKL(PMB|P) = 0.693 while that of the empiri- cal distribution is DKL(\u20b1|P) = 0.819. This indi- cates that the model-based distribution provides a better estimate of the true model probability than the Monte Carlo estimate. Analytical Results: The model-based estimate is guaranteed to be better than the Monte Carlo es- timate with respect to the KL divergence. In fact, it is optimal over the probability distribution with support restricted to Href . Theorem 4.1. The model-based distribution min- imizes the Kullbuck-Leiber divergence over the probability distribution with support restricted to Href. PMB = arg min {DKL (p|P)} , PEA ();Href) where A();Href) = {PEA(V) | Vy E } \\ Href, p(y) = 0} ."
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 5,
        "page_content": "Target P Monte Carlo Estimate Sampled Sequences But telling the truth is not a crime. However, telling the truth is not a crime. But to tell the truth is not a crime. 0.1 0.3 0.33 0.1 DKL (.|P) Model-Based Estimate \u20b1MB 0.6 0.33 0.2 0.33 0.2 0 0.819 0.693 Table 2: Illustrative example of the adjusted empirical distribution. While the empirical distribution \u20b1 uniformly assigns the same weight to the sampled text without considering the P, the adjusted empirical distribution takes advantage of the fact that the P is given in the text generation task and uses P to adjust the distribution accordingly. A(2) is a collection of probability distributions over y. The proof is in the Appendix A. 0.65 Corollary 4.1.1. The KL divergence of the model- based estimate from the true model probability is less than or equal to that of the Monte Carlo esti- mate: --- P: Monte Carlo (MBR) PMB: Model-Based (MBMBR) 0.60 0.55 KL Divergence DKL(PMB|P) \u2264 DKL(\u20b1|P). (12) The proof is immediate from the Theorem 4.1. Note that the equality holds only under extremely unlikely circumstances where the ratio of the num- ber of occurrences of each hypothesis y in Href exactly matches the ratio of P(y). Empirical Results: To empirically evaluate the model-based distribution in text generation tasks, we compute the KL divergence and the Jensen- Shannon divergence on WMT'19 De-En (Barrault et al., 2019), IWSLT'17 Fr-En (Cettolo et al., 2017), XSum (Lewis et al., 2020), and MS COCO (Lin et al., 2014) datasets. We use the first 1000 in- puts for each dataset. We sample 256 sentences for WMT'19 De-En and 64 for the other datasets using epsilon sampling (\u20ac = 0.02). Figure 1 shows the KL of the model-based and empirical distributions from Pmodel, averaged over the source sentences of WMT'19 De-En. See Appendix B for the rest of the datasets and for the Jensen-Shannon diver- gence. We observe that the KL of the model-based distribution is significantly smaller than that of the empirical distribution in all four datasets. Approxi- mately twice as many samples are required for the Monte Carlo estimate to achieve the same diver- gence as the model-based estimate. The empiri- cal result shows that the model-based estimate is significantly more accurate than the Monte Carlo estimate in terms of KL divergence in a wide range of tasks. 0.50 0.45 0 50 100 150 200 250 Number of samples Figure 1: Kullback-Leibler divergence of the empirical distribution and the adjusted empirical distribution from the true model probability Pmodel, averaged over the source sentences. Evaluated on WMT'19 De-En. 5 Experiments We evaluate MBR, MBMBR, and MBMBRL on machine translation, text summarization, and im- age captioning tasks. We use BERTScore (Zhang* et al., 2020) as the utility function of MBR for all experiments. We use Huggingface's Trans- formers library to run all experiments (Wolf et al., 2020). For reproducibility, all experiments are per- formed using publicly available pretrained models and datasets. 5.1 Machine Translation We evaluate machine translation in two scenarios: (1) decoding a domain-specific machine translation model and (2) decoding a general-purpose LLM with a prompt to trigger the translation. 5.1.1 Machine Translation Model We evaluate the performance of machine transla- tion models using the WMT'19 dataset (Barrault et al., 2019). WMT'19 dataset examines translation"
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 6,
        "page_content": "WMT' 19 De-En 1761 MBR 4 31.26 32.79 MBMBR MBMBRL 32.01 34.09 31.96 33.53 1761 4 MBR 29.73 31.43 MBMBR 30.87 MBMBRL 30.68 31.97 1761 4 MBR 28.71 MBMBR 29.63 MBMBRL 29.34 Epsilon Sampling (\u20ac = 0.02) 8 16 32 64 128 256 34.04 35.43 35.43 35.62 35.77 34.89 35.47 35.95 35.84 35.86 34.29 35.77 35.61 35.85 35.84 Top-k Sampling (k = 10) 8 16 32 64 128 256 32.84 33.57 32.34 33.79 33.35 34.26 34.55 35.09 35.34 35.62 34.24 34.73 34.82 35.49 34.78 35.26 Nucleus Sampling (p = 0.9) 8 16 32 64 30.35 33.46 31.90 34.13 31.49 32.43 33.88 34.14 34.89 35.49 31.12 32.35 33.78 34.10 128 256 34.69 34.80 34.88 35.21 1761 4 MBR 24.31 MBMBR 23.84 MBMBRL 23.78 25.68 Ancestral Sampling 8 16 25.74 27.75 29.11 25.48 27.43 29.04 27.71 32 29.19 64 128 256 31.01 31.85 32.77 30.70 32.05 32.89 30.81 31.92 33.05 #wins MBMBRL > MBR 24/28 #wins MBMBRL > MBMBR 7/28 #wins MBMBR > MBR 23/28 Table 3: BLEU score of MBMBR and MBMBRL with different sampling strategies on WMT'19 De-En. The best score with the same sampling algorithm and number of samples is in bold. The second best score is underlined. between English and other languages in the news domain. We run experiments on four language pairs: English <> German (En<>De) and English <> Russian (En\u2039>Ru) using the pretrained models of each language pair provided by fairseq (Ng et al., 2019).1 We sample 256 sentences per source sentence for De and Ru->En. We use epsilon sampling (Hewitt et al., 2022; Freitag et al., 2023), top-k sampling (Fan et al., 2018), nucleus sampling (Holtzman et al., 2020), and ancestral sampling. The param- eters for the sampling methods are set according to the work of Freitag et al. (2023). For epsilon sampling, \u20ac = 0.02. k is set to k = 10 for top-k sampling. For nucleus sampling, p = 0.9 is set. The temperature is set to 1.0 for all algorithms. We evaluate the BLEU score (Papineni et al., 2002) using the sacreBLEU library (Post, 2018). Table 3 and 4 show the BLEU score from Ger- man and Russian to English. Overall, we observe that MBMBR and MBMBRL outperform the base- line, except for ancestral sampling. Table 5 shows the BLEU score of En-De and En-Ru with epsilon sampling. We observe that the proposed method outperforms the baseline in all settings. 5.1.2 Large Language Model with Prompting For the experiments on a large language model, we use IWSLT 2017 French -> English dataset as a benchmark (Cettolo et al., 2017). We use BLOOMZ and mTO model (7.1B) loaded in 8-bit precision to reduce memory consumption (Muen- nighoff et al., 2023).2 64 sentences are sampled using epsilon sampling with \u20ac = 0.02. We use the 1https://github. com/facebookresearch/fairseq/ blob/main/examples/wmt19/README .md 2https://huggingface.co/bigscience/ bloomz-7b1-mt"
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 7,
        "page_content": "1761 4 MBR 28.00 30.03 31.14 MBMBR MBMBRL 29.55 28.99 1761 4 MBR 27.09 MBMBR MBMBRL 27.96 28.11 29.54 30.39 31.26 4 MBR MBMBR MBMBRL 25.51 26.65 26.67 1761 4 MBR MBMBR 21.34 20.46 MBMBRL 20.72 21.88 24.15 WMT' 19 Ru-En Epsilon Sampling (\u20ac = 0.02) 8 16 32 64 128 256 31.95 31.94 32.68 32.61 30.77 31.69 32.25 32.21 32.58 32.90 30.50 31.40 32.27 32.27 33.01 32.86 Top-k Sampling (K = 10) 8 16 32 64 128 256 29.33 30.23 30.95 31.46 31.69 31.80 29.71 30.75 31.48 31.81 31.86 31.93 31.59 32.11 32.09 Nucleus Sampling (p = 0.9) 8 16 32 64 128 256 28.46 29.25 30.15 30.79 31.43 31.35 28.47 29.40 30.54 31.24 31.68 31.71 28.61 29.83 30.21 31.04 31.51 31.61 Ancestral Sampling 8 16 32 64 128 256 22.35 24.60 25.83 21.42 24.01 26.67 27.39 27.30 25.07 25.96 26.75 27.18 25.44 26.59 27.49 27.69 #wins MBMBRL > MBR 23/28 #wins MBMBRL > MBMBR 16/28 #wins MBMBR > MBR 20/28 Table 4: BLEU score of MBMBR and MBMBRL with different sampling strategies on WMT'19 Ru-En. BERTScore is used as a utility function. The best score using the same sampling algorithm and number of samples is in bold. The second best score is underlined. following prompt as a guide to trigger its transla- tion capability: each dataset. For SAMSum dataset, we use the entire test dataset (819 inputs). Translate the following sentence from French to English. Q: [QUESTION]] A: The BLEU score of IWSLT' 17 Fr-En is reported in Table 5. We observe that MBMBR outperforms MBR, especially when the number of samples is small. 5.2 Text Summarization We evaluate text summarization using (1) a BART model specifically fine-tuned for each text summa- rization dataset and (2) a general-purpose LLM with prompting. Due to computational resource constraints, we evaluate the first 1000 inputs of 5.2.1 Text Summarization Model We use XSum (Narayan et al., 2018) and SAM- Sum (Gliwa et al., 2019) datasets to evaluate the performance of MBMBR on the text summariza- tion task. XSum dataset is a benchmark for an abstractive single-document summarization. The documents are collected from BBC articles. We use a BART model pretrained on XSum dataset (Lewis et al., 2020).3 SAMSum corpus is a collection of messenger-like conversations with summaries. We use a BART model pretrained on SAMSum dataset.4 We evaluate ROUGE-L using Hugging- 3https: //huggingface. co/facebook/ bart-large-xsum 4https://huggingface.co/philschmid/ bart-large-cnn-samsum"
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 8,
        "page_content": "WMT'19 En-De (BLEU) 1761 4 MBR 29.03 30.73 MBMBR MBMBRL 30.00 29.94 31.17 8 16 32 64 4 32.10 33.18 34.04 16.72 31.45 32.32 33.06 32.26 33.35 34.00 17.37 34.41 16.75 CNN/DM (ROUGE-L) 8 16 32 64 16.62 16.94 17.16 16.75 17.63 16.98 17.55 17.26 17.73 18.07 17.63 17.38 WMT'19 En-Ru (BLEU) 1761 4 24.68 MBR MBMBR MBMBRL 25.52 25.15 26.60 8 16 32 64 4 26.26 27.25 27.80 27.97 27.12 26.72 27.19 28.14 28.53 28.75 30.56 33.12 34.25 34.96 27.34 28.08 28.19 28.65 29.29 MSCOCO (BLEU) 8 28.46 16 30.42 31.51 33.24 31.14 32 64 32.54 33.99 IWSLT'17 Fr-En (BLEU) 176 4 MBR 25.33 26.89 27.94 MBMBR MBMBRL 26.58 26.26 8 16 32 64 4 28.48 29.43 25.43 27.77 28.52 29.51 29.25 23.31 27.16 28.38 29.15 29.51 28.86 NoCaps (BLEU) 8 16 27.67 28.73 30.93 24.73 25.87 26.91 29.78 30.47 32 64 31.42 27.27 32.10 32.68 1741 4 MBR 32.01 MBMBR 31.69 33.09 MBMBRL 32.65 XSum (ROUGE-L) 8 16 32 64 4 33.96 34.80 34.00 34.01 35.00 35.41 35.90 20.49 20.77 34.72 35.51 17.79 35.63 36.18 19.93 21.28 E2E NLG (BLEU) 8 16 20.42 17.01 15.80 16.26 16.07 20.88 20.99 22.03 32 64 21.04 22.30 1761 MBR MBMBR MBMBRL SAMSum (ROUGE-L) 4 8 16 32 30.55 31.16 31.68 32.44 31.17 32.06 31.98 32.51 33.03 33.14 33.86 34.50 32.62 32.99 Total 64 #wins MBMBRL > MBR 42/45 #wins MBMBRL > MBMBR 19/45 #wins MBMBR > MBR 27/45 33.14 Table 5: Evaluation of model-based MBR (MBMBR) and MBR. We use epsilon sampling with \u20ac = 0.02. BERTScore is used as the utility function for all experiments. BLEU is reported as an evaluation metric for machine translation (WMT'19 and IWSLT'17), image captioning (MS COCO and NoCaps), and data-to-text (E2E NLG). ROUGE-L is reported for text summarization (XSum and CNN/DM) and dialogue summarization (SAMSum). Due to computational constraints, we evaluate the first 1000 inputs of the dataset rather than the entire dataset. For SAMSum we use the entire dataset (819 inputs) Face's evaluate library (Lin, 2004).5 Although the performance of sampling algo- rithms has been extensively compared for ma- chine translation tasks (Freitag et al., 2023), lit- tle has been evaluated for other text generation tasks. As a preliminary experiment, we evaluate the performance of MBR using epsilon sampling (\u20ac = 0.02), top-k sampling (k = 10), nucleus sam- pling (p = 0.9), and ancestral sampling on text summarization tasks (XSum and SAMSum) and image captioning tasks (MS COCO and NoCaps; see Section 5.3). The results show that epsilon sam- 5https://huggingface.co/spaces/ evaluate-metric/rouge pling outperforms others on all datasets (Appendix C). Based on this observation, we will continue the evaluation of MBMBR and MBR using epsilon sampling with \u20ac = 0.02. The result using epsilon sampling is shown in Table 5. Overall, MBMBRL outperforms MBR in both datasets. However, we observe that MBMBR drops in the ROUGE-L score in XSum dataset. To investigate the effect of length nor- malization, we compute the relative length of the selected sentence compared to the reference text (l(y)/(ref) by the three methods. The average (standard deviation) of the relative length over the dataset of MBR, MBMBR, and MBMBRL are"
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 9,
        "page_content": "0.870(0.268), 0.777(0.234), and 0.851(0.270), re- spectively. MBMBR has decreased the relative length by about 10% compared to MBR, which we speculate is the reason for the decrease in the ROUGE-L score. MBMBRL also becomes shorter but only by about 2%. For the SAMSum dataset, the decrease in relative length of MBMBR is about 2% which may be the reason why MBMBR suc- cessfully achieves a higher ROUGE-L score. 5.2.2 Large Language Model with Prompting CNN/DM is a collection of news articles and their summaries by journalists from CNN and the Daily Mail (Hermann et al., 2015). We run experiments on the first 1000 inputs of CNN/DM with Mistral- 7B-Instruct-v0.1 (Jiang et al., 2023) loaded with 4-bit precision.6 We use the following prompt: Given a BBC article, write a short sum- mary of the article in one sentence. Article: [[QUESTION]] Q: Summarize the above article briefly in one sentence. A: The result is shown in Table 5. We see that MBMBR has a higher ROUGE-L score compared to MBR. 5.3 Image Captioning with BLIP-2 We evaluate our method for image captioning tasks. Although the input for the image captioning task is an image rather than a text, MBR is applicable as is. That is, we generate a set of hypotheses conditional on the input image and then select the best hypothesis using the MBR objective. We use BLIP-2 (Li et al., 2023) with Flan T5- xl (Chung et al., 2022) loaded in 8-bit to evaluate on two datasets: MS COCO dataset (Lin et al., 2014) and NoCaps dataset (Agrawal et al., 2019). We use a fine-tuned model for MS COCO dataset and a base model for NoCaps dataset.78 Due to computational resource constraints, we evaluate the first 1000 inputs of each dataset. Overall, we observe that MBMBRL outperforms MBR in both datasets (Table 5). 6https://huggingface.co/mistralai/ Mistral-7B-Instruct-v0.1 7https: //huggingface. co/Salesforce/ blip2-flan-t5-xl-coco 8https: //huggingface. co/Salesforce/ blip2-flan-t5-xl 5.4 Data-to-Text with Few-Shot Learning We evaluate the proposed method on a data-to-text generation task using E2E NLG dataset (Novikova et al., 2017). E2E NLG is a data-driven natural language generation task in the restaurant domain. Given a set of key-value pairs about a restaurant, the task is to provide a short English description of the restaurant in a few sentences. We use a Mistral- 7B-Instruct-v0.1 loaded with 4-bit with a prompt provided by Suzgun et al. (2023).9 Due to compu- tational resource constraints, we evaluate the first 1000 inputs of the dataset. The result is shown in Table 5. Overall, we observe no improvement over the baseline in E2E NLG dataset. We speculate that this is because the quality of the sentences gen- erated is not high enough for the MBR to work properly. 6 Related Work MBR decoding has been studied in many NLP tasks including parsing (Goodman, 1996), speech recognition (Goel and Byrne, 2000), bilingual word alignment (Kumar and Byrne, 2002), and machine translation (Kumar and Byrne, 2004). MBR de- coding has recently gained attention in machine translation as a decision rule to overcome some of the biases of MAP decoding in neural machine translation (Eikema and Aziz, 2020; M\u00fcller and Sennrich, 2021; Eikema and Aziz, 2022). Freitag et al. (2022) and Fernandes et al. (2022) show that using neural-based utility functions such as BLEURT (Sellam et al., 2020; Pu et al., 2021) and COMET (Rei et al., 2020, 2022) instead of lex- ical overlap metrics (e.g. BLEU) further improves MBR. The main drawback of MBR decoding is that it is prohibitively expensive. It requires a total of O(|7|2) evaluations of the utility function at infer- ence time. Eikema and Aziz (2022) present two approaches, N-by-S and Coarse-to-Fine, to reduce the number of utility function evaluations without compromising translation quality. Finkelstein and Freitag (2023) show that self-training a machine translation model using its own MBR-decoded out- put can improve the performance of more efficient decoding methods such as beam search. 9https://github.com/suzgunmirac/ crowd-sampling/blob/main/prompts/e2e_nlg_clean_ fs. txt"
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 10,
        "page_content": "7 Conclusions We propose Model-Based Minimum Bayes Risk (MBMBR) decoding, a variant of MBR that uses a model-based distribution as an estimator of the model probability. We evaluate the model-based distribution analytically and empirically to show that it is closer to the true model probability than the Monte Carlo estimate with respect to KL diver- gence. The result suggests that the model-based dis- tribution is likely to be a better estimator for the pur- pose of MBR decoding. We perform MBMBR de- coding on a variety of text generation tasks, includ- ing machine translation, text summarization, image captioning, and data-to-text, using both domain- specific sequence-to-sequence models and domain- independent large language models. The empirical results show that MBMBR outperforms MBR in most cases. 8 Limitations The weakness of the proposed method is that it requires access to the model probability. Therefore, it is not applicable to a situation where the sam- ples are generated by an external model where the model probability is not available. One of the critical limitations of MBR decod- ing is its decoding speed. It requires computing a utility function that is quadratic in the number of samples. This makes MBR decoding very expen- sive and impractical in many situations. MBMBR inherits the same limitation because it is derived from MBR. Due to limited computational resources, some of our experiments use only part of the dataset instead of the whole dataset. As a result, the scores are not directly comparable with the existing literature. References Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. 2019. no- caps: novel object captioning at scale. In Proceed- ings of the IEEE International Conference on Com- puter Vision, pages 8948-8957. Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2017. Guided open vocabulary im- age captioning with constrained beam search. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 936- 945, Copenhagen, Denmark. Association for Compu- tational Linguistics. Lo\u00efc Barrault, Ond\u0159ej Bojar, Marta R. Costa-juss\u00e0, Christian Federmann, Mark Fishel, Yvette Gra- ham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias M\u00fcller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 conference on machine trans- lation (WMT19). In Proceedings of the Fourth Con- ference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1-61, Florence, Italy. As- sociation for Computational Linguistics. Amanda Bertsch, Alex Xie, Graham Neubig, and Matthew R Gormley. 2023. It's mbr all the way down: Modern generation techniques through the lens of minimum bayes risk. arXiv preprint arXiv:2310.01387. Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian St\u00fcker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann. 2017. Overview of the IWSLT 2017 evaluation campaign. In Proceedings of the 14th International Conference on Spoken Language Translation, pages 2-14, Tokyo, Japan. International Workshop on Spoken Language Translation. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv: 2210.11416. Eldan Cohen and Christopher Beck. 2019. Empirical analysis of beam search performance degradation in neural sequence models. In Proceedings of the 36th International Conference on Machine Learn- ing, volume 97 of Proceedings of Machine Learning Research, pages 1290-1299. PMLR. Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information Theory. Wiley-Interscience, USA. Bryan Eikema and Wilker Aziz. 2020. Is MAP decoding all you need? the inadequacy of the mode in neural machine translation. In Proceedings of the 28th Inter- national Conference on Computational Linguistics, pages 4506-4520, Barcelona, Spain (Online). Inter- national Committee on Computational Linguistics. Bryan Eikema and Wilker Aziz. 2022. Sampling-based approximations to minimum Bayes risk decoding for neural machine translation. In Proceedings of the 2022 Conference on Empirical Methods in Natu- ral Language Processing, pages 10978-10993, Abu Dhabi, United Arab Emirates. Association for Com- putational Linguistics. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889-898, Melbourne, Australia. Association for Computational Linguistics. Ant\u00f3nio Farinhas, Jos\u00e9 G. C. de Souza, and Andr\u00e9 F. T. Martins. 2023. An empirical study of translation"
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 11,
        "page_content": "hypothesis ensembling with large language models. arXiv preprint arXiv:2310.11430. Patrick Fernandes, Ant\u00f3nio Farinhas, Ricardo Rei, Jos\u00e9 G. C. de Souza, Perez Ogayo, Graham Neubig, and Andre Martins. 2022. Quality-aware decoding for neural machine translation. In Proceedings of the 2022 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, pages 1396-1412, Seattle, United States. Association for Computational Linguistics. Mara Finkelstein and Markus Freitag. 2023. Mbr and qe finetuning: Training-time distillation of the best and most expensive decoding methods. arXiv preprint arXiv:2309.10966. Markus Freitag, Behrooz Ghorbani, and Patrick Fer- nandes. 2023. Epsilon sampling rocks: Investi- gating sampling strategies for minimum bayes risk decoding for machine translation. arXiv preprint arXiv:2305.09860. Markus Freitag, David Grangier, Qijun Tan, and Bowen Liang. 2022. High quality rather than high model probability: Minimum Bayes risk decoding with neu- ral metrics. Transactions of the Association for Com- putational Linguistics, 10:811-825. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek- sander Wawer. 2019. SAMSum corpus: A human- annotated dialogue dataset for abstractive summa- rization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70-79, Hong Kong, China. Association for Computational Linguis- tics. Vaibhava Goel and William J Byrne. 2000. Minimum bayes-risk automatic speech recognition. Computer Speech & Language, 14(2):115-135. Joshua Goodman. 1996. Parsing algorithms and metrics. In 34th Annual Meeting of the Association for Com- putational Linguistics, pages 177-183, Santa Cruz, California, USA. Association for Computational Lin- guistics. Alex Graves. 2012. Sequence transduction with arXiv preprint recurrent neural networks. arXiv: 1211.3711. Karl Moritz Hermann, Tomas Kocisky, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in neural information processing systems, volume 28. John Hewitt, Christopher Manning, and Percy Liang. 2022. Truncation sampling as language model desmoothing. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 3414- 3427, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text de- generation. In International Conference on Learning Representations. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men- sch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guil- laume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b. arXiv. Shankar Kumar and William Byrne. 2002. Minimum Bayes-risk word alignments of bilingual texts. In Pro- ceedings of the 2002 Conference on Empirical Meth- ods in Natural Language Processing (EMNLP 2002), pages 140-147. Association for Computational Lin- guistics. Shankar Kumar and William Byrne. 2004. Minimum Bayes-risk decoding for statistical machine transla- tion. In Proceedings of the Human Language Tech- nology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 169-176, Boston, Mas- sachusetts, USA. Association for Computational Lin- guistics. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and com- prehension. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computa- tional Linguistics. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. BLIP-2: Bootstrapping language-image pre- training with frozen image encoders and large lan- guage models. In Proceedings of the 40th Interna- tional Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 19730-19742. PMLR. Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Computer Vision- ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer. Clara Meister, Ryan Cotterell, and Tim Vieira. 2020. If beam search is the answer, what was the question? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2173-2185, Online. Association for Computa- tional Linguistics."
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 12,
        "page_content": "Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai- ley Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Al- banie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual generaliza- tion through multitask finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15991-16111, Toronto, Canada. Association for Computational Linguistics. Mathias M\u00fcller and Rico Sennrich. 2021. Understand- ing the properties of minimum Bayes risk decoding in neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers), pages 259-272, Online. Asso- ciation for Computational Linguistics. Kenton Murray and David Chiang. 2018. Correcting length bias in neural machine translation. In Proceed- ings of the Third Conference on Machine Translation: Research Papers, pages 212-223, Brussels, Belgium. Association for Computational Linguistics. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for ex- treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1797-1807, Brussels, Bel- gium. Association for Computational Linguistics. Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. 2019. Facebook FAIR's WMT19 news translation task submission. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 314-319, Florence, Italy. Association for Computational Linguistics. Jekaterina Novikova, Ond\u0159ej Du\u0161ek, and Verena Rieser. 2017. The E2E dataset: New challenges for end- to-end generation. In Proceedings of the 18th An- nual SIGdial Meeting on Discourse and Dialogue, pages 201-206, Saarbr\u00fccken, Germany. Association for Computational Linguistics. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Con- ference of the North American Chapter of the Associa- tion for Computational Linguistics (Demonstrations), pages 48-53, Minneapolis, Minnesota. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186- 191, Brussels, Belgium. Association for Computa- tional Linguistics. Amy Pu, Hyung Won Chung, Ankur Parikh, Sebastian Gehrmann, and Thibault Sellam. 2021. Learning compact metrics for MT. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan- guage Processing, pages 751-762, Online and Punta Cana, Dominican Republic. Association for Compu- tational Linguistics. Ricardo Rei, Jos\u00e9 G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and Andr\u00e9 F. T. Martins. 2022. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578-585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. Unbabel's participation in the WMT20 metrics shared task. In Proceedings of the Fifth Con- ference on Machine Translation, pages 911-920, On- line. Association for Computational Linguistics. Christian P Robert and George Casella. 1999. Monte Carlo statistical methods, volume 2. Springer. Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sen- tence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan- guage Processing, pages 379-389, Lisbon, Portugal. Association for Computational Linguistics. Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text genera- tion. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics. Felix Stahlberg and Bill Byrne. 2019. On NMT search errors and model errors: Cat got your tongue? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 3356- 3362, Hong Kong, China. Association for Computa- tional Linguistics. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2, NIPS'14, page 3104-3112, Cambridge, MA, USA. MIT Press."
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 13,
        "page_content": "Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2023. Follow the wisdom of the crowd: Effective text generation via minimum Bayes risk decoding. In Findings of the Association for Computational Linguistics: ACL 2023, pages 4265-4293, Toronto, Canada. Association for Computational Linguistics. Sean Welleck, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang, and Kyunghyun Cho. 2020. Consistency of a recurrent language model with respect to incomplete decoding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5553-5568, Online. Association for Computational Linguistics. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans- formers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, \u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's neural machine translation system: Bridging the gap between human and machine trans- lation. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Eval- uating text generation with bert. In International Conference on Learning Representations."
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 14,
        "page_content": "A Proof of Theorem 4.1 We prove Theorem 4.1 by the method of Lagrange multipliers. Proof. The Lagrangian function L(p, A, w, u) for the optimization problem minDEA();Href) {DKL (p|P)} is given as follows: L(p, A, w, p) = DKL(p|P) + Ah(p) + Wygy(p)+ > My fy(p), yE)\\ H ref yE Href h(p)=>p(y) -1, yey Vy E V \\ Href, gy(p) = p(y), Vy E Href, fy(p) = - p(y). Defining p* := arg minDEA ();Href) {DKL (p|P)}, p* must satisfy the KKT condition: VpL(p*, A, w, p) = 0, h(p*)=0, Vy E V \\ Href, gy (p*) = 0, Vy E Href, fy(p*) \u2264 0, Vy E Href, Myfy(p*) = 0, Vy E Href, Hy \u2265 0. Thus, we get: Vy E Href, In p*(y) +1-My =0, P(y) p* (y) + + Wy = 0, Vy E V \\ Href, In P(y) >p*(y) = 1, yey Vy E V \\ Href, p*(y) = 0, Vy E Href, p*(y) \u2265 0, Vy E Href, Hyp*(y) = 0, Vy E Href, Hy \u2265 0. From the fifth condition, we have My = 0 or p*(y) = 0 for each y E Href. If p*(y) = 0, we have: ln P(y) + / - py = - 00, p* (y) which contradicts the first condition. Thus, we have Hy = 0 for any y \u20ac Href, and then: Vy E Href, p*(y) = P(y) exp(-1). (13) Therefore, from the third and fourth conditions, we get: yEU >>p*(y) = >p*(y) yEHref = exp(->) > P(y) = 1, yEHref and then: > = ln M P(y). (14) yEH ref By combining Eq. (13) and (14), we have: Vy E Href, P* (y) = Ey'EHref P(y')\" P(y) Thus, we get PMB = p*, and thus PMB satisfies the KKT condition. This concludes the statement. B Empirical Evaluation of the Divergence The result of the KL divergence on IWSLT'17 Fr- En, XSum, and MS COCO is shown in Figure 2. See Section 4 for the experimental setup. The model-based estimate significantly reduces the KL divergence compared to the Monte Carlo estimate in all domains. As an additional measure of the divergence, we compute the Jensen-Shannon Divergence (JSD). Note that the JSD of a distribution p and P is com- putable without enumerating all the hypotheses in J if the support of p is restricted to Href. Let M = + (p + P) be a mixture distribution of p and P. Using the convention 0 In () = 0 (Cover and"
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 15,
        "page_content": "0.650 --- P: Monte Carlo (MBR) PMB: Model-Based (MBMBR) 0.625 0.600 KL Divergence 0.575 0.550 0.525 0.500 0 10 20 30 40 Number of samples 50 60 0.690 (a) IWSLT'17 Fr-En --- P: Monte Carlo (MBR) PMB: Model-Based (MBMBR) 0.685 KL Divergence 0.680 0.675 0.670 0 10 20 30 0.68 40 Number of samples (b) XSum 50 60 P: Monte Carlo (MBR) PMB: Model-Based (MBMBR) 0.66 0.64 KL Divergence 0.62 0.60 0.58 0.56 0 10 20 30 40 Number of samples (c) MS COCO 50 60 Figure 2: Kullback-Leibler Divergence of the Monte Carlo estimate and the model-based estimate to the Pmodel, averaged over the source sentences. Thomas, 2006): JSD(p|P) 1 (DKL(p|M)+DKL(P|M)) 2 1 Ep(y) In(- p(y) 2 M(y) yey P(y) + >P(y) In( M(y) p(y) yEy 1 2 > p(y) In( M(y) yEHref p(y) + p(y) In( M(y) yE)\\ Href P(y) + P(y) In( M(y) yEHref P(y) + M P(y) In( M(y) yE)\\ H ref 112 p(y) - p(y) In( M(y) yEHref + 0 + >> P(y) In( M(y) P(y) yEHref +(1 - >P(y)) In(2)). yEHref Therefore, JSD(p||P) is computable without enu- merating all the hypotheses in J. Figure 3 shows the JSD of the model-based and the Monte Carlo estimator. For all datasets, the JSD of the model- based distribution is significantly smaller than that of the empirical distribution. Compared to the Monte Carlo estimate, the model-based estimate requires less than half of the samples to achieve the same divergence. C Evaluation of Sampling Algorithms We evaluate the performance of MBR with epsilon sampling (\u20ac = 0.02), top-k sampling (k = 10), nu- cleus sampling (p = 0.9), and ancestral sampling on text summarization and image captioning. Other experimental settings are the same as in Section 5. The results are shown in Table 6, 7, 8, and 9. We observe that epsilon sampling outperforms the oth- ers on all four datasets, and its results are further improved with MBMBR and MBMBRL."
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 16,
        "page_content": "0.65 --- P: Monte Carlo (MBR) PMB: Model-Based (MBMBR) 0.650 0.60 0.625 0.600 0.55 Jensen-Shannon Divergence Jensen-Shannon Divergence 0.575 0.550 0.50 0.525 0.45 0.500 0 50 100 150 Number of samples 200 250 0 10 (a) WMT'19 De-En 0.690 -- \u20b1: Monte Carlo (MBR) PMB: Model-Based (MBMBR) --- \u20b1: Monte Carlo (MBR) PMB: Model-Based (MBMBR) 20 30 40 Number of samples 50 60 (b) IWSLT'17 Fr-En -- 0.68 \u20b1: Monte Carlo (MBR) PMB: Model-Based (MBMBR) - 0.66 0.685 0.64 Jensen-Shannon Divergence 0.680 Jensen-Shannon Divergence 0.62 0.675 0.60 0.670 0 10 20 30 0.58 0.56 40 50 60 0 10 Number of samples (c) XSum 20 30 40 50 60 Number of samples (d) MS COCO Figure 3: Jensen-Shannon Divergence of the Monte Carlo estimate and the model-based estimate to the Pmodel, averaged over the source sentences."
    },
    {
        "filename": "2311.05263.pdf",
        "page_number": 17,
        "page_content": "XSum Epsilon Sampling (\u20ac = 0.02) 1761 4 8 16 32 64 MBR 32.01 33.96 34.80 35.41 35.90 MBMBR MBMBRL 31.69 33.09 32.65 34.01 34.00 34.72 35.51 35.00 35.63 36.18 Top-k Sampling (k = 10) 176 4 33.60 32.83 33.76 8 16 32 64 MBR 31.89 34.15 35.03 35.42 31.71 34.28 34.90 32.28 33.76 34.38 35.21 35.83 MBMBR MBMBRL Nucleus Sampling (p = 0.9) 1761 4 8 16 32 64 MBR 30.73 32.03 33.15 33.92 34.60 MBMBR MBMBRL 30.49 30.69 31.41 32.55 33.25 33.72 31.79 32.96 33.96 34.68 Ancestral Sampling 4 8 16 32 64 MBR 27.48 28.27 29.41 30.60 31.82 26.97 MBMBR MBMBRL 24.72 25.88 28.63 30.16 24.75 25.97 27.15 29.09 31.05 Table 6: ROUGE-L scores on XSum dataset. BERTScore is used as the utility function. 1761 MBR MBMBR MBMBRL 28.65 29.29 31.14 32.54 33.99 1761 MBR MBMBR MBMBRL 1721 MBR MBMBR 21.68 24.36 26.84 MBMBRL 21.75 176 MBR 22.70 25.87 MBMBR MBMBRL 18.59 20.48 22.65 25.90 29.52 MS COCO Epsilon Sampling (\u20ac = 0.02) 4 8 16 32 64 27.12 28.46 30.42 31.51 33.24 28.75 30.56 33.12 34.25 34.96 Top-k Sampling (k = 10) 4 8 16 32 64 26.13 21.36 23.80 28.08 30.07 31.89 32.40 26.22 28.81 30.93 21.36 23.51 26.46 28.76 30.85 Nucleus Sampling (p = 0.9) 4 8 16 32 64 26.40 29.12 31.12 31.88 32.98 29.08 31.00 24.31 27.07 29.08 30.95 Ancestral Sampling 8 16 4 32 64 28.08 29.84 31.55 18.63 20.88 22.93 25.26 27.59 Table 8: BLEU scores on MS COCO dataset. BERTScore is used as the utility function. 1761 MBR MBMBR MBMBRL SAMSum Epsilon Sampling (\u20ac = 0.02) 4 8 16 31.98 32 64 30.55 31.16 32.51 33.03 31.68 32.44 33.14 33.86 34.50 MBMBRL 28.86 29.78 30.47 31.17 32.06 32.62 32.99 33.14 Top-k Sampling (k = 10) 4 8 16 32 64 1761 MBR MBMBR MBMBRL 28.04 28.21 28.17 28.33 31.81 29.84 30.76 28.32 28.91 28.26 28.90 29.74 30.47 31.47 31.34 Nucleus Sampling (p = 0.9) 4 8 16 32 64 MBR MBMBR 27.87 MBMBRL 27.87 28.26 28.35 28.69 27.76 27.84 28.21 28.32 28.13 28.24 30.95 28.68 29.17 29.27 MBMBRL Ancestral Sampling 4 8 176 MBR MBMBR MBMBRL 16 32 64 25.94 26.33 25.81 25.86 25.81 25.86 25.90 26.43 26.60 29.83 25.90 26.07 26.24 26.07 26.26 MBMBRL Table 7: ROUGE-L scores on SAMSum dataset. BERTScore is used as the utility function. 1761 MBR MBMBR 23.31 24.73 1721 MBR MBMBR MBMBRL 26.64 28.63 29.15 31.53 31.58 1741 MBR MBMBR 20.89 22.91 1711 MBR MBMBR NoCaps Epsilon Sampling (\u20ac = 0.02) 4 8 16 25.43 27.67 28.73 30.93 31.42 25.87 26.91 27.27 32 64 32.10 32.68 Top-k Sampling (k = 10) 8 4 16 32 64 23.69 25.91 27.41 29.33 30.67 21.39 23.01 24.05 25.18 25.66 Nucleus Sampling (p = 0.9) 4 21.58 25.76 25.48 28.72 29.72 4 8 16 32 64 27.20 28.55 29.24 25.29 26.70 27.14 30.56 31.43 Ancestral Sampling 8 16 32 64 18.48 21.54 23.93 17.42 19.80 21.40 25.10 27.57 24.89 25.61 22.35 23.40 23.66 28.47 29.36 Table 9: BLEU scores on NoCaps dataset. BERTScore is used as the utility function."
    }
]