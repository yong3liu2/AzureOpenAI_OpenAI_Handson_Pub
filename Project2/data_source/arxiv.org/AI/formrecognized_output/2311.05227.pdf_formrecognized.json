[
    {
        "filename": "2311.05227.pdf",
        "page_number": 1,
        "page_content": "Kantian Deontology Meets AI Alignment: Towards Morally Robust Fairness Metrics Carlos Mougan University of Southampton United Kingdom c.mougan@soton.ac. uk Joshua Brand Institut Polytechnique de Paris-T\u00e9l\u00e9com Paris France joshua. brand@telecom-paris.fr Abstract Deontological ethics, specifically understood through Immanuel Kant, provides a moral framework that emphasizes the importance of duties and principles, rather than the consequences of action. Understanding that despite the prominence of deontology, it is currently an overlooked approach in fairness metrics, this pa- per explores the compatibility of a Kantian deontological framework in fairness metrics, part of the AI alignment field. We revisit Kant's critique of utilitarian- ism, which is the primary approach in AI fairness metrics and argue that fairness principles should align with the Kantian deontological framework. By integrating Kantian ethics into AI alignment, we not only bring in a widely-accepted promi- nent moral theory but also strive for a more morally grounded AI landscape that better balances outcomes and procedures in pursuit of fairness and justice. arXiv:2311.05227v1 [cs.AI] 9 Nov 2023 1 Introduction Deontological ethics, with Immanuel Kant as its central figure and in particular through his seminal work Groundwork of the Metaphysics of Morals Kant (1785), emphasizes that it is through duties, or universal set of rules, that the ethical value of actions is determined. Generally, deontology maintains that these duties flow from the recognition that all humans are intrinsically valuable- it is our rational nature that gives us reason to act in one way or another. This duty, or rule-based, moral theory thus aligns well with various declarations, such as the Universal Declaration of Human Rights, that recognize a set of inalienable rights based on the individual, such as the right to life and the right to equal treatment under the law; with deontology, moral action involves respecting inescapable duties that lay the fundamental foundation of the rights of others. While not all duties entail rights, all rights suppose a corresponding duty (Watson, 2021). Given this emphasis on evaluating actions through a set of rules or principles, deontology is often compared to utilitarianism, a consequentialist ethical theory. Utilitarianism, understood through the likes of Jeremy Bentham or John Stuart Mill, focuses on the consequences of an action to determine its ethical value, considering the utility or benefit brought about by the action. In traditional utilitar- ian accounts, utility is identified as pleasure, well-being, or happiness. The goal of moral action is therefore generally construed as increasing the amount of pleasure or happiness in our communities. This is why utilitarian ethics is often favoured in quantitative sciences due to its emphasis on mea- surable outcomes, such as in the fields of economics (Posner, 1979) or public policy (Kymlicka, 2002; Goodin, 1995). It should be, however, noted that utilitarians face criticism on how one can quantify such concepts as happiness (Knutsson, 2016). By quantifying and comparing the various consequential benefits and harms of different actions or policies, the utilitarian approach claims to achieve the most favourable outcome for the greatest number of individuals. Preprint. Under review."
    },
    {
        "filename": "2311.05227.pdf",
        "page_number": 2,
        "page_content": "In the AI alignment field, the challenge relies on ensuring that AI systems align with a diverse set of human values, encompassing moral, societal, justice-oriented, and other relevant principles. As London and Heidari (2023) state succinctly, \"It should be axiomatic that producing some sort of benefit is a necessary condition for the ethical and responsible use of AI.\" It is here that fairness metrics play a vital role by helping to evaluate whether AI systems meet these prescribed values and treat a diverse set of individuals and groups affected by AI systems fairly-in other words, if they are treated in an impartial or equitable manner. As one of the primary aims of every moral theory or prescribed set of values is to establish the definition of the fair treatment of others, fairness metrics are an integral component of AI alignment. Machine learning researchers in fairness metrics balance fairness-accuracy trade-offs to find an equilibrium between predictive performance metrics like accuracy, f1-score, AUC, mean squared errors, and utilitarian interpretations of distributive justice principles, such as equal opportunity, equal outcomes, or equal treatment (Hardt et al., 2016; Heidari et al., 2019). Striving to opti- mize these trade-offs, researchers attempt pre-processing (Fabbrizzi et al., 2023; Ntoutsi et al., 2020), in-processing (Pedreschi et al., 2008; Menon and Williamson, 2018; Zafar et al., 2017; Kamishima et al., 2012), and post-processing strategies (Rodolfa et al., 2021). The utilitarian approach currently takes primacy over other moral accounts, such as deontology, in fairness metrics. Understanding that deontology is a prominent moral theory well-suited to widely accepted conceptions of human rights that recognize individual dignity and worth, and which has important critiques against utilitarianism, we argue that it should be adopted into the fairness metrics approach to counter utilitarian dominance. In this paper we present how a deontological account can be introduced to the field of fairness metrics. We present this paper with the following contributions: \u00b7 Overview of Kantian Deontology The paper provides an analysis of the key concept that guides deontological ethics, the categorical imperative. We then explore the deontological position against utilitarianism. \u00b7 Overview of Fairness Metrics We provide an overview of the fairness metrics field, high- lighting the prevalent utilitarian perspective that currently dominates this domain. \u00b7 Deontology and AI Alignment: We explore the compatibility of a deontological ethical approach with fairness metrics in the context of AI alignment. It examines the potential benefits of integrating deontological principles into the prevailing utilitarian framework, aiming to promote a more ethically grounded and balanced AI landscape. 2 Overview of Kantian Deontology 2.1 The Categorical Imperative Much can be discussed regarding Kant's moral theory, but one of the most fundamental aspects of his ethics, outlined in Groundwork(Kant, 1785), is the categorical imperative. This universal principle, which unconditionally binds all rational beings, provides us with a formal procedure to evaluate our actions-if actions adhere to the categorical imperative they are morally fit. Kant provides us with three primary interpretations of the categorical imperative, with the first two being the most widely discussed.1 The first is the Formula of Universal Law: Act only on a maxim that you can also will to become a universal law (Groundwork, 4:421)2 In other terms, this asks us to consider whether our actions can be performed universally, by all rational agents, without contradiction. Making false promises cannot be willed universally as if everyone made false promises, such as falsely promising to pay back a loan, no one would believe one another as they would distrust everyone and thus, making a false promise would be an impossible act (4:422). 1While there is disagreement on whether or not the formulations are synonymous, they nevertheless all lead us to the same set of duties, see for example (Korsgaard, 1996) 2 All further citations of this kind refer to (Kant, 1785) 2"
    },
    {
        "filename": "2311.05227.pdf",
        "page_number": 3,
        "page_content": "The second formulation, Formula of Humanity, however, is considered by Kant to be \"closer to intuition\" (4:436): Act in such a way that you treat humanity, whether in your own person or anyone else's, never merely as a means, but also always as an end (4:429) This provides us with the material of the moral law (4:436), the rational nature of individuals, (or more specifically, our free will and humanity), that Kantian ethics aims to protect from any instru- mental subordination to another individual's, or group of individuals', ends. This is to recognize that all persons are ends-in-themselves, meaning that they have an intrinsic value \"exalted above all price\" (4:434). This requires engagement with them as freely-choosing rational agents who have the capacity to decide what is valuable to them and to pursue their own projects and, importantly, when they are in a position of not fully realizing this capacity, to provide them with the necessary assistance (4:430). Returning to the case of the false promise, the categorical imperative forbids it because it hinges on deceiving another individual and forcing them into a situation for which they have not principally consented. We primarily focus on this formulation throughout the rest of this paper. 2.2 Kantian Critique of Utilitarianism With this concise outline of Kant's moral law, we now move on to consider the Kantian critique of utilitarianism. Kant was certainly not fond of taking external concepts, such as happiness, as the source of morality. In The Metaphysics of Morals he states: \" ... if eudaemonism (the principle of happiness) is set up as the basic principle instead of eleutheron- omy (the principle of the freedom of internal lawgiving), the result is the euthanasia (the death) of all morals.\"(6:378 Kant and Gregor (1996)). Kant, however, did not directly address the utilitarian of his time, Jeremy Bentham, so it is helpful to look to contemporary Kantians, Onora O'Neill and Christine Korsgaard, who provide a direct critique of utilitarian ethics. The following provides a foundational, but certainly not exhaustive, list of critiques. The first is that utilitarianism believes that all moral actions ought to be aimed at obtaining the 'good' as defined as an impersonal absolute value, such as happiness or pleasure, that can be maximized. It thus follows that the value of any individual relies on the experience of happiness or pleasure. In more crude terms, individuals are a mere vessel for these valuable concepts. Peter Singer writes that the classical utilitarian approach, \"regards sentient beings as valuable only insofar as they make possible the existence of intrinsically valuable experiences like pleasure.\"(Singer, 2011). Singer, himself a utilitarian, continues this notion that it is not the individuals that matter, but the defined utility, through the replaceability argument. While he appreciates that all individuals have inherent worth, this value does not extend far enough to guarantee its protection. Singer has thus argued that there are circumstances (albeit rare) where killing humans, is the right thing to do if it brings about more happiness in the community e.g., Singer (2011). This, of course, is in contrast to the second formula of the categorical imperative that stresses that each individual is intrinsically valuable and is not to be instrumentally subordinated to the well- being, happiness, or pleasure of others. An individual cannot be replaced by another individual even if their existence brings about more happiness. This is because Kantians disagree that the 'good' can be understood as something abstract and impersonal, something that is simply just 'good'. Rather, the good is a relational concept because, as Korsgaard argues, all value is tethered. Happiness or pleasure matters because they are good-for individuals (Korsgaard, 2018). Therefore, crucially, something can only be considered as universally good when it is good-for everyone, from every individual's perspective. This brings us to the second critique: the issue of aggregation. Because utilitarians do not consider individuals as intrinsically valuable, the moral decision procedure thus allows aggregating the good. There is therefore no prohibition against using individuals, even if they are innocent, as mere means, as long as the loss of their dignity is for maximizing a sufficient amount of 'good' in the community. Yet, this aggregation does not make sense, not only because it may violate the second formula of the categorical imperative, but also considering the tethered notion of the 'good'. As Korsgaard contends, \"what is good-for me plus what is good-for you is not necessarily good-for anyone.\" (page 3"
    },
    {
        "filename": "2311.05227.pdf",
        "page_number": 4,
        "page_content": "157 Korsgaard (2018)). Moreover, there is a problem when maximizing the good involves taking something away from an individual: if giving something to Adam brings him more pleasure than it would Eve, the utilitarian would say we ought to give it to Adam. But this action would just be good-for Adam and it would be worse-for Eve-it is not better for everyone. The good, because it is tethered, is not something we can aggregate (Korsgaard, 2018). This particularly is more concerning when those on the receiving end of aggregation are already marginalized individuals. The third critique is that the utilitarian approach of calculating outcomes, whether it be maximizing benefit or reducing harm, is not a robust approach to identifying which actions should be prohibited. As Onora O'Neill points out, circumstances can change the outcome. Take the act of lying as an example: typically, lying is considered to be immoral and thus, generally prohibited. Yet not all lies, such as 'white lies', cause harm. The same goes for telling the truth, which often provides a benefit, but it can also inflict emotional harm. Focusing on consequences is unpredictable and, therefore, provides a less robust standard to measure the rightness and wrongness of actions. The link between harmful consequences and actions is unstable. The connection between norms, or duties, and actions is a more robust approach to morally evaluating actions. Duties, such as the duties of civility or to respect the freedom of others, provide a more useful and robust way of evaluating actions(O'Neill, 2022). Utilitarianism thus has a limitless scope of acceptable actions as it does not depend on the type of action but rather the consequence, or utility, it brings about. Being dishonest for the utilitarian, for example, is not categorically condemned; for this reason, it lacks precision as the rightness or wrongness of an action can only be determined until after it is carried out and the consequences are known. The Kantian approach, while it has a more restricted scope given its categorical stance on duties, provides a precise and fundamental standard of action, where moral authority persists irrespective of any potential harmful consequence. Being dishonest (to some Kantians) is always wrong irrespective of unforeseen consequences. 3 Measuring Fairness Metrics In the field of computer science, researchers have traditionally quantified various statistical dispar- ities that exist among protected attributes. These measures are often labelled using terms from the social sciences (Binns, 2018), such as equal opportunity (Hardt et al., 2016) and equal treat- ment (Candelas et al., 2023). These notions have been incorporated into the study of algorithmic fairness to address historical and systemic biases that can arise in decision-making systems. Re- searchers have subsequently proposed various methods to improve these metrics. Deciding which metrics are best suited is an important yet complex task as researchers have demonstrated incom- patibility between certain metrics, thus arguing that several AI fairness concepts cannot be achieved simultaneously. It is important to note that these concepts, understood through the social sciences, are not mathematically engineered to measure statistical disparities but are derived from an under- standing of historical events, social inequalities, and discriminatory practices. In order to support our subsequent discussions, we now outline the notation we use to define a subset of metrics which will be central to our position. Let X and Y be random variables taking values in X ( Rd and ) ( R, respectively. A predictor is a function f : X > y, also called a model. For conceptual simplicity, the domain of the target feature is dom(Y) = {0,1} (binary classification). We assume a feature modelling protected social groups denoted by Z, called protected feature, and assume it to be binary valued in the theoretical analysis. In this study, we employ a specific subset of fairness metrics. For a comprehensive overview refer to Barocas et al. (2019). First, we refer to group fairness metrics that seek to establish some form of parity between groups of individuals based on protected or sensitive attributes, such as gender and race. Definition 3.1. (Demographic Parity (DP)). A model f achieves demographic parity if f (X) L Z. We can derive an unfairness metric as d(P(f(X)|Z = z), P(f(X)), where d(.) is a distance between probability distributions .. Definition 3.2. (Equal Opportunity (EO)) A model f achieves equal opportunity if Vz. P(f(X)|Y=1,Z=z)=P(f(X)=1|Y=1). Hardt et al. (2016) Definition 3.3. (Accuracy Equality(AE)) A model f achieves accuracy equality if Vz. Vi E (0,1) P(f(X)=i|Y=i,Z=z)=P((X)=i|Y = i). Wachter et al. (2021) 4"
    },
    {
        "filename": "2311.05227.pdf",
        "page_number": 5,
        "page_content": "Definition 3.4. (Equal Treatment (ET)). A model f achieves equal treatment if S(f, X) 1 Z. Where S are feature attributions explanations Mougan et al. (2023) On the other hand, individual fairness metrics are grounded in the principle of treating similar in- dividuals equally. These metrics aim to ensure that any two individuals who exhibit similarity in the context of a specific task are classified in an identical manner. Metrics such as Lipschitz condi- tion (Dwork et al., 2012) and similarity graphs between individual (Petersen et al., 2021) calculate this on different scenarios. Definition 3.5. Individual Fairness We say that a model achieves individual fairness w.r.t. two individual samples if d(f(x), f(x')) \u2264L . d(x, x')Vx, x' E X where L > 0 is a constant and d(., .) is a distance function (Petersen et al., 2021) At the intersection of these two fields an already recognized misalignment exists between the fair machine-learning and philosophical approach (Fazelpour and Lipton, 2020). In light of this, there are several research lines that propose frameworks to help understand existing notions of algorith- mic fairness in moral philosophy terms (Heidari et al., 2019; Simons et al., 2021). In our work we argument for introducing Kantian deontology in fairness metrics. 4 Deontological AI Alignment This section considers the compatibility of Kantian deontological ethics with the field of AI fairness and alignment. We also offer examples of how Kantian ethics can inform AI fairness metrics. 4.1 The Issue of Aggregation One of the key Kantian critiques charged against utilitarianism is that the utilitarian aggregates the concept of 'good', which inevitably involves optimization of group outcomes at the expense of one or several individuals' well-being. The intrinsic value of every individual is set aside in the utilitarian context in favour of accumulating a perceived external 'good', such as happiness or pleasure(cf. Section 2). Analyzing an individual through a group setting, such as treating them based on immutable characteristics like ethnicity, and subsequently optimizing or altering decisions based on the group's characteristics erodes the intrinsic value of each individual. If we take individuals to be intrinsically valuable, this means that their value is independent of specific characteristics; we cannot see them as mere parts to be subordinated for collective or group ends. We cannot, according to Kantian principles, deny an individual's intrinsic self-worth by focusing on specific features to bring about a desired social outcome. The moral law grants impenetrable worth to each individual and therefore, every individual is to be treated as an autonomous entity equal to everyone else3. This critique finds relevance in contemporary discussions around group-based fairness metrics in AI; when optimizing for fairness there is also a need to ensure the protection of individual rights and dignity. An illustrative example of this concern can be found in Yule's effect on fairness, as highlighted by Ruggieri et al. (2023): When pursuing group fairness objectives, such as independence, but failing to account for the control variable Z, fair machine learning algorithms may inad- vertently yield disparate effects on distinct distributions. Some subgroups may benefit positively from these fairness efforts, experiencing higher fairness, while others may suffer negative consequences, resulting in lower fairness. This observation underscores the nuanced challenge of balancing group-level fairness with the preservation of individual rights and dignity, resonating with the Kantian call for a thorough ex- amination of actions and how they affect each individual. AI fairness underscores the importance of designing fairness metrics that consider the impact on all individuals, such as individual fairness (cf. Definition 3), and avoiding the aggregation of harms or benefits that might undermine the intrinsic value of each person. 3 Giving preference to underprivileged groups may however be permitted as the categorical imperative inter- preted through the humanity formula rejects indifference 5"
    },
    {
        "filename": "2311.05227.pdf",
        "page_number": 6,
        "page_content": "4.2 Utilitarian AI Fairness Metrics In this section, we draw parallels between the utilitarian approach and fairness metrics rooted in measuring error disparities among protected groups. Specifically, we such align metrics as equal opportunity, Definition 3.2 (differences in the True Positive Rates), and accuracy equality Defini- tion 3.3 (accuracy differences). These metrics primarily hinge on evaluating outcomes, particularly group disparities of model errors. We thus argue that these metrics align with a utilitarian approach. In contrast, and in light of our Kantian deontological review, we argue that deontological fairness metrics prioritize assessing model behavior rather than an exclusive focus on error rates. Metrics such as demographic parity (cf. Definition 3.1), equal treatment (cf. Definition 3.4), and individual fairness (cf. Definition 3) put an emphasis on the conduct of AI models and align more closely with the Kantian approach. Kantian ethics offers a compelling lens to analyze AI fairness metrics. As Kant maintained individ- uals ought to be treated as ends in themselves, thus safeguarding their intrinsic worth against any potential outcomes. In the context of AI fairness, this underscores the importance of treating all individuals irrespective of their relationship to model errors. Consider the example of demographic parity Definition 3.1. Kantian ethics would generally contend that an AI model achieves fairness when its predictions are independent of the protected attribute, echoing the Kantian categorical im- perative (humanity formula) to treat individuals as ends, never as mere means to an end. As we mentioned in the introduction, the deontological approach aligns well with widely accepted inter- national declarations that recognize a set of inalienable rights based on the individual, such as the Universal Declaration of Human Rights. If the AI fairness metric approach wishes to align itself with well-established international standards and rights, it ought to adopt a Kantian stance within its scope. With these two ethical approaches in mind, if we examine comprehensive surveys of fairness metrics, such as the one by Wachter et al. (2021), twelve out of the fourteen formalized group fairness metrics rely on model error. There is therefore a disproportionate focus on utilitarian fairness metrics. The other two metrics rely on model behaviour, as outlined in the previous section 4.1. Given the lack of a deontological Kantian approach for fairness metrics, a discussion on how to incorporate this ethical approach encourages an exploration of the moral foundations of fairness metrics and moreover, potentially better aligning AI systems with the widespread recognition of values surrounding the intrinsic dignity and worth of the individual. 4.3 Balancing Fairness and Performance Metrics The classical utilitarian approach argues that morality is founded upon the search for the summum bonum, or the highest form of the good, which, for example, Mill defined as happiness (Mill, 1966). In the context of AI fairness, this pursuit can be likened to a secondary optimization endeavor, akin to fine-tuning, where the goal is to achieve fairness in tandem with optimizing model performance. Techniques such as post-processing adjustments or regularization methods are normally used in this secondary optimization stage, aiming to reshape AI systems into less discriminatory entities while maintaining high-performance algorithms. However, following the third Kantian critique of utilitarianism, in Section 2, we question the trade- off of maximizing benefit or reducing harm as the sole approach for determining which actions should be deemed morally permissible or prohibited. This critique reminds us that focusing on the consequences may not provide a sufficiently robust foundation for determining what constitutes as ethical decisions and actions, especially in complex domains like AI fairness. 4.4 Categorical Imperative as Procedural Fairness The Kantian moral law proposes that individuals should act according to the various formulations of the categorical imperative, such as that the action could be universally applied as a law. This approach to morality is thus procedural. As Korsgaard argues, the Kantian approach: \" ... thinks that there are answers to moral questions because there are correct procedures for arriv- ing at them. \" Korsgaard (1996)(p. 36-37) 6"
    },
    {
        "filename": "2311.05227.pdf",
        "page_number": 7,
        "page_content": "With the Kantian approach there is a set of procedural principles, the formulations of the categorical imperative, that provide a systematic method to help us arrive at morally permissible actions. Applied to AI, this notion encourages the development of ethical procedures that are impartial and equitable across diverse contexts. This aligns with the notion of procedural fairness that has been introduced in fair machine learning literature. Procedural fairness relates to the process, not just the model, which includes the data, processing and mechanism used to achieve the out- come (Marcinkowski et al., 2020; Grgic-Hlaca et al., 2018). If we assume that machines are judged against the same standards, their results will be perceived as fair if people understand that the system meets these same criteria. Focusing on procedure is a familiar approach as it is also integral to many legal systems; it is not only the outcomes but the integrity of due process that defines the quality of the legal system. For example, while going to prison is an undesirable outcome, if the procedure leading to this decision was rightly followed by the lawyers and judge, the decision is more often than not deemed as fair. Procedural fairness, as seen through the lens of the categorical imperative, underscores the impor- tance of crafting AI systems that adhere to universal rules and treat all individuals with equal con- sideration. In practice, this means that the procedures governing AI decision-making should be free from biases, discrimination, and arbitrary distinctions, not just having an unbiased algorithm. By upholding principles that transcend specific circumstances and group identities, AI systems can align with the moral imperative of treating every individual as an end in themselves, rather than as a means to an end. Kant's categorical imperative thus provides a philosophical foundation for pro- cedural fairness in AI ethics, guiding the development of AI systems that operate within a universal framework of equitable and just procedures. 5 Conclusion In this paper, we have examined the compatibility of Kantian deontological ethics with the field of AI alignment with fairness metrics. We revisited Kant's critique of utilitarianism, which prioritizes outcomes, and explored its relevance to AI fairness. Kant's ethical framework emphasizes treating individuals as ends in themselves, aligning with the foundational principles of fairness in AI that seek equitable and just treatment for all, irrespective of group identities and model errors. The current landscape of AI fairness metrics primarily focuses on optimizing outcomes and reducing disparities in model errors. Kant's critique of this utilitarian approach reminds us of the need for a deeper examination of the moral principles underlying fairness. The deontological approach also offers a procedural fairness framework for AI ethics, highlighting the importance of impartial and equitable procedures in AI decision-making. This aligns with the established concept of procedural fairness in AI ethics, ensuring that AI systems uphold universal principles of equitable treatment and respect for individual dignity. By integrating Kantian ethics into AI alignment, we not only bring in an widely-accepted prominent moral theory, but also strive for a more morally grounded AI landscape that better balances outcomes and procedures in pursuit of fairness and justice. References Barocas, S., Hardt, M., and Narayanan, A. (2019). Fairness and Machine Learning: Limitations and Opportunities. fairmlbook.org. http://www.fairmlbook.org. Binns, R. (2018). Fairness in machine learning: Lessons from political philosophy. In FAT, vol- ume 81 of Proceedings of Machine Learning Research, pages 149-159. PMLR. Candelas, J. Q., Wu, Y., Hsu, B., Jain, S., Ramos, J., Adams, J., Hallman, R., and Basu, K. (2023). Disentangling and operationalizing AI fairness at linkedin. In FAccT, pages 1213-1228. ACM. Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel, R. S. (2012). Fairness through awareness. In Goldwasser, S., editor, Innovations in Theoretical Computer Science 2012, Cambridge, MA, USA, January 8-10, 2012, pages 214-226. ACM. Fabbrizzi, S., Zhao, X., Krasanakis, E., and et al. (2023). Studying bias in visual features through the lens of optimal transport. Data Mining and Knowledge Discovery. 7"
    },
    {
        "filename": "2311.05227.pdf",
        "page_number": 8,
        "page_content": "Fazelpour, S. and Lipton, Z. C. (2020). Algorithmic fairness from a non-ideal perspective. In AIES, pages 57-63. ACM. Goodin, R. E. (1995). Utilitarianism as a public philosophy. . Grgic-Hlaca, N., Zafar, M. B., Gummadi, K. P., and Weller, A. (2018). Beyond distributive fairness in algorithmic decision making: Feature selection for procedurally fair learning. In AAAI, pages 51-60. AAAI Press. Hardt, M., Price, E., and Srebro, N. (2016). Equality of opportunity in supervised learning. In NIPS, pages 3315-3323. Heidari, H., Loi, M., Gummadi, K. P., and Krause, A. (2019). A moral framework for understanding fair ML through economic models of equality of opportunity. In danah boyd and Morgenstern, J. H., editors, Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* 2019, Atlanta, GA, USA, January 29-31, 2019, pages 181-190. ACM. Kamishima, T., Akaho, S., Asoh, H., and Sakuma, J. (2012). Fairness-aware classifier with prejudice remover regularizer. In ECML/PKDD (2), volume 7524 of Lecture Notes in Computer Science, pages 35-50. Springer. Kant, I. (1785). Groundwork of the Metaphysics of Morals. Cambridge University Press. Kant, I. and Gregor, M. J. (1996). Groundwork of the metaphysics of morals: Practical philosophy. Trans MJ Gregor (Cambridge University Press, Cambridge, 1996), 4(456):p102. Knutsson, S. (2016). Measuring happiness and suffering. Foundational Research Institute. Korsgaard, C. M. (1996). The sources of normativity. Cambridge University Press. Korsgaard, C. M. (2018). Fellow creatures: Our obligations to the other animals. Oxford University Press. Kymlicka, W. (2002). Contemporary political philosophy: An introduction. oxford: oxford Univer- sity Press. London, A. J. and Heidari, H. (2023). Beneficent intelligence: A capability approach to modeling benefit, assistance, and associated moral failures through ai systems. Marcinkowski, F., Kieslich, K., Starke, C., and L\u00fcnich, M. (2020). Implications of AI (un-)fairness in higher education admissions: the effects of perceived AI (un-)fairness on exit, voice and orga- nizational reputation. In FAT*, pages 122-130. ACM. Menon, A. K. and Williamson, R. C. (2018). The cost of fairness in binary classification. In FAT, volume 81 of Proceedings of Machine Learning Research, pages 107-118. PMLR. Mill, J. S. (1966). Utilitarianism. Springer. Mougan, C., State, L., Ferrara, A., Ruggieri, S., and Staab, S. (2023). Equal treatment: Measuring fairness using explanation distributions. Ntoutsi, E., Fafalios, P., Gadiraju, U., Iosifidis, V., Nejdl, W., Vidal, M., Ruggieri, S., Turini, F., Papadopoulos, S., Krasanakis, E., Kompatsiaris, I., Kinder-Kurlanda, K., Wagner, C., Karimi, F., Fern\u00e1ndez, M., Alani, H., Berendt, B., Kruegel, T., Heinze, C., Broelemann, K., Kasneci, G., Tiropanis, T., and Staab, S. (2020). Bias in data-driven artificial intelligence systems - an introductory survey. WIREs Data Mining Knowl. Discov., 10(3). O'Neill, O. (2022). A philosopher looks at digital communication, volume 4. Cambridge University Press. Pedreschi, D., Ruggieri, S., and Turini, F. (2008). Discrimination-aware data mining. In KDD, pages 560-568. ACM. Petersen, F., Mukherjee, D., Sun, Y., and Yurochkin, M. (2021). Post-processing for individual fairness. In NeurIPS, pages 25944-25955. 8"
    },
    {
        "filename": "2311.05227.pdf",
        "page_number": 9,
        "page_content": "Posner, R. A. (1979). Utilitarianism, economics, and legal theory. The Journal of Legal Studies, 8(1):103-140. Rodolfa, K. T., Lamba, H., and Ghani, R. (2021). Empirical observation of negligible fairness- accuracy trade-offs in machine learning for public policy. Nat. Mach. Intell., 3(10):896-904. Ruggieri, S., \u00c1lvarez, J. M., Pugnana, A., State, L., and Turini, F. (2023). Can we trust fair-ai? In AAAI, pages 15421-15430. AAAI Press. Simons, J., Bhatti, S. A., and Weller, A. (2021). Machine learning and the meaning of equal treat- ment. In Fourcade, M., Kuipers, B., Lazar, S., and Mulligan, D. K., editors, AIES '21: AAAI/ACM Conference on AI, Ethics, and Society, Virtual Event, USA, May 19-21, 2021, pages 956-966. ACM. Singer, P. (2011). Practical Ethics. Cambridge University Press, 3rd edition. Wachter, S., Mittelstadt, B., and Russell, C. (2021). Bias preservation in machine learning: The legality of fairness metrics under eu non-discrimination law. West Virginia Law Review, 123(3). Watson, L. (2021). The right to know: Epistemic rights and why we need them. Routledge. Zafar, M. B., Valera, I., Gomez-Rodriguez, M., and Gummadi, K. P. (2017). Fairness constraints: Mechanisms for fair classification. In AISTATS, volume 54 of Proceedings of Machine Learning Research, pages 962-970. PMLR. 9"
    }
]