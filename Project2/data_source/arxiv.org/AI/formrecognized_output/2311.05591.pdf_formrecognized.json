[
    {
        "filename": "2311.05591.pdf",
        "page_number": 1,
        "page_content": "Accuracy of a Vision-Language Model on Challenging Medical Cases Thomas Buckley1, James A. Diao, B.S.1, Adam Rodman, M.D.2, and Arjun K. Manrai, Ph.D.1* 1 Department of Biomedical Informatics, Harvard Medical School, Boston, MA 2 Department of Medicine, Beth Israel Deaconess Medical Center, Boston, MA *Correspondence: Arjun K. Manrai, Ph.D. Department of Biomedical Informatics, Harvard Medical School 10 Shattuck St., Boston, MA, 02115 Arjun Manrai@hms.harvard.edu 1"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 2,
        "page_content": "ABSTRACT Background: General-purpose large language models that utilize both text and images have not been evaluated on a diverse array of challenging medical cases. Methods: Using 934 cases from the NEJM Image Challenge published between 2005 and 2023, we evaluated the accuracy of the recently released Generative Pre-trained Transformer 4 with Vision model (GPT-4V) compared to human respondents overall and stratified by question difficulty, image type, and skin tone. We further conducted a physician evaluation of GPT-4V on 69 NEJM clinicopathological conferences (CPCs). Analyses were conducted for models utilizing text alone, images alone, and both text and images. Results: GPT-4V achieved an overall accuracy of 61% (95% CI, 58-64%) compared to 49% (95% CI, 49-50%) for humans. GPT-4V outperformed humans at all levels of difficulty and disagreement, skin tones, and image types; the exception was radiographic images, where performance was equivalent between GPT-4V and human respondents. Longer, more informative captions were associated with improved performance for GPT-4V but similar performance for human respondents. GPT-4V included the correct diagnosis in its differential for 80% (95% CI, 68-88%) of CPCs when using text alone, compared to 58% (95% CI, 45-70%) of CPCs when using both images and text. Conclusions: GPT-4V outperformed human respondents on challenging medical cases and was able to synthesize information from both images and text, but performance deteriorated when images were added to highly informative text. Overall, our results suggest that multimodal Al models may be useful in medical diagnostic reasoning but that their accuracy may depend heavily on context. 2"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 3,
        "page_content": "INTRODUCTION Clinical reasoning is a complex cognitive task that includes the integration of multiple types of clinical data.1 Much of this data comes from the patient, including the chief concern and history of present illness as well as the physical examination. However, increasingly this information is stored in electronic format, such as progress notes and discharge summaries, clinical images, and patient-provider communications. This information is often poorly organized, lengthy, and duplicative.2 Expert clinicians query, organize, and interpret signals from this complex mixture of data to construct a comprehensive clinical picture of the patient. With increasing complexity of clinical information, artificial intelligence (Al) applications in medicine have shown promise to assist with clinical reasoning tasks. To date, most Al models focus on a specific clinical task using a single data modality; for example, detecting diabetic retinopathy from retinal fundus photographs.3,4 At the same time, large language models (LLMs) like Generative Pre-trained Transformer 4 (GPT-4)5,6 have demonstrated strong performance on an array of text-based tasks including answering medical licensing exam questions,7 writing empathetic responses to patient queries,8 and solving challenging diagnostic cases.9 Just as human experts can better interpret image studies when provided clinical context, multimodal Al models that utilize both medical images and text may exhibit superior general diagnostic performance compared to unimodal systems that utilize images or text alone. However, there is limited evidence of multimodal reasoning on a general collection of challenging medical cases; this may be attributable to both limitations in data benchmarks used and Al model availability. Here, we evaluated the accuracy of the new GPT-4 with vision model (GPT-4V), 10 an Al vision-language model released on September 25, 2023, on 934 cases from the New England Journal of Medicine (NEJM) Image Challenge spanning 2005 to 2023.11 We further analyzed the Al model's ability to use images in 69 cases from the NEJM clinicopathological conferences, 3"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 4,
        "page_content": "which present complex medical scenarios culminating in definitive pathological diagnoses. GPT-4V interacts with text, images, or text and images together. The clinical cases include paired text prompts that vary considerably in their length and information content, enabling fine-grained analysis of relative contributions to accuracy from images themselves versus the text that accompany them. We compare GPT-4V's accuracy to human respondents across case difficulty and disagreement levels, image types, clinical domains, and skin tones, and further analyze common errors by both humans and the Al model to understand the potential near-term relevance of multimodal Al in medical diagnosis. 4"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 5,
        "page_content": "METHODS Image Challenge Cases and Clinicopathological Conferences We retrieved 936 cases from the NEJM Image Challenge11 published between 2005 and October 12, 2023. Each case consists of a medical image and associated prompt question (e.g., \"What is the most likely diagnosis?\"), five multiple-choice options, and a hidden correct answer. Many cases additionally provide text captions with relevant clinical context or other background information. Respondents, likely comprising physicians and medical trainees, submit answers through the NEJM website and mobile app; we obtained the distribution of responses for each case. The images included radiographic images, natural and dermatoscopic skin images, electrocardiograms, histopathology images, endoscopy images, and ophthalmoscopy images. The NEJM clinicopathological conferences, also known as the Case Records of the Massachusetts General Hospital, are challenging cases that begin with an initial case presentation comprising both text and images. An expert physician is asked to provide an initial differential diagnosis and most likely diagnosis, followed by a review of additional testing and the final diagnosis. We retrieved text and images from the \"Presentation of Case\" section of 80 clinicopathological cases published between January 2021 and December 2022, and applied the same exclusion criteria used in a prior study that used GPT-4 on text only.9 We excluded 4 cases that were not diagnostic dilemmas, 6 cases that were too long to run in a single prompt, and one case that GPT-4V refused to run based on content, leaving 69 total cases. GPT-4V Experiments All image challenge cases were run through the ChatGPT web interface using the GPT-4V model during October 2023. A new session was used for each question to prevent information leakage from previous queries. A prefix prompt describing the format of the image challenge and instructing the model to act as a medical expert was prepended to each question 5"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 6,
        "page_content": "(Supplementary Appendix). Evaluation was performed by prompting GPT-4 to score the GPT-4V response to each question by comparing a prediction to the reference answer and providing three scoring options: correct, incorrect, or refused (Supplementary Appendix). Manual inspection of the scoring was performed for half of the multimodal trial cases to ensure accuracy. Since GPT-4V is pretrained on massive amounts of data including scraped web pages, memorization of leaked cases could affect performance. The training of GPT-4V ended in 2022.6 We therefore performed a sensitivity analysis and measured the accuracy on cases in 2023 alone to compare to its performance in earlier years. To study multimodality, we evaluated model performance on cases with only the image included, only the text included, and both included; the prefix prompt was adjusted accordingly (Supplementary Appendix). We then stratified performance by decile of question word count. We further compared the accuracy of GPT-4V by image type and skin tone using a prior set of annotations from the NEJM Image Challenge.12 Annotations were available for 764 cases. Since the NEJM Image Challenge covers a broad spectrum of specialities, we limited evaluation to image types with at least 30 cases, which included: cutaneous-subcutaneous (311 cases), radiology (219 cases), oral-external (62 cases), and ocular-external (43 cases). Remaining annotated images were categorized as other (129 cases). To elicit reasoning for case study examples, we used chain-of-thought (CoT) prompting which adds \"Let's think step by step\" to the end of the prompt. 13 For the NEJM clinicopathological conferences, we provided the text of the case up to but not including the discussant's initial response and differential diagnosis. Image captions and tables were not included. The same prompt was used for both text-only and multimodal (text and image) trials. Our primary outcome was whether or not the final diagnosis appeared in the differential. Secondary outcomes included having the correct diagnosis listed as the top diagnosis on the differential, the length of the differential, and the quality of the differential 6"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 7,
        "page_content": "scored using a previously-published ordinal 5-point rating scale that describes both accuracy and usefulness, where a 5 is given for having the exact diagnosis on the differential, and a 0 is given when no diagnoses were close.14 A physician (A.R.) graded both multimodal GPT-4V and GPT-4V (Text Only) outputs for all NEJM clinicopathological conferences. Additionally, over the two weeks we ran these experiments, we noticed a change in the content moderation policy on ChatGPT. After the second week, 2 cases were no longer accepted by the ChatGPT web interface (cases dated 05-19-2016 and 11-03-2011), reducing the initial set of 936 images to 934 images. This is likely due to added filters preventing images that are assumed to be sexually explicit. Statistical Analysis For human accuracy on a set of cases, we computed the mean of proportion correct for each question. 95% confidence intervals for human accuracy were computed using the t-distribution. GPT accuracy on a set of cases was computed as the proportion correct with 95% confidence intervals computed using exact Clopper-Pearson intervals. Absolute differences were computed using an unpaired t-test while relative differences were computed using bootstrapping with 5000 replicates. P-values were adjusted for multiple testing using the Holm-Bonferroni method. Statistical analyses were performed in R version 4.3.0. The Harvard Medical School Institutional Review Board (IRB) determined that this study does not require IRB oversight. 7"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 8,
        "page_content": "RESULTS Accuracy of a Multimodal LLM on Challenging Medical Cases Compared to Humans GPT-4V achieved an overall accuracy of 61% (95% CI, 58% to 64%) across the 934 image challenge cases, compared to an overall accuracy of 49% (95% CI, 49% to 50%) for human respondents (Table 1). We ranked cases by their level of difficulty using human respondent accuracy and grouped them into tertiles representing easy, medium, and hard cases. GPT-4V outperformed human respondents at every level of difficulty, ranging from 72% (95% CI, 66% to 77%) on easy cases to 44% (95% CI, 39% to 50%) on hard cases. Similarly, we analyzed performance of GPT-4V and human respondents by the level of disagreement amongst human respondents on questions, measured using Shannon entropy. Human agreement was associated with improved performance for both human respondents and GPT-4V, with GPT-4V again outperforming human respondents at each level. Because pre-training data for GPT-4V went through 202210 and may include previously published NEJM Image Challenge cases, we compared the 41 cases published in 2023 with the 893 cases published between 2005 and 2022, shown in Table 1. GPT-4V had an accuracy of 76% (95% CI, 60% to 88%) in 2023 and 60% (95% CI, 57% to 64%) in 2005 to 2022. Respondents had an accuracy of 51% (95% CI, 47% to 54%) in 2023 and 49% (95% CI, 49% to 50%) in 2005 to 2022. Evidence of Multimodal Reasoning Over Text, Images, and Both Together The evolving format of the NEJM Image Challenge, including varying length and information content of accompanying text captions, allowed for a unique analysis of the multimodal reasoning capabilities of GPT-4V. Specifically, while every case includes an image, the cases vary substantially in the length and information content of the accompanying textual description. Several hundred cases contain only the four-word text prompt \"What is the diagnosis?\" while 8"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 9,
        "page_content": "other text descriptions contain information on notable image features, clinical and social history, and results of other clinical tests. For each case, we compared the performance of multimodal \"GPT-4V\" (using both image and text inputs) to \"GPT-4V (Text Only)\" where the model is provided only the case text description, and \"GPT-4V (Image Only)\" where the model is provided only the image. We stratified performance by the decile of the question word count as a proxy for the information content in the accompanying text description (Figure 1A). At one extreme of low word count, when the text is uninformative (e.g. \"What is the diagnosis?\"), GPT-4V (Text Only) often refuses to answer the question (Supplemental Table 1A) and multimodal GPT-4V's performance matched that of GPT-4V (Image Only), outperformed random guessing, and underperformed human respondents. At the other extreme, when text is highly informative, GPT-4V (Text Only) performs equally well as multimodal GPT-4V, and both substantially outperform human respondents and GPT-4V (Image Only). Between these extremes, multimodal GPT-4V outperforms both of its text-only and image-only counterparts, and starts performing better than human respondents after the third decile of question word count. Unlike GPT-4V, human respondents do not perform better with longer text captions, suggesting that they do not rely much on the text descriptions (Figure 1A). Over all 934 cases, performance was highest for GPT-4V, followed by GPT-4V (Text Only) and human respondents, and then by GPT-4V (Image Only), as shown in Figure 1B. All GPT-4V variants and human respondents performed substantially better than random guesses. Accuracy by Image Type and by Skin Color Cases published between October 2005 to July 2020 were previously labeled by image type, including natural and dermatoscopic images of skin disease, radiographic images, external ocular images, and external oral images.12 GPT-4V outperformed human respondents in each category. The one exception was radiographic images, on which GPT-4V had similar accuracy 9"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 10,
        "page_content": "to human respondents. On balance, natural images, including those of skin, eyes, and mouth, were associated with the highest diagnostic accuracy. Of the 767 cases published between October 13, 2005 and July 9, 2020, 420 indexed cases included images containing skin with identifiable skin tone. Each of these images was given a Fitzpatrick skin type rating by a board-certified dermatologist in a prior study.12 We analyzed GPT-4V and human respondent performance by skin type, categorized into \"light\" (1-2), \"intermediate\" (3-4), and \"dark\" (5-6). GPT-4V performed better than human respondents across all skin tones and we found that neither humans nor GPT-4V exhibited differences in accuracy by skin tone. Multimodal Reasoning Explanations by GPT-4V GPT-4V, GPT-4V (Image Only), and GPT-4V (Text Only) demonstrated varying degrees of clinical reasoning under uncertainty, which were elucidated by examining success and failure modes (Figure 3). When performed correctly, text-based reasoning allowed GPT-4V to answer difficult questions that the majority of respondents answered incorrectly. For example, in a case of diagnosing non-pruritic, non-hypoethestic lesions in a recent immigrant from Pakistan, GPT-4V (Text Only) inferred the correct answer based on prevalence in Pakistan, correctly noting that hypoesthesia is common but not universally present in leprosy (Figure 3A). GPT-4V (Multimodal) agreed, while GPT-4V (Image Only) incorrectly classified the lesions as scrofula. In another case of diagnosing a rash under ultraviolet light, all GPT-4V models correctly answered \"erythrasma\" (Figure 3B). The multimodal GPT-4V model correctly notes that these lesions fluoresce coral red under ultraviolet light. The GPT-4V (Text Only) model correctly answered with the following reasoning: \"the diagnosis that can be specifically identified by ultraviolet light (Wood's lamp) is 'Erythrasma'.\" This indicates both the ability of vision-language models to provide best guess answers to multiple-choice questions under limited information, but also its tendency to provide answers even when a diagnosis cannot be determined from the given data. 10"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 11,
        "page_content": "GPT-4V also exhibited several important failure modes. These include the inability to perform correct visual assessments of key image features, as observed in the case on pectus excavatum (Figure 3C), where 79% of human respondents identified the correct diagnosis. Integrating Images with Text in Clinicopathological Conferences Multimodal GPT-4V was evaluated on 69 NEJM clinicopathological conferences published between January 2021 and December 2022 to compare with a prior study of unimodal GPT-4 that used case text only.9 As shown in Figure 4, physician-assessed performance showed that the final diagnosis was included in the differential in 40 cases (58%, 95% CI, 45-70%) when using GPT-4V and 55 cases (80%, 95% CI, 68-88%) when using GPT-4V (Text Only). Multimodal GPT-4V assigned the correct diagnosis in 21 cases (30%, 95% CI, 20-43%) while GPT-4V (Text Only) assigned the correct diagnosis in 22 cases (32%, 95% CI, 21-44%). The mean quality score was 4.2 (SD 1.1) for GPT-4V and 4.7 (SD 0.7) for GPT-4V (Text Only). Qualitative differences between the two models are shown in examples in Supplemental Table 2. GPT-4V (Text Only) included more specific diagnoses whereas multimodal GPT-4V included more general diagnoses like \"inflammatory conditions\" or \"metastatic cancer.\" GPT-4V also demonstrated less causal language, while GPT-4V (Text Only) typically included more complex diagnoses such as \"metastatic disease in the lung causing the cavitary lesion.\" 11"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 12,
        "page_content": "DISCUSSION GPT-4V outperformed human respondents in the NEJM Image Challenge across all levels of case difficulty and disagreement, skin tones, and image types. The one exception was for radiographic images, where the Al model matched human respondents. GPT-4V answered questions using text descriptions alone, images alone, or both modalities together, in many cases providing coherent explanations for its reasoning. The performance of GPT-4V increased with the amount of informative text, whereas the performance of human respondents remained unchanged. Conversely, the addition of images in the challenging NEJM clinicopathological conferences reduced performance compared to using text alone. Overall, these results suggest that multimodal Al models may be useful in medical diagnostic reasoning but that their accuracy may depend heavily on context. In the NEJM clinicopathological conferences, providing image data paradoxically diminished model performance. These cases contain an abundance of informative text, which alone allowed GPT-4V (Text Only) to determine the correct diagnosis within the differential in 80% of cases, outperforming a recent study9 using the same set of clinicopathological conferences (64% of cases). We note that the previous analysis provided tabular data such as laboratory values to GPT-4 whereas we included only the text from the case presentation itself. Furthermore, in our study, the addition of images in multimodal GPT-4V worsened performance (58% of cases) in the clinicopathological conferences. One possible explanation for these findings is that the increased data15 were redundant with text summaries in the case presentation and diluted the contributions of important information in the text. Our study demonstrates that text-based context is critical for the performance of vision-language models. A recent study limited to radiology and pathology images, and without a human baseline, showed limited diagnostic performance of GPT-4V.16 Consistent with this study, our findings showed diminished performance of GPT-4V on radiology images in the 12"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 13,
        "page_content": "NEJM Image Challenge cases, though still matching that of human respondents. Across the many other image types, GPT-4V demonstrated increased multimodal performance and the ability to reason using accompanying informative text. Although our study does not provide comparisons with expert physicians, the steady improvement of multimodal language models presents the potential to eventually broaden access to high-quality interpretation of clinical images and cases. Our study has several limitations. First, since GPT-4V is not open-source, the data used in model pre-training are unknown and model updates over time may affect reproducibility of results. However, our results do not show diminished performance for cases published after the training cut-off date. Second, multiple-choice questions are not representative of the breadth of clinical decision-making. Third, the NEJM Image Challenge and clinicopathologic conferences often reflect interesting, unusual, or educational cases rather than cases that would be commonly observed \"in-the-wild.\" Finally, recorded responses do not necessarily come from unique respondents, and some of these may reflect blind guessing. Deep learning techniques have demonstrated increasing capabilities in medicine. Large language models (LLMs) such as LLaMa,17 GPT-3,18 and GPT-46 have passed the USMLE by over 20 points,7 perform well on medical question answering benchmarks, 19 and produce sensible differential diagnoses on NEJM clinicopathologic conferences using text alone.9 Med-PaLM M, a recently developed multimodal language model combining PaLM and a vision transformer,20 showed state-of-the-art performance in multimodal tasks. Our findings about GPT-4V on challenging medical cases present a compelling proof-of-concept towards further development of multimodal Al tools in medicine. Further improvements may come from fine-tuning the baseline model18,21 or methodological advances that improve the model's image-based assessments. Future research on multimodal Al should account for human use of such systems. The diagnostic process is replete with different modes of input, including spoken and written 13"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 14,
        "page_content": "language, clinical images, the physical exam, and other visual observations. Evidence from radiology and non-medical fields suggest that Al recommendations can affect human behavior in unexpected ways, even decreasing user accuracy despite good model performance.22 Other challenges include the risk of shortcut reasoning,23 including stereotyping or bias,24 that may occur when Al models are tasked with formulating predictions absent sufficient evidence. As input modes continue to multiply, comprehensive studies on the many opportunities and limitations of vision-language models in Al will be necessary prior to clinical use. 14"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 15,
        "page_content": "REFERENCES 1. Ledley RS, Lusted LB. Reasoning foundations of medical diagnosis; symbolic logic, probability, and value theory aid our understanding of how physicians reason. Science 1959;130(3366):9-21. 2. Steinkamp J, Kantrowitz JJ, Airan-Javia S. Prevalence and Sources of Duplicate Information in the Electronic Medical Record. JAMA Netw Open 2022;5(9):e2233348. 3. Gulshan V, Peng L, Coram M, et al. Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs. JAMA 2016;316(22):2402-10. 4. Abramoff MD, Lavin PT, Birch M, Shah N, Folk JC. Pivotal trial of an autonomous Al-based diagnostic system for detection of diabetic retinopathy in primary care offices. NPJ Digit Med 2018;1:39. 5. Lee P, Bubeck S, Petro J. Benefits, limits, and risks of GPT-4 as an Al chatbot for medicine. N Engl J Med 2023;388(13):1233-9. 6. OpenAl. GPT-4 System Card. 2023;Available from: https://cdn.openai.com/papers/gpt-4-system-card.pdf 7. Nori H, King N, Mckinney SM, Carignan D, Horvitz E. Capabilities of GPT-4 on Medical Challenge Problems [Internet]. arXiv [cs.CL]. 2023;Available from: http://arxiv.org/abs/2303.13375 8. Ayers JW, Poliak A, Dredze M, et al. Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum. JAMA Intern Med 2023;183(6):589-96. 9. Kanjee Z, Crowe B, Rodman A. Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge. JAMA 2023;330(1):78-80. 10. Open Al. GPT-4V(ision) System Card [Internet]. [cited 2023 Oct 25];Available from: https://cdn.openai.com/papers/GPTV_System_Card.pdf 11. Image Challenge [Internet]. New England Journal of Medicine. [cited 2023 Oct 25];Available from: https://www.nejm.org/image-challenge 12. Diao JA, Adamson AS. Representation and misdiagnosis of dark skin in a large-scale visual diagnostic challenge. J Am Acad Dermatol 2022;86(4):950-1. 13. Kojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y. Large Language Models are Zero-Shot Reasoners [Internet]. arXiv [cs.CL]. 2022;22199-213. Available from: https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16 f326-Paper-Conference.pdf 14. Bond WF, Schwartz LM, Weaver KR, Levick D, Giuliano M, Graber ML. Differential diagnosis generators: an evaluation of currently available computer programs. J Gen Intern Med 2012;27(2):213-9. 15"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 16,
        "page_content": "15. Liu NF, Lin K, Hewitt J, et al. Lost in the Middle: How Language Models Use Long Contexts [Internet]. arXiv [cs.CL]. 2023;Available from: http://arxiv.org/abs/2307.03172 16. Yan Z, Zhang K, Zhou R, He L, Li X, Sun L. Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V [Internet]. arXiv [cs.CV]. 2023;Available from: http://arxiv.org/abs/2310.19061 17. Touvron H, Martin L, Stone K, et al. Llama 2: Open Foundation and Fine-Tuned Chat Models [Internet]. arXiv [cs. CL]. 2023;Available from: http://arxiv.org/abs/2307.09288 18. Brown T, Mann B, Ryder N, et al. Language models are few-shot learners. Adv Neural Inf Process Syst 2020;33:1877-901. 19. Singhal K, Azizi S, Tu T, et al. Large language models encode clinical knowledge. Nature [Internet] 2023;Available from: http://dx.doi.org/10.1038/s41586-023-06291-2 20. Tu T, Azizi S, Driess D, et al. Towards Generalist Biomedical Al [Internet]. arXiv [cs.CL]. 2023;Available from: http://arxiv.org/abs/2307.14334 21. Wei J, Wang X, Schuurmans D, et al. Chain-of-thought prompting elicits reasoning in large language models [Internet]. arXiv [cs.CL]. 2022;24824-37. Available from: https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31ab ca4-Paper-Conference.pdf 22. Dell'Acqua F, McFowland E, Mollick ER, et al. Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of Al on Knowledge Worker Productivity and Quality [Internet]. 2023 [cited 2023 Nov 6];Available from: https://papers.ssrn.com/abstract=4573321 23. Geirhos R, Jacobsen J-H, Michaelis C, et al. Shortcut learning in deep neural networks. Nature Machine Intelligence 2020;2(11):665-73. 24. Omiye JA, Lester JC, Spichak S, Rotemberg V, Daneshjou R. Large language models propagate race-based medicine. NPJ Digit Med 2023;6(1):195. 16"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 17,
        "page_content": "Accuracy of a vision-language model on challenging medical cases Table 1. Accuracy of a Vision-Language Model vs. Human Respondents on Challenging Medical Cases 18 Figure 1. Evidence of Multimodal Clinical Reasoning by GPT-4V. 19 Figure 2. Accuracy by Image Type and Image Skin Color. 20 Figure 3. Multimodal Reasoning Explanations Provided by GPT-4V 21 Figure 4. Multimodal Evaluation on Clinicopathological Conferences 25 17"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 18,
        "page_content": "Table 1. Accuracy of a Vision-Language Model versus Human Respondents on Challenging Medical Cases Case Category Overall Difficulty Easy Medium 318 89,162 Hard Disagreement Low No. Cases Median No. Human Responses 89,918 934 303 79,101 313 105,457 312 82,949 Human Accuracy (95% CI) GPT-4V Accuracy (95% CI) 0.49 (0.49 to 0.50) 0.61 (0.58 to 0.64) 0.65 (0.64 to 0.66) (0.66 to 0.77) 0.72 0.49 0.67 (0.49 to 0.50) (0.62 to 0.72) 0.35 (0.34 to 0.36) 0.44 (0.39 to 0.50) Absolute Difference (95% CI) 0.12 (0.08 to 0.15) 0.072 (0.021 to 0.12) 0.18 (0.12 to 0.23) 0.10 (0.040 to 0.15) Relative Difference (95% CI) 23% (17% to 30%) 11% (3.3% to 19%) 36% (26% to 46%) 28% (13% to 43%) 0.64 0.69 0.051 8.0% (0.63 to 0.65) (0.63 to 0.74) (-0.0011 to 0.10) (-0.18% to 16%) Medium 311 High Time Period 2005-2022 893 89,630 311 103,804 92,303 2023 41 36,673 0.49 (0.48 to 0.49) 0.70 (0.64 to 0.75) 0.36 0.45 (0.35 to 0.37) (0.39 to 0.50) 0.49 0.60 (0.49 to 0.50) (0.57 to 0.64) 0.51 0.76 (0.47 to 0.54) (0.60 to 0.88) 0.21 (0.16 to 0.26) 0.087 (0.031 to 0.14) 0.11 (0.076 to 0.14) 0.25 (0.11 to 0.39) 43% (32% to 53%) 25% (10% to 39%) 22% (16% to 29%) 49% (22% to 75%) P-value < 0.001 < 0.01 < 0.001 < 0.01 0.055 < 0.001 < 0.01 < 0.001 < 0.01 Table 1. Accuracy of a Vision-Language Model vs. Human Respondents on Challenging Medical Cases. Performance comparisons between GPT-4V and human respondents on NEJM Image Challenge cases between 2005 and 2023. Accuracy is shown overall and by case difficulty, level of human disagreement, and time period. The accuracy for human respondents is the mean of the proportion correct for each set of cases. The accuracy for GPT-4V is the proportion of correct responses for each set of cases. 95% confidence intervals for human accuracy are computed from a t-distribution. 95% confidence intervals (95% Cls) for GPT-4V accuracy are computed using Clopper-Pearson intervals. P-values and 95% Cls for absolute difference were computed using a t-test between GPT-4V mean accuracy and human mean accuracy. P-values were adjusted for multiple tests using the Holm-Bonferroni procedure. The relative difference estimates and 95% Cls were computed by bootstrapping with 5000 replicates. 18"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 19,
        "page_content": "Figure 1. Evidence of Multimodal Clinical Reasoning by GPT-4V A. Comparison of GPT-4V and Human Respondents Across Modalities - GPT-4V (Image Only) - GPT-4V (Text Only) - GPT-4V - Images More Informative Text More Informative 1.00 0.75 Accuracy 0.50 0.25 Random Guess 0.00 1 2 3 4 5 6 7 8 9 Question Word Count Decile B. Overall Accuracy of GPT-4V by Modality Compared to Human Respondents Human I 10 Human H GPT-4V GPT-4V (Text Only) GPT-4V (Image Only) - 0.0 0.2 0.4 0.6 Overall Accuracy (934 cases) Figure 1. Evidence of Multimodal Clinical Reasoning by GPT-4V. A. Accuracy of GPT-4V when using images only, text only, or both on 934 NEJM Image Challenge cases compared to human respondents, stratified by decile of question word count. Word count ranges from 4 to 128. B. Overall accuracy of GPT-4V across all 934 cases when using images only, text only, or both compared to human respondents. Error bars indicate 95% Cls and the dashed black line indicates random guesses. A refusal to answer here is interpreted as incorrect, which explains an accuracy less than random. For example, for trials with text only, the model would refuse to respond when the question was \"What is the diagnosis\" with no image for context. 19"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 20,
        "page_content": "Figure 2. Accuracy by Image Type and Image Skin Color A. Performance of GPT-4V versus Human Respondents by Image Type GPT-4V Human 1.00 0.75 T Accuracy 0.50 T T T 0.25 0.00 cutaneous- subcutaneous radiology ocular-external Image Type oral-external other B. Performance of GPT-4V vs. Human Respondents by Fitzpatrick Skin Type GPT-4V Human 1.00 T 0.75 T Accuracy 0.50 H T 0.25 0.00 1-2 3-4 5-6 Fitzpatrick skin type unsure Figure 2. Accuracy by Image Type and Image Skin Color. Accuracy of GPT-4V vs. human respondents in the NEJM Image Challenges by A. type of image and B. Fitzpatrick skin type. Image annotations were retrieved from Diao et al. JAAD 2020 and Fitzpatrick skin type was rated by a board-certified dermatologist, categorized into \"light\" (1-2), \"intermediate\" (3-4), and \"dark\" (5-6). Error bars are 95% confidence intervals and the dashed black line indicates random guesses. 20"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 21,
        "page_content": "Figure 3. Multimodal Reasoning Explanations Provided by GPT-4V A. GPT-4V correct on difficult question NEJM Image Challenge 06-14-2007 Question These lesions appeared in a recent immigrant from Pakistan. The lesions were neither pruritic nor hypoesthetic. What is the diagnosis? Respondents 1. Granuloma annulare (23%) 2. Scrofula (7%) 3. Leprosy (25%) 4. Pityriasis rosea (15%) 5. Tinea corporis (29%) [max respondents] GPT-4V (Image Only): Based on the image provided, the most likely diagnosis is 'Scrofula'. GPT-4V (Text Only): Given that the individual is a recent immigrant from Pakistan and presents with skin lesions that are neither pruritic (itchy) nor hypoesthetic (reduced sense of touch), the most likely diagnosis from the provided options is \"Leprosy\" (also known as Hansen's disease). Leprosy is caused by Mycobacterium leprae and is more prevalent in certain parts of the world, including parts of Pakistan. The loss or decrease of sensation (hypoesthesia) is a hallmark of leprosy, but not all lesions will necessarily manifest this symptom immediately. So, the most probable diagnosis is \"Leprosy\". Multimodal GPT-4V: Based on the image provided and the information given, the most likely diagnosis for these lesions in a recent immigrant from Pakistan that are neither pruritic nor hypoesthetic is 'Leprosy'. Multimodal GPT-4V with CoT: Alright, let's approach this methodically. Case Information: Lesions on the skin. Neither pruritic (itchy) nor hypoesthetic (reduced sense of touch). Recent immigrant from Pakistan. Differential Diagnoses: \u00b7 Granuloma annulare: This condition usually presents as raised, reddish or skin-colored bumps that form ring patterns. They often appear on the hands and feet. There's no itchiness associated, which matches the description, but the overall presentation doesn't entirely match. \u00b7 Scrofula (Tuberculous lymphadenitis): This is a form of tuberculosis affecting the lymph nodes, typically the cervical nodes. The patient's background as a recent immigrant might make this a consideration, but the image does not depict enlarged lymph nodes. \u00b7 Leprosy: Also known as Hansen's disease, it's caused by the bacteria Mycobacterium leprae. It leads to skin lesions which can be discolored and may become numb due to nerve involvement. Given the description that the lesions are not hypoesthetic, it might not be the first choice, but leprosy is endemic in some parts of Pakistan. \u00b7 Pityriasis rosea: It starts as a large rounded or oval patch on the chest, abdomen, or back. After 1-2 weeks, smaller patches may appear on the back or chest. It's typically itchy, which doesn't align with our description. \u00b7 Tinea corporis: Commonly known as ringworm, it's a fungal infection of the skin. The rash is typically ring-shaped, but it is itchy, which again doesn't align with our description. Conclusion: Based on the provided image, the description, and the patient's background, the most likely diagnosis would be Leprosy. 21"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 22,
        "page_content": "B. GPT-4V (Text Only) correct on difficult question NEJM Image Challenge 03-24-2011 Question Ultraviolet light was shone on this patient's rash. What is the diagnosis? Respondents 1. Erythrasma (28% 2. Intertrigo (9%) 3. Pityriasis rosea (14%) 4. Psoriasis (9%) 5. Tinea versicolor (39%) [max respondents] GPT-4V (Image Only): Based on the image provided, my answer would be: 'Erythrasma' GPT-4V (Text Only): The use of ultraviolet light, specifically Wood's lamp examination, can help diagnose certain skin conditions based on the fluorescence or coloration they produce under this light. Among the choices given: 'Erythrasma' is caused by the bacteria Corynebacterium minutissimum and will fluoresce a coral-red color under a Wood's lamp. 'Intertrigo' is skin irritation in the folds of the skin, often due to moisture, friction, and bacterial or fungal colonization. It does not specifically fluoresce under a Wood's lamp. 'Pityriasis rosea' is a self-limited rash that usually begins with a \"herald patch\" followed by the appearance of smaller lesions. It does not fluoresce under a Wood's lamp. 'Psoriasis' is an autoimmune condition that causes thick, scaly plaques on the skin. It also does not fluoresce under Wood's lamp. 'Tinea versicolor' is a fungal infection caused by Malassezia species and might show a yellowish or gold fluorescence under a Wood's lamp, though it's not always definitive. Given this information and the limited context of the question, the diagnosis that can be specifically identified by ultraviolet light (Wood's lamp) is 'Erythrasma'. So, the answer is: Erythrasma. Multimodal GPT-4V: The image shows a rash that fluoresces coral red under ultraviolet light, which is characteristic of Erythrasma caused by the bacteria Corynebacterium minutissimum. The correct answer is: 'Erythrasma'. Multimodal GPT-4V with CoT: Alright! Let's break it down step by step: Ultraviolet Light Examination: When ultraviolet light is shone on certain skin conditions, they may fluoresce or glow. This technique can help differentiate certain skin conditions from others. Description of the Image: The image appears to show a rash with an irregular pattern and some areas that seem to be fluorescing under the ultraviolet light. Options Given: \u00b7 Erythrasma: This is a bacterial infection that can fluoresce a coral-red color under ultraviolet light. It's caused by the Corynebacterium minutissimum bacteria and often affects areas where the skin folds, like the groin or armpits. . Intertrigo: This is a rash that occurs in skin folds, caused by friction. It doesn't typically fluoresce under ultraviolet light. \u00b7 Pityriasis rosea: This is a skin rash that usually appears on the torso, arms, and legs. It's characterized by a large, round spot called a \"herald patch,\" followed by smaller patches. It doesn't fluoresce under UV light. . Psoriasis: This is a chronic skin condition that causes cells to build up rapidly on the surface of the skin. The extra skin cells form scales and red patches that are sometimes painful or itchy. Psoriasis does not fluoresce under UV light. \u00b7 Tinea versicolor: This is a fungal infection of the skin. It's also known as pityriasis versicolor. The yeast interferes with the normal pigmentation of the skin, resulting in small, discolored patches. Under ultraviolet light, tinea versicolor can fluoresce a yellowish-green color. Conclusion: Based on the image and the fluorescence seen under ultraviolet light, the most likely diagnosis is Erythrasma due to the coral-red fluorescence which is characteristic of this condition. 22"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 23,
        "page_content": "C. GPT-4V incorrect on easy question NEJM Image Challenge 07-03-2008 Question What is the diagnosis? Respondents 1. Flail chest (4%) 2. Pectus arcuatum (5%) 3. Pectus carinatum (9%) 4. Pectus excavatum (79%) [max respondents] 5. Spondylocostal dysostosis (3%) GPT-4V (Image Only): Based on the image provided, the most likely diagnosis is 'Pectus carinatum'. GPT-4V (Text Only): Unfortunately, you haven't provided the case description, so I can't determine the most likely diagnosis from the options given. Please provide the case description so I can assist you further. Multimodal GPT-4V: Based on the provided image and the list of options, the most likely diagnosis is \"pectus carinatum.\" Multimodal GPT-4V with CoT: Alright, let's approach this systematically. Image Description: The image appears to show the chest region of an individual. There seems to be a prominence or protrusion in the chest wall, specifically at the sternum area. Possible Diagnoses: . Flail chest: This is a life-threatening medical condition where a segment of the rib cage breaks and detaches from the rest of the chest wall. The image does not depict multiple broken ribs causing a segment of chest wall to move paradoxically. \u00b7 Pectus arcuatum: This is a less commonly referenced condition and is not a standard term in many medical textbooks. It may be a variation or subtype of another condition, but the image does not show any characteristics specifically pointing to this. \u00b7 Pectus carinatum: Also known as \"pigeon chest\", it is characterized by a protrusion of the sternum and ribs. The sternum appears to be pushed outward. This seems to fit with the image provided. Pectus excavatum: Also known as \"funnel chest\", it is the opposite of Pectus carinatum. In this condition, there is a concave appearance or indentation in the chest wall. This does not fit the image's appearance. \u00b7 Spondylocostal dysostosis: This is a rare congenital disorder involving vertebral and rib anomalies. It would likely present with more widespread skeletal anomalies and not just a singular protrusion like seen in the image. Conclusion: Based on the image and the available options, the most likely diagnosis is Pectus carinatum. 23"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 24,
        "page_content": "D. GPT-4V (Text Only) indeterminate on easy question NEJM Image Challenge 01-12-2006 Question What is most likely to account for the findings on this abdominal radiograph? Respondents 1. Schistosomiasis (4%) 2. Chronic laxative use (14%) 3. Hyperparathyroidism (9%) 4. Ischemic colitis (5%) 5. Ingestion of a heavy metal (69%) [max respondents] GPT-4V (Image Only): Based on the image provided, the radiological appearance is suggestive of numerous calcifications within the colon, often referred to as \"appendicoliths\" or \"fecaliths\" when in the appendix or fecal matter, respectively. The most consistent diagnosis with such a presentation among the options provided would be: Chronic laxative use. Chronic laxative use can lead to calcium or barium salt deposition in the colon, resulting in the characteristic appearance of \"milk of calcium\" or \"soap bubble\" appearance on X-rays. GPT-4V (Text Only): To answer this question accurately, I would need to see the abdominal radiograph in question. However, given that I cannot see the image, I can provide some insight into each of the options:\\nSchistosomiasis: A parasitic infection that primarily affects the urinary and intestinal systems. Radiographic findings might show calcifications in the bladder wall or intestines. Chronic laxative use: This could result in a radiograph that displays an overall loss of haustral markings in the colon (called \"cathartic colon\"), which is a result of the long-term stimulant laxative use damaging the nerves and muscles of the colon. Hyperparathyroidism: This condition can result in kidney stones and bone abnormalities like subperiosteal bone resorption. On an abdominal radiograph, you might see nephrocalcinosis or renal stones. Ischemic colitis: This is when blood flow to a part of the colon is reduced. An abdominal radiograph might show bowel wall thickening or thumbprinting due to edema and hemorrhage. Ingestion of a heavy metal: Ingestion of certain heavy metals, like lead or barium, might show up as opaque or dense areas on the radiograph, depending on the metal and its concentration. To determine the most likely cause based on the abdominal radiograph, I'd need a description of the findings or the ability to see the image itself. Multimodal GPT-4V: The image provided is an abdominal radiograph that displays dense stippling throughout the colon. This finding is consistent with ingestion of a radio-opaque substance. Given the options: 'schistosomiasis' is a parasitic infection that affects the liver, bladder, and intestines but does not present as radio-opaque stippling in the colon. 'chronic laxative use' can cause melanosis coli, which presents as a darkening of the colon wall, but it doesn't present as dense stippling. 'hyperparathyroidism' can lead to hypercalcemia and the development of kidney stones, but it does not cause dense stippling in the colon. 'ischemic colitis' results from reduced blood flow to the colon, and it doesn't present with this pattern. The most consistent answer with the findings in the radiograph is: 'ingestion of a heavy metal' such as barium or lead, which can produce the radio-opaque stippling seen throughout the colon. So, the most likely answer based on the findings on this abdominal radiograph is 'ingestion of a heavy metal'. Multimodal GPT-4V with CoT: Certainly, let's analyze the image and the provided options step by step. Image Analysis: The radiograph depicts multiple small opacities throughout the colon. These opacities are uniformly distributed and are consistent in size and appearance. The pattern is reminiscent of a radiopaque substance present within the colon. Options Analysis: \u00b7 Schistosomiasis: This is a parasitic disease caused by blood flukes. While it can affect the gastrointestinal system, it does not produce the type of radiographic appearance shown in the image. . Chronic laxative use: Chronic use of certain laxatives can result in the accumulation of residual barium, a radiopaque substance. This would appear as multiple small opacities in the colon on an X-ray, similar to what we see in the image. \u00b7 Hyperparathyroidism: While this condition can result in various gastrointestinal and bone manifestations, it does not produce the type of radiographic appearance depicted in the image. . Ischemic colitis: This refers to inflammation and injury of the colon due to reduced blood flow. While ischemic colitis can cause changes in the appearance of the colon on imaging, it doesn't typically manifest as multiple small opacities. \u00b7 Ingestion of a heavy metal: Ingested heavy metals, such as lead or bismuth, can appear as radiopaque substances in the gastrointestinal system. However, the pattern of distribution and the consistency of the opacities seen here are more suggestive of residual barium from laxatives rather than ingested heavy metals. Conclusion: Based on the radiographic appearance and the provided options, the most likely diagnosis accounting for the findings on this abdominal radiograph is 'Chronic laxative use'. This pattern is suggestive of the retention of barium, a radiopaque substance used in some laxatives. Figure 3. Multimodal Reasoning Explanations Provided by GPT-4V. Four example cases and GPT-4V output for GPT-4V (Image Only), GPT-4V (Text Only), Multimodal GPT-4V, and Multimodal GPT-4V with CoT (chain of thought) are shown. Multimodal CoT (chain of thought) uses the same prompt but adds \"Let's think step by step\" to the end of the question to elicit reasoning. Correct answers are highlighted in green and the respondent distribution is also shown. We ranked cases by human performance and selected A-B. Difficult examples that illustrate GPT-4V reasoning multimodally and from text alone, as well as easy examples where C. GPT-4V answers incorrectly and D. GPT-4V (Text Only) refuses to answer without the image. 24"
    },
    {
        "filename": "2311.05591.pdf",
        "page_number": 25,
        "page_content": "Figure 4. Multimodal Evaluation on Clinicopathological Conferences GPT-4V GPT-4V (Text Only) 40 Number of Cases 20 0 0 1 2 Score 3 4 5 Figure 4. Multimodal Evaluation on Clinicopathological Conferences. Shown are physician-scored performance (0 = worst, 5 = best) of Multimodal GPT-4V vs. GPT-4V (Text Only) on 69 NEJM clinicopathological conferences (CPCs) using the text (+/- images) in the \"Presentation of Case\" section of the CPCs. Each case was scored by a board-certified physician using the Bond et al. Gen Intern Med. 2012 criteria. 25"
    }
]