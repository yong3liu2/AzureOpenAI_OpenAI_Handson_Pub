[
    {
        "File name": "2311.05263.pdf",
        "Summary": " We also show that MBMBR outperforms MBR in several text generation tasks, both with encoder-decoder models and with large language models. 2 Related Work MBR decoding has been studied in the context of machine translation (Kumar and Byrne, 2004; Eikema and Aziz, 2020, 2022; Freitag et al., 2022; Bertsch et al., 2023), text summarization (Rush et al., 2015; Narayan et al., 2018), and image captioning (Anderson et al., 2017). Kumar and Byrne (2004) propose a method to compute the expected utility of a hypothesis using a Monte Carlo estimate. Eikema and Aziz (2020) propose a method to compute the expected utility of a hypothesis using a Monte Carlo estimate and a beam search algorithm. Freitag et al. (2022) propose a method to compute the expected utility of a hypothesis using a Monte Carlo estimate and a beam search algorithm with a heuristic to reduce the number of samples. Bertsch et al. (2023) propose a method to compute the expected utility of a hypothesis using a Monte Carlo estimate and a beam search algorithm with a heuristic to reduce the number of samples and a heuristic to reduce the number of hypotheses. 3 Model-Based Minimum Bayes Risk Decoding In this section, we describe the proposed method, Model-Based MBR (MBMBR). We first describe the main limitation of MBR decoding and then introduce MBMBR as a solution to this limitation. 3.1 Limitation of MBR Decoding The main limitation of MBR decoding is that it relies on the model probability P, which is known to be a poor approximation of Phuman (Stahlberg and Byrne, 2019; Holtzman et al., 2020; Meister et al., 2020). This is because the model probability is computed using a limited set of training data, which may not be representative of the true distribution of human-generated sentences. As a result, the model probability may assign high probabilities to sequences that are unlikely to be generated by humans. This can lead to suboptimal decoding decisions. To address this issue, we propose a new decoding strategy called model-based minimum Bayes risk (MBMBR) decoding. 3.2 Model-Based Minimum Bayes Risk Decoding MBMBR is similar to MBR decoding, but instead of using the model probability P, it uses a model-based distribution Pmodel(y) to approximate Phuman. The model-based distribution is defined as: Pmodel(y) = Pmodel(y|x)Pmodel(x) (7) where Pmodel(x) is the probability of the input sentence x. In practice, Pmodel(x) is often unknown and is approximated by the uniform distribution. The model-based distribution is then used to compute the expected utility of a candidate hypothesis h: hmodel = arg max hEHcand y~Pmodel E [u(h, y)] = arg max hE Hcand yEy > u(h, y) . Pmodel(y). (8) Similar to MBR decoding, the integral in Eq. (8) is approximated by a Monte Carlo estimate using a set of reference hypotheses Href sampled from the model-based distribution Pmodel: hMB = arg max 1H ref 1 yEHref u(h,y). (9) hEHcand 3 Empirical Results We empirically evaluate the divergence of the model-based distribution from the model probability. We use the same setup as in Section 5.1.1. We sample 256 sentences per source sentence for De and Ru->En. We use epsilon sampling (Hewitt et al., 2022; Freitag et al., 2023), top-k sampling (Fan et al., 2018), nucleus sampling (Holtzman et al., 2020), and ancestral sampling. The parameters for the sampling methods are set according to the work of Freitag et al. (2023). For epsilon sampling, \u20ac = 0.02. k is set to k = 10 for top-k sampling. For nucleus sampling, p = 0.9 is set. The temperature is set to 1.0 for all algorithms. We evaluate the KL divergence of the model-based distribution from the model probability using the sacreBLEU library (Post, 2018). Table 6 shows the KL divergence of the model-based distribution from the model probability. We observe that the model-based distribution has a smaller KL divergence than the Monte Carlo estimate in all settings. 5 Experiments We evaluate the proposed method on two tasks: machine translation and summarization. We use the WMT'19 De-En dataset (Ng et al., 2019) for machine translation and the XSum dataset (Narayan et al., 2018) for summarization. We use the fairseq library (Ott et al., 2019) for machine translation and the HuggingFace library (Wolf et al., 2020) for summarization. We use the BLEU score (Papineni et al., 2002) for machine translation and the ROUGE score (Lin, 2004) for summarization. We use the sacreBLEU library (Post, 2018) for machine translation and the pyrouge library (Lin, 2004) for summarization. We use the same setup as in Section 5.1.1. We sample 256 sentences per source sentence for De and Ru->En. We use epsilon sampling (Hewitt et al., 2022; Freitag et al., 2023), top-k sampling (Fan et al., 2018), nucleus sampling (Holtzman et al., 2020), and ancestral sampling. The parameters for the sampling methods are set according to the work of Freitag et al. (2023). For epsilon sampling, \u20ac = 0.02. k is set to k = 10 for top-k sampling. For nucleus sampling, p = 0.9 is set. The temperature is set to 1.0 for all algorithms. We evaluate the BLEU score (Papineni et al., 2002) using the sacreBLEU library (Post, 2018). Table 3 and 4 show the BLEU score from Ger- man and Russian to English. Overall, we observe that MBMBR and MBMBRL outperform the base- line, except for ancestral sampling. Table 5 shows the BLEU score of En-De and En-Ru with epsilon sampling. We observe that the proposed method outperforms the baseline in all settings. 5.1.2 Large Language Model with Prompting For the experiments on a large language model, we use IWSLT 2017 French -> English dataset as a benchmark (Cettolo et al., 2017). We use BLOOMZ and mTO model (7.1B) loaded in 8-bit precision to reduce memory consumption (Muen- nighoff et al., 2023).2 64 sentences are sampled using epsilon sampling with \u20ac = 0.02. We use the 1https://github. com/facebookresearch/fairseq/ blob/main/examples/wmt19/README .md 2https://huggingface.co/bigscience/ bloomz-7b1-mt-iwslt17-fr-en 1761 4 MBR 24.31 MBMBR 23.84 MBMBRL 23.78 25.68 Ancestral Sampling 8 16 25.74 27.75 29.11 25.48 27.43 29.04 27.71 32 29.19 64 128 256 31.01 31.85 32.77 30.70 32.05 32.89 30.81 31.92 33.05 #wins MBMBRL > MBR 24/28 #wins MBMBRL > MBMBR 7/28 #wins MBMBR > MBR 23/28 Table 3: BLEU score of MBMBR and MBMBRL with different sampling strategies on WMT'19 De-En. The best score with the same sampling algorithm and number of samples is in bold. The second best score is underlined. between English and other languages in the news domain. We run experiments on four language pairs: English <> German (En<>De) and English <> Russian (En\u2039>Ru) using the pretrained models of each language pair provided by fairseq (Ng et al., 2019).1 We sample 256 sentences per source sentence for De and Ru->En. We use epsilon sampling (Hewitt et al., 2022; Freitag et al., 2023), top-k sampling (Fan et al., 2018), nucleus sampling (Holtzman et al., 2020), and ancestral sampling. The param- eters for the sampling methods are set according to the work of Freitag et al. (2023). For epsilon sampling, \u20ac = 0.02. k is set to k = 10 for top-k sampling. For nucleus sampling, p = 0.9 is set. The temperature is set to 1.0 for all algorithms. We evaluate the BLEU score (Papineni et al., 2002) using the sacreBLEU library (Post, 2018). Table 3 and 4 show the BLEU score from Ger- man and Russian to English. Overall"
    }
]